{
    "desc": "",
    "roles": {
        "Mike": {
            "name": "Mike",
            "profile": "Team Leader",
            "goal": "Manage a team to assist users",
            "constraints": "",
            "desc": "",
            "is_human": false,
            "enable_memory": true,
            "role_id": "",
            "states": [
                "0. RunCommand"
            ],
            "actions": [
                {
                    "name": "RunCommand",
                    "i_context": "",
                    "prefix": "\nYou are a Team Leader, named Mike, your goal is Manage a team to assist users. \nYour team member:\nMike: Team Leader, Manage a team to assist users\nEmma: Product Manager, Analyze user needs, create a PRD or Product Research report (including competitive analysis, market research) after collecting information and data online.\nBob: Architect, Design a concise, usable, complete software system. Output the system design.\nAlex: Engineer, Take on game, app, web development and deployment.\nDavid: Data Analyst, Take on any data-related tasks, such as machine learning, deep learning, web browsing, web scraping, web searching, terminal operation, etc.\n\n",
                    "desc": "",
                    "__module_class_name": "metagpt.actions.di.run_command.RunCommand"
                }
            ],
            "rc": {
                "memory": {
                    "storage": [
                        {
                            "id": "35cca740059444188b1a9c2c91664ce9",
                            "timestamp": "20251126060539155",
                            "content": "# Project Summary\nThe \"Talk to My Lawyer\" project is a SaaS application designed to facilitate legal document generation and review. It supports three user roles: Subscribers who generate letters, Employees who assist with sales and earn commissions through coupon codes, and Admins who review and approve the generated letters. The platform leverages AI for drafting legal documents and integrates with payment processors for subscriptions.\n\n# Project Module Description\nThe application comprises several functional modules:\n- **User Authentication**: Manages user roles and sessions.\n- **Letter Generation**: Allows subscribers to create legal letters using AI.\n- **Subscription Management**: Handles user subscriptions and coupon codes.\n- **Admin Review**: Provides a dashboard for admins to review, edit, and approve letters.\n- **Email Notifications**: Sends emails to users regarding their letters and subscriptions.\n- **PDF Generation**: Converts approved letters into downloadable PDF format.\n\n# Directory Tree\n```\nttml_app/\n\u251c\u2500\u2500 main-main/\n\u2502   \u251c\u2500\u2500 app/\n\u2502   \u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 dashboard/\n\u2502   \u2502   \u251c\u2500\u2500 globals.css\n\u2502   \u2502   \u2514\u2500\u2500 layout.tsx\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u251c\u2500\u2500 supabase/\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 styles/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 package.json\n```\n\n# File Description Inventory\n- `app/api/`: Contains API routes for handling requests related to user authentication, letter generation, and subscriptions.\n- `app/dashboard/`: Contains components for the admin and user dashboards, including letter management and analytics.\n- `components/`: Reusable UI components used throughout the application.\n- `supabase/`: Contains database migrations and functions related to the Supabase backend.\n- `scripts/`: Includes scripts for database setup and administration tasks.\n- `styles/`: Contains global styles for the application.\n- `README.md`: Documentation for the project setup and usage.\n\n# Technology Stack\n- **Frontend**: Next.js 14, TypeScript, Tailwind CSS\n- **Backend**: Supabase, OpenAI GPT-4\n- **Payment Processing**: Stripe\n- **Email Service**: Resend\n\n# Usage\n1. **Install Dependencies**: Run the command to install required packages.\n2. **Build the Application**: Use the build command to compile the application.\n3. **Run the Application**: Start the application server for local development.\n\n\n# INSTRUCTION\n- You can search for the file path in the `/workspace/`\n- After modifying the project files, if this project can be previewed, then you need to reinstall dependencies, restart service and preview;\n\n\nChange the working directory to `/workspace/ttml_app/main-main`.",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "df14fbea5cb742d489435448bacf7ffe",
                            "timestamp": "20251126061240991",
                            "content": "[Message] from User to Mike: there are 0 tables ",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>",
                                "Mike"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "fd44fa9c7dae47b6877901e2654b3026",
                            "timestamp": "20251126061332241",
                            "content": "I need to understand the current state of the Supabase database and help set up the necessary tables for the \"Talk to My Lawyer\" application.\n\n<Terminal.run>\n<cmd>cd /workspace/ttml_app/main-main && find . -type f -name \"*.sql\" -o -name \"*migration*\" -o -name \"*schema*\" | grep -E \"\\.(sql|ts|js)$\" | head -20</cmd>\n</Terminal.run>\n\n<Terminal.run>\n<cmd>cd /workspace/ttml_app/main-main && find supabase -type f 2>/dev/null | head -20</cmd>\n</Terminal.run>\n\n<Editor.read>\n<path>/workspace/ttml_app/main-main/README.md</path>\n</Editor.read>\n\n<RoleZero.reply_to_human>\n<content>\nI understand you're seeing 0 tables in your Supabase database. Let me investigate the database schema and migration files to help you set up the necessary tables for the \"Talk to My Lawyer\" application. I'm checking the project structure to locate the database setup files.\n</content>\n</RoleZero.reply_to_human>",
                            "instruct_content": null,
                            "role": "assistant",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "f53d7801471a4f0093be3647ae85c40e",
                            "timestamp": "20251126061332242",
                            "content": "Command Terminal.run executed:\n[command]: cd /workspace/ttml_app/main-main && find . -type f -name \"*.sql\" -o -name \"*migration*\" -o -name \"*schema*\" | grep -E \"\\.(sql|ts|js)$\" | head -20 \n[command output] : ./COMPLETE_MIGRATION.sql\n./fix_permissions.sql\n./main-main/COMPLETE_MIGRATION.sql\n./main-main/fix_permissions.sql\n./main-main/scripts/001_setup_schema.sql\n./main-main/scripts/002_setup_rls.sql\n./main-main/scripts/003_seed_data.sql\n./main-main/scripts/004_create_functions.sql\n./main-main/scripts/005_letter_allowance_system.sql\n./main-main/scripts/006_audit_trail.sql\n./main-main/scripts/007_add_missing_letter_statuses.sql\n./main-main/scripts/008_employee_coupon_auto_generation.sql\n./main-main/scripts/009_add_missing_subscription_fields.sql\n./main-main/scripts/010_add_missing_functions.sql\n./main-main/scripts/011_security_hardening.sql\n./main-main/supabase/migrations/001_setup_schema.sql\n./main-main/supabase/migrations/002_setup_rls.sql\n./main-main/supabase/migrations/003_seed_data.sql\n./main-main/supabase/migrations/004_create_functions.sql\n./main-main/supabase/migrations/005_letter_allowance_system.sql\n\n\nCommand Terminal.run executed:\n[command]: cd /workspace/ttml_app/main-main && find supabase -type f 2>/dev/null | head -20 \n[command output] : \nsupabase/functions/generate-letter/index.ts\nsupabase/migrations/001_setup_schema.sql\nsupabase/migrations/002_setup_rls.sql\nsupabase/migrations/003_seed_data.sql\nsupabase/migrations/004_create_functions.sql\nsupabase/migrations/005_letter_allowance_system.sql\nsupabase/migrations/006_audit_trail.sql\nsupabase/migrations/007_add_missing_letter_statuses.sql\nsupabase/migrations/008_employee_coupon_auto_generation.sql\nsupabase/migrations/009_add_missing_subscription_fields.sql\nsupabase/migrations/010_add_missing_functions.sql\nsupabase/migrations/010_admin_role_rls.sql\nsupabase/migrations/011_security_hardening.sql\nsupabase/migrations/012_create_coupon_usage_table.sql\nsupabase/migrations/20251122000003_add_missing_functions.sql\nsupabase/migrations/20251124000001_finalize_rls_policies.sql\nsupabase/migrations/COMPLETE_MIGRATION.sql\n\n\nCommand Editor.read executed: path='/workspace/ttml_app/main-main/README.md' content='001|# Talk-To-My-Lawyer - Legal Letter Generation Platform\\n002|\\n003|A production-ready SaaS platform for generating legal letters with AI assistance and attorney review. Built with Next.js 16, Supabase, and OpenAI GPT-4.\\n004|\\n005|## \ud83c\udfd7\ufe0f Architecture\\n006|\\n007|### Tech Stack\\n008|- **Frontend**: Next.js 16 (App Router), TypeScript, Tailwind CSS\\n009|- **Backend**: Supabase (PostgreSQL), Next.js API Routes\\n010|- **AI**: OpenAI GPT-4 for letter generation and improvement\\n011|- **Authentication**: Supabase Auth with role-based access control\\n012|- **Database**: PostgreSQL with comprehensive RLS policies\\n013|- **Deployment**: Vercel-ready with CI/CD support\\n014|\\n015|### User Roles\\n016|1. **Subscriber**: Creates and manages legal letters\\n017|2. **Employee**: Manages coupon codes and earns commissions\\n018|3. **Admin**: Reviews letters, manages users and commissions\\n019|\\n020|## \ud83d\ude80 Quick Start\\n021|\\n022|### Prerequisites\\n023|- Node.js 18+\\n024|- Supabase account and project\\n025|- OpenAI API key\\n026|\\n027|### Installation\\n028|\\n029|1. **Clone and install dependencies**\\n030|```bash\\n031|git clone <repository-url>\\n032|cd talk-to-my-lawyer\\n033|npm install\\n034|```\\n035|\\n036|2. **Configure environment variables**\\n037|```bash\\n038|cp .env.example .env.local\\n039|# Fill in your environment variables\\n040|```\\n041|\\n042|3. **Set up database**\\n043|```bash\\n044|# Run SQL scripts in order against your Supabase project\\n045|supabase db push scripts/001_setup_schema.sql\\n046|supabase db push scripts/002_setup_rls.sql\\n047|supabase db push scripts/003_seed_data.sql\\n048|supabase db push scripts/004_create_functions.sql\\n049|supabase db push scripts/005_letter_allowance_system.sql\\n050|supabase db push scripts/006_audit_trail.sql\\n051|supabase db push scripts/007_add_missing_letter_statuses.sql\\n052|supabase db push scripts/008_employee_coupon_auto_generation.sql\\n053|supabase db push scripts/009_add_missing_subscription_fields.sql\\n054|supabase db push scripts/010_add_missing_functions.sql\\n055|supabase db push scripts/011_security_hardening.sql\\n056|```\\n057|\\n058|4. **Start development server**\\n059|```bash\\n060|npm run dev\\n061|```\\n062|\\n063|Visit `http://localhost:3000` to see the application.\\n064|\\n065|## \ud83d\udccb Environment Variables\\n066|\\n067|Required environment variables (see `.env.example`):\\n068|\\n069|```bash\\n070|# Base URLs\\n071|NEXT_PUBLIC_APP_URL=https://your-domain.com\\n072|NEXT_PUBLIC_DEV_SUPABASE_REDIRECT_URL=http://localhost:3000\\n073|\\n074|# Supabase Configuration\\n075|NEXT_PUBLIC_SUPABASE_URL=your-supabase-url\\n076|NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key\\n077|SUPABASE_SERVICE_ROLE_KEY=your-service-role-key\\n078|\\n079|# AI Configuration\\n080|OPENAI_API_KEY=your-openai-api-key\\n081|\\n082|# Security\\n083|CRON_SECRET=your-cron-secret-key\\n084|```\\n085|\\n086|## \ud83d\udcb3 Pricing Model\\n087|\\n088|- **Single Letter**: $299 (one-time)\\n089|- **Monthly Plan**: $299/month (4 letters)\\n090|- **Yearly Plan**: $599/year (8 letters)\\n091|\\n092|## \ud83d\udc65 User Flow\\n093|\\n094|### Subscriber Journey\\n095|1. **Free Trial**: First letter is free with AI generation\\n096|2. **Subscription Purchase**: Choose plan via Stripe checkout\\n097|3. **Letter Creation**: Fill intake form, AI generates draft\\n098|4. **Attorney Review**: Admin reviews and improves with AI assistance\\n099|5. **Delivery**: Download PDF or send via email\\n100|\\n101|### Employee Journey\\n102|1. **Coupon Assignment**: Auto-generated unique coupon code\\n103|2. **Sharing**: Distribute 20% discount codes to potential customers\\n104|3. **Commission**: Earn 5% on each successful subscription\\n105|4. **Tracking**: Monitor usage, earnings, and points\\n106|\\n107|### Admin Journey\\n108|1. **Letter Review**: Review and approve/reject subscriber letters\\n109|2. **AI Improvement**: Use OpenAI GPT-4 to enhance letter quality\\n110|3. **User Management**: Manage subscribers, employees, and roles\\n111|4. **Commission Management**: Track and process employee commissions\\n112|5. **Analytics**: Monitor platform usage and revenue\\n113|\\n114|## \ud83d\udd12 Security Features\\n115|\\n116|- **Row Level Security**: Complete data isolation between roles\\n117|- **Employee Isolation**: Employees cannot access any letter content\\n118|- **Audit Trail**: Complete logging of all letter status changes\\n119|- **Input Validation**: Comprehensive data sanitization and validation\\n120|- **API Security**: Authentication on all endpoints, proper error handling\\n121|- **Environment Security**: Proper secrets management and configuration\\n122|\\n123|## \ud83d\udcca Key Features\\n124|\\n125|### Core Functionality\\n126|- \u2705 AI-powered letter generation (OpenAI GPT-4)\\n127|- \u2705 Professional attorney review and AI improvement\\n128|- \u2705 PDF generation and email delivery\\n129|- \u2705 Complete audit trail and logging\\n130|- \u2705 Role-based dashboards and access control\\n131|\\n132|### Business Features\\n133|- \u2705 Free trial system\\n134|- \u2705 Subscription management with Stripe\\n135|- \u2705 Employee coupon and commission system\\n136|- \u2705 Monthly letter allowance tracking\\n137|- \u2705 Comprehensive analytics and reporting\\n138|\\n139|### Technical Features\\n140|- \u2705 TypeScript throughout\\n141|- \u2705 Responsive design with Tailwind CSS\\n142|- \u2705 Database functions and triggers\\n143|- \u2705 Security hardening and best practices\\n144|- \u2705 Production-ready deployment setup\\n145|\\n146|## \ud83d\uddc4\ufe0f Database Schema\\n147|\\n148|### Main Tables\\n149|- `profiles`: User accounts with roles\\n150|- `letters`: Legal letters with status tracking\\n151|- `subscriptions`: User subscriptions and credits\\n152|- `employee_coupons`: Employee discount codes\\n153|- `commissions`: Employee earnings tracking\\n154|- `letter_audit_trail`: Complete audit logging\\n155|\\n156|### Key Functions\\n157|- `deduct_letter_allowance`: Manage letter credits\\n158|- `add_letter_allowances`: Add credits on subscription\\n159|- `reset_monthly_allowances`: Monthly credit reset\\n160|- `log_letter_audit`: Audit trail logging\\n161|- `validate_coupon`: Coupon validation\\n162|\\n163|## \ud83d\ude80 Deployment\\n164|\\n165|### Vercel Deployment (Recommended)\\n166|\\n167|1. **Connect to Vercel**\\n168|```bash\\n169|npm i -g vercel\\n170|vercel\\n171|```\\n172|\\n173|2. **Configure Environment Variables**\\n174|Add all required environment variables to Vercel dashboard\\n175|\\n176|3. **Deploy**\\n177|```bash\\n178|vercel --prod\\n179|```\\n180|\\n181|### Manual Deployment\\n182|\\n183|1. **Build application**\\n184|```bash\\n185|npm run build\\n186|```\\n187|\\n188|2. **Start production server**\\n189|```bash\\n190|npm start\\n191|```\\n192|\\n193|## \ud83d\udd04 Cron Jobs\\n194|\\n195|Set up monthly subscription reset:\\n196|\\n197|```bash\\n198|# Add to your cron tab (runs 1st of each month at midnight UTC)\\n199|0 0 1 * * curl -X POST https://your-domain.com/api/subscriptions/reset-monthly \\\\\\n200|  -H \"Authorization: Bearer $CRON_SECRET\"\\n201|```\\n202|\\n203|## \ud83d\udcc8 Monitoring & Analytics\\n204|\\n205|- **User Analytics**: Track registration, subscription, and usage metrics\\n206|- **Revenue Analytics**: Monitor subscription revenue and commission payouts\\n207|- **Letter Analytics**: Track letter generation, approval rates, and types\\n208|- **Security Monitoring**: Comprehensive audit trail and security event logging\\n209|\\n210|## \ud83d\udee0\ufe0f Development\\n211|\\n212|### Database Migrations\\n213|All database changes are managed through SQL scripts in the `scripts/` directory. Scripts must be run in numerical order.\\n214|\\n215|### Adding New Features\\n216|1. Follow the established patterns in existing components\\n217|2. Ensure proper RLS policies for any new tables\\n218|3. Add comprehensive error handling and logging\\n219|4. Update audit trails for any status changes\\n220|5. Test with all user roles (subscriber, employee, admin)\\n221|\\n222|### Code Quality\\n223|- TypeScript for type safety\\n224|- Comprehensive error handling\\n225|- Security-first development approach\\n226|- Responsive design principles\\n227|- Performance optimization\\n228|\\n229|## \ud83e\udd1d Support\\n230|\\n231|For technical support or questions:\\n232|- Review the comprehensive documentation in the `/docs` directory\\n233|- Check the security checklist for security-related questions\\n234|- Monitor audit logs for troubleshooting user issues\\n235|\\n236|---\\n237|\\n238|**Built with \u2764\ufe0f for democratizing access to professional legal services.**'\n\nCommand RoleZero.reply_to_human executed: SUCCESS, human has received your reply. Refrain from resending duplicate messages.  If you no longer need to take action, use the command \u2018end\u2019 to stop.",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.di.run_command.RunCommand",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "f5190485b8f64a9da3300715da24a645",
                            "timestamp": "20251126061338398",
                            "content": "I can see the project has comprehensive SQL migration files. Let me check the main migration file and help you set up the database tables.\n\n<Editor.read>\n<path>/workspace/ttml_app/main-main/supabase/migrations/COMPLETE_MIGRATION.sql</path>\n</Editor.read>\n\n<Terminal.run>\n<cmd>cd /workspace/ttml_app/main-main && ls -la supabase/migrations/ | head -20</cmd>\n</Terminal.run>\n\n<RoleZero.reply_to_human>\n<content>\nI found your database migration files! The project has a comprehensive set of SQL scripts to set up all necessary tables. There's a `COMPLETE_MIGRATION.sql` file that should contain all the schema setup.\n\nLet me read the migration file to understand the complete database structure, and then I'll guide you on how to apply these migrations to your Supabase database to create all the required tables.\n</content>\n</RoleZero.reply_to_human>",
                            "instruct_content": null,
                            "role": "assistant",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "d48aa9c8d4f44aef89c1a1c8a755e2c1",
                            "timestamp": "20251126061338399",
                            "content": "Command Editor.read executed: path='/workspace/ttml_app/main-main/supabase/migrations/COMPLETE_MIGRATION.sql' content='001|-- ============================================================================\\n002|-- COMPLETE MIGRATION FOR TALK-TO-MY-LAWYER\\n003|-- ============================================================================\\n004|-- This file combines all migrations in the correct order for easy deployment\\n005|-- Run this on a fresh Supabase database to set up the complete schema\\n006|-- ============================================================================\\n007|\\n008|-- TASK 9: Complete migration file combining all migrations\\n009|\\n010|-- ============================================================================\\n011|-- 001: SETUP SCHEMA\\n012|-- ============================================================================\\n013|\\n014|-- Enable extensions\\n015|CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\\n016|\\n017|-- Create ENUM types for type safety\\n018|DO $$ BEGIN\\n019|    CREATE TYPE user_role AS ENUM (\\'subscriber\\', \\'employee\\', \\'admin\\');\\n020|EXCEPTION\\n021|    WHEN duplicate_object THEN null;\\n022|END $$;\\n023|\\n024|DO $$ BEGIN\\n025|    CREATE TYPE letter_status AS ENUM (\\n026|        \\'draft\\', \\n027|        \\'generating\\', \\n028|        \\'pending_review\\', \\n029|        \\'under_review\\', \\n030|        \\'approved\\', \\n031|        \\'rejected\\', \\n032|        \\'completed\\', \\n033|        \\'failed\\'\\n034|    );\\n035|EXCEPTION\\n036|    WHEN duplicate_object THEN null;\\n037|END $$;\\n038|\\n039|DO $$ BEGIN\\n040|    CREATE TYPE subscription_status AS ENUM (\\'active\\', \\'canceled\\', \\'past_due\\');\\n041|EXCEPTION\\n042|    WHEN duplicate_object THEN null;\\n043|END $$;\\n044|\\n045|DO $$ BEGIN\\n046|    CREATE TYPE commission_status AS ENUM (\\'pending\\', \\'paid\\');\\n047|EXCEPTION\\n048|    WHEN duplicate_object THEN null;\\n049|END $$;\\n050|\\n051|-- Create profiles table\\n052|CREATE TABLE IF NOT EXISTS profiles (\\n053|    id UUID PRIMARY KEY REFERENCES auth.users(id) ON DELETE CASCADE,\\n054|    email TEXT NOT NULL UNIQUE,\\n055|    full_name TEXT,\\n056|    role user_role DEFAULT \\'subscriber\\',\\n057|    phone TEXT,\\n058|    company_name TEXT,\\n059|    is_super_user BOOLEAN DEFAULT false,\\n060|    created_at TIMESTAMPTZ DEFAULT NOW(),\\n061|    updated_at TIMESTAMPTZ DEFAULT NOW()\\n062|);\\n063|\\n064|-- Create employee_coupons table\\n065|CREATE TABLE IF NOT EXISTS employee_coupons (\\n066|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n067|    employee_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n068|    code TEXT NOT NULL UNIQUE,\\n069|    discount_percent INT CHECK (discount_percent BETWEEN 0 AND 100) DEFAULT 20,\\n070|    is_active BOOLEAN DEFAULT true,\\n071|    usage_count INT DEFAULT 0,\\n072|    created_at TIMESTAMPTZ DEFAULT NOW(),\\n073|    updated_at TIMESTAMPTZ DEFAULT NOW()\\n074|);\\n075|\\n076|-- Create subscriptions table\\n077|CREATE TABLE IF NOT EXISTS subscriptions (\\n078|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n079|    user_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n080|    status subscription_status DEFAULT \\'active\\',\\n081|    coupon_code TEXT,\\n082|    plan TEXT DEFAULT \\'single_letter\\',\\n083|    plan_type TEXT,\\n084|    price NUMERIC(10,2) DEFAULT 299.00,\\n085|    discount NUMERIC(10,2) DEFAULT 0.00,\\n086|    stripe_subscription_id TEXT,\\n087|    remaining_letters INT DEFAULT 0,\\n088|    credits_remaining INT DEFAULT 0,\\n089|    last_reset_at TIMESTAMPTZ,\\n090|    current_period_start TIMESTAMPTZ,\\n091|    current_period_end TIMESTAMPTZ,\\n092|    created_at TIMESTAMPTZ DEFAULT NOW(),\\n093|    updated_at TIMESTAMPTZ DEFAULT NOW()\\n094|);\\n095|\\n096|-- Create commissions table\\n097|CREATE TABLE IF NOT EXISTS commissions (\\n098|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n099|    employee_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n100|    subscription_id UUID NOT NULL REFERENCES subscriptions(id) ON DELETE CASCADE,\\n101|    commission_rate NUMERIC(5,4) DEFAULT 0.05,\\n102|    subscription_amount NUMERIC(10,2) NOT NULL DEFAULT 0,\\n103|    commission_amount NUMERIC(10,2) NOT NULL DEFAULT 0,\\n104|    status commission_status DEFAULT \\'pending\\',\\n105|    paid_at TIMESTAMPTZ,\\n106|    created_at TIMESTAMPTZ DEFAULT NOW()\\n107|);\\n108|\\n109|-- Create letters table\\n110|CREATE TABLE IF NOT EXISTS letters (\\n111|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n112|    user_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n113|    title TEXT NOT NULL,\\n114|    status letter_status DEFAULT \\'draft\\',\\n115|    letter_type TEXT,\\n116|    intake_data JSONB DEFAULT \\'{}\\',\\n117|    ai_draft_content TEXT,\\n118|    final_content TEXT,\\n119|    reviewed_by UUID REFERENCES profiles(id),\\n120|    reviewed_at TIMESTAMPTZ,\\n121|    review_notes TEXT,\\n122|    rejection_reason TEXT,\\n123|    approved_at TIMESTAMPTZ,\\n124|    created_at TIMESTAMPTZ DEFAULT NOW(),\\n125|    updated_at TIMESTAMPTZ DEFAULT NOW()\\n126|);\\n127|\\n128|-- Create coupon_usage table (TASK 2)\\n129|CREATE TABLE IF NOT EXISTS coupon_usage (\\n130|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n131|    user_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n132|    coupon_code TEXT NOT NULL,\\n133|    employee_id UUID REFERENCES profiles(id) ON DELETE SET NULL,\\n134|    discount_percent INT CHECK (discount_percent BETWEEN 0 AND 100),\\n135|    amount_before NUMERIC(10,2) NOT NULL,\\n136|    amount_after NUMERIC(10,2) NOT NULL,\\n137|    created_at TIMESTAMPTZ DEFAULT NOW()\\n138|);\\n139|\\n140|-- Create letter_audit_trail table\\n141|CREATE TABLE IF NOT EXISTS letter_audit_trail (\\n142|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n143|    letter_id UUID NOT NULL REFERENCES letters(id) ON DELETE CASCADE,\\n144|    action TEXT NOT NULL,\\n145|    old_status TEXT,\\n146|    new_status TEXT,\\n147|    performed_by UUID REFERENCES profiles(id),\\n148|    notes TEXT,\\n149|    created_at TIMESTAMPTZ DEFAULT NOW()\\n150|);\\n151|\\n152|-- Create indexes for performance\\n153|CREATE INDEX IF NOT EXISTS idx_profiles_role ON profiles(role);\\n154|CREATE INDEX IF NOT EXISTS idx_profiles_email ON profiles(email);\\n155|CREATE INDEX IF NOT EXISTS idx_employee_coupons_code ON employee_coupons(code);\\n156|CREATE INDEX IF NOT EXISTS idx_employee_coupons_employee ON employee_coupons(employee_id);\\n157|CREATE INDEX IF NOT EXISTS idx_letters_user_id ON letters(user_id);\\n158|CREATE INDEX IF NOT EXISTS idx_letters_status ON letters(status);\\n159|CREATE INDEX IF NOT EXISTS idx_commissions_employee ON commissions(employee_id);\\n160|CREATE INDEX IF NOT EXISTS idx_subscriptions_user ON subscriptions(user_id);\\n161|CREATE INDEX IF NOT EXISTS idx_coupon_usage_user ON coupon_usage(user_id);\\n162|CREATE INDEX IF NOT EXISTS idx_coupon_usage_code ON coupon_usage(coupon_code);\\n163|CREATE INDEX IF NOT EXISTS idx_coupon_usage_employee ON coupon_usage(employee_id);\\n164|CREATE INDEX IF NOT EXISTS idx_letter_audit_letter ON letter_audit_trail(letter_id);\\n165|\\n166|-- ============================================================================\\n167|-- 002: SETUP RLS POLICIES\\n168|-- ============================================================================\\n169|\\n170|-- Enable RLS on all tables\\n171|ALTER TABLE profiles ENABLE ROW LEVEL SECURITY;\\n172|ALTER TABLE employee_coupons ENABLE ROW LEVEL SECURITY;\\n173|ALTER TABLE subscriptions ENABLE ROW LEVEL SECURITY;\\n174|ALTER TABLE commissions ENABLE ROW LEVEL SECURITY;\\n175|ALTER TABLE letters ENABLE ROW LEVEL SECURITY;\\n176|ALTER TABLE coupon_usage ENABLE ROW LEVEL SECURITY;\\n177|ALTER TABLE letter_audit_trail ENABLE ROW LEVEL SECURITY;\\n178|\\n179|-- Helper function to get user role\\n180|CREATE OR REPLACE FUNCTION get_user_role(user_id UUID)\\n181|RETURNS TEXT AS $$\\n182|  SELECT role::TEXT FROM profiles WHERE id = user_id;\\n183|$$ LANGUAGE SQL STABLE SECURITY DEFINER;\\n184|\\n185|-- Profiles policies\\n186|CREATE POLICY \"Users can view own profile\" ON profiles FOR SELECT USING (auth.uid() = id);\\n187|CREATE POLICY \"Users can update own profile\" ON profiles FOR UPDATE USING (auth.uid() = id);\\n188|CREATE POLICY \"Admins can view all profiles\" ON profiles FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n189|\\n190|-- Letters policies\\n191|CREATE POLICY \"Users can view own letters\" ON letters FOR SELECT USING (auth.uid() = user_id);\\n192|CREATE POLICY \"Users can insert own letters\" ON letters FOR INSERT WITH CHECK (auth.uid() = user_id);\\n193|CREATE POLICY \"Users can update own letters\" ON letters FOR UPDATE USING (auth.uid() = user_id);\\n194|CREATE POLICY \"Admins can view all letters\" ON letters FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n195|CREATE POLICY \"Admins can update all letters\" ON letters FOR UPDATE USING (get_user_role(auth.uid()) = \\'admin\\');\\n196|\\n197|-- Subscriptions policies\\n198|CREATE POLICY \"Users can view own subscriptions\" ON subscriptions FOR SELECT USING (auth.uid() = user_id);\\n199|CREATE POLICY \"Admins can view all subscriptions\" ON subscriptions FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n200|\\n201|-- Employee coupons policies\\n202|CREATE POLICY \"Employees can view own coupons\" ON employee_coupons FOR SELECT USING (auth.uid() = employee_id);\\n203|CREATE POLICY \"Anyone can check coupon validity\" ON employee_coupons FOR SELECT USING (true);\\n204|CREATE POLICY \"Admins can manage all coupons\" ON employee_coupons FOR ALL USING (get_user_role(auth.uid()) = \\'admin\\');\\n205|\\n206|-- Commissions policies\\n207|CREATE POLICY \"Employees can view own commissions\" ON commissions FOR SELECT USING (auth.uid() = employee_id);\\n208|CREATE POLICY \"Admins can view all commissions\" ON commissions FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n209|\\n210|-- Coupon usage policies\\n211|CREATE POLICY \"Users can view own coupon usage\" ON coupon_usage FOR SELECT USING (auth.uid() = user_id);\\n212|CREATE POLICY \"Admins can view all coupon usage\" ON coupon_usage FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n213|\\n214|-- Audit trail policies\\n215|CREATE POLICY \"Users can view audit for own letters\" ON letter_audit_trail FOR SELECT \\n216|  USING (EXISTS (SELECT 1 FROM letters WHERE letters.id = letter_audit_trail.letter_id AND letters.user_id = auth.uid()));\\n217|CREATE POLICY \"Admins can view all audit trails\" ON letter_audit_trail FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n218|\\n219|-- ============================================================================\\n220|-- 004: CREATE FUNCTIONS\\n221|-- ============================================================================\\n222|\\n223|-- Function to check letter allowance\\n224|CREATE OR REPLACE FUNCTION check_letter_allowance(u_id UUID)\\n225|RETURNS TABLE (\\n226|  has_allowance BOOLEAN,\\n227|  remaining INT,\\n228|  plan_name TEXT,\\n229|  is_super BOOLEAN\\n230|) AS $$\\n231|DECLARE\\n232|  v_is_super BOOLEAN;\\n233|  v_remaining INT;\\n234|  v_plan TEXT;\\n235|BEGIN\\n236|  -- Check if user is super user\\n237|  SELECT is_super_user INTO v_is_super FROM profiles WHERE id = u_id;\\n238|  \\n239|  IF v_is_super THEN\\n240|    RETURN QUERY SELECT true, 999999, \\'Super User\\'::TEXT, true;\\n241|    RETURN;\\n242|  END IF;\\n243|  \\n244|  -- Check active subscription\\n245|  SELECT \\n246|    COALESCE(credits_remaining, 0),\\n247|    COALESCE(plan, \\'None\\')\\n248|  INTO v_remaining, v_plan\\n249|  FROM subscriptions\\n250|  WHERE user_id = u_id AND status = \\'active\\'\\n251|  ORDER BY created_at DESC\\n252|  LIMIT 1;\\n253|  \\n254|  IF v_remaining > 0 THEN\\n255|    RETURN QUERY SELECT true, v_remaining, v_plan, false;\\n256|  ELSE\\n257|    RETURN QUERY SELECT false, 0, COALESCE(v_plan, \\'None\\'::TEXT), false;\\n258|  END IF;\\n259|END;\\n260|$$ LANGUAGE plpgsql SECURITY DEFINER;\\n261|\\n262|-- Function to log audit trail\\n263|CREATE OR REPLACE FUNCTION log_letter_audit(\\n264|  p_letter_id UUID,\\n265|  p_action TEXT,\\n266|  p_old_status TEXT DEFAULT NULL,\\n267|  p_new_status TEXT DEFAULT NULL,\\n268|  p_notes TEXT DEFAULT NULL\\n269|)\\n270|RETURNS UUID AS $$\\n271|DECLARE\\n272|  v_audit_id UUID;\\n273|BEGIN\\n274|  INSERT INTO letter_audit_trail (\\n275|    letter_id,\\n276|    action,\\n277|    old_status,\\n278|    new_status,\\n279|    performed_by,\\n280|    notes\\n281|  ) VALUES (\\n282|    p_letter_id,\\n283|    p_action,\\n284|    p_old_status,\\n285|    p_new_status,\\n286|    auth.uid(),\\n287|    p_notes\\n288|  ) RETURNING id INTO v_audit_id;\\n289|  \\n290|  RETURN v_audit_id;\\n291|END;\\n292|$$ LANGUAGE plpgsql SECURITY DEFINER;\\n293|\\n294|-- ============================================================================\\n295|-- TRIGGERS\\n296|-- ============================================================================\\n297|\\n298|-- Trigger to auto-create profile on signup\\n299|CREATE OR REPLACE FUNCTION public.handle_new_user()\\n300|RETURNS TRIGGER AS $$\\n301|BEGIN\\n302|    INSERT INTO public.profiles (id, email, full_name)\\n303|    VALUES (NEW.id, NEW.email, NEW.raw_user_meta_data->>\\'full_name\\');\\n304|    RETURN NEW;\\n305|END;\\n306|$$ LANGUAGE plpgsql SECURITY DEFINER;\\n307|\\n308|DROP TRIGGER IF EXISTS on_auth_user_created ON auth.users;\\n309|CREATE TRIGGER on_auth_user_created\\n310|    AFTER INSERT ON auth.users\\n311|    FOR EACH ROW EXECUTE FUNCTION public.handle_new_user();\\n312|\\n313|-- ============================================================================\\n314|-- COMMENTS\\n315|-- ============================================================================\\n316|\\n317|COMMENT ON TABLE coupon_usage IS \\'Tracks all coupon code usage including TALK3 and employee coupons\\';\\n318|COMMENT ON COLUMN coupon_usage.employee_id IS \\'Employee who owns the coupon, NULL for special codes like TALK3\\';\\n319|COMMENT ON COLUMN coupon_usage.discount_percent IS \\'Percentage discount applied (0-100)\\';\\n320|COMMENT ON COLUMN profiles.is_super_user IS \\'Business flag for unlimited letter allowances, NOT an admin role\\';\\n321|\\n322|-- ============================================================================\\n323|-- COMPLETION\\n324|-- ============================================================================\\n325|\\n326|-- Migration complete\\n327|SELECT \\'Talk-To-My-Lawyer database schema setup complete!\\' as status;'\n\nCommand Terminal.run executed:\n[command]: cd /workspace/ttml_app/main-main && ls -la supabase/migrations/ | head -20 \n[command output] : \ntotal 300\ndrwxrwxr-x 18 root root 110592 Nov 26 01:34 .\ndrwxrwxr-x 22 root root 126976 Nov 26 01:34 ..\n-rw-rw-r--  1 root root   4661 Nov 26 01:34 001_setup_schema.sql\n-rw-rw-r--  1 root root   5010 Nov 26 01:34 002_setup_rls.sql\n-rw-rw-r--  1 root root    662 Nov 26 01:34 003_seed_data.sql\n-rw-rw-r--  1 root root   1635 Nov 26 01:34 004_create_functions.sql\n-rw-rw-r--  1 root root   2966 Nov 26 01:34 005_letter_allowance_system.sql\n-rw-rw-r--  1 root root   1748 Nov 26 01:34 006_audit_trail.sql\n-rw-rw-r--  1 root root    761 Nov 26 01:34 007_add_missing_letter_statuses.sql\n-rw-rw-r--  1 root root    769 Nov 26 01:34 008_employee_coupon_auto_generation.sql\n-rw-rw-r--  1 root root   1443 Nov 26 01:34 009_add_missing_subscription_fields.sql\n-rw-rw-r--  1 root root   2692 Nov 26 01:34 010_add_missing_functions.sql\n-rw-rw-r--  1 root root   7813 Nov 26 01:34 010_admin_role_rls.sql\n-rw-rw-r--  1 root root   4539 Nov 26 01:34 011_security_hardening.sql\n-rw-rw-r--  1 root root   1257 Nov 26 01:34 012_create_coupon_usage_table.sql\n-rw-rw-r--  1 root root   8705 Nov 26 01:34 20251122000003_add_missing_functions.sql\n-rw-rw-r--  1 root root   7412 Nov 26 01:34 20251124000001_finalize_rls_policies.sql\n-rw-rw-r--  1 root root  12409 Nov 26 01:34 COMPLETE_MIGRATION.sql\n\n\nCommand RoleZero.reply_to_human executed: SUCCESS, human has received your reply. Refrain from resending duplicate messages.  If you no longer need to take action, use the command \u2018end\u2019 to stop.",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.di.run_command.RunCommand",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "0939603518ca44b4adb3cfe0a84b2876",
                            "timestamp": "20251126061348530",
                            "content": "I can see you have a comprehensive `COMPLETE_MIGRATION.sql` file that contains all the necessary database schema. This file will create all 7 tables needed for your \"Talk to My Lawyer\" application. Let me help you apply this migration to your Supabase database.\n\n<RoleZero.reply_to_human>\n<content>\nI found your database migration file! The `COMPLETE_MIGRATION.sql` contains the complete schema that will create all 7 tables:\n\n1. **profiles** - User accounts with roles (subscriber, employee, admin)\n2. **employee_coupons** - Employee discount codes and tracking\n3. **subscriptions** - User subscriptions and letter credits\n4. **commissions** - Employee earnings tracking\n5. **letters** - Legal letters with status tracking\n6. **coupon_usage** - Tracks all coupon code usage\n7. **letter_audit_trail** - Complete audit logging\n\n**To set up your database, you have two options:**\n\n**Option 1: Using Supabase SQL Editor (Recommended)**\n1. Go to your Supabase project dashboard\n2. Click on \"SQL Editor\" in the left sidebar\n3. Click \"New Query\"\n4. Copy the entire contents of `/workspace/ttml_app/main-main/supabase/migrations/COMPLETE_MIGRATION.sql`\n5. Paste it into the SQL Editor\n6. Click \"Run\" to execute\n\n**Option 2: Using Supabase CLI**\nIf you have Supabase CLI installed locally:\n```bash\ncd /workspace/ttml_app/main-main\nsupabase db push supabase/migrations/COMPLETE_MIGRATION.sql\n```\n\nAfter running the migration, you'll have all tables created with:\n- Proper indexes for performance\n- Row Level Security (RLS) policies for data protection\n- Database functions for business logic\n- Triggers for automatic profile creation\n- Complete audit trail system\n\nWould you like me to help you with any specific part of the setup, or do you need the SQL file content to copy?\n</content>\n</RoleZero.reply_to_human>\n\n<end></end>",
                            "instruct_content": null,
                            "role": "assistant",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "36fc78b2042f457db7c7aa3e3a8defbc",
                            "timestamp": "20251126061348531",
                            "content": "Command RoleZero.reply_to_human executed: SUCCESS, human has received your reply. Refrain from resending duplicate messages.  If you no longer need to take action, use the command \u2018end\u2019 to stop.\n\nCommand end executed:",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.di.run_command.RunCommand",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        }
                    ],
                    "index": {
                        "metagpt.actions.add_requirement.UserRequirement": [
                            {
                                "id": "df14fbea5cb742d489435448bacf7ffe",
                                "timestamp": "20251126061240991",
                                "content": "[Message] from User to Mike: there are 0 tables ",
                                "instruct_content": null,
                                "role": "user",
                                "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                                "sent_from": "",
                                "send_to": [
                                    "<all>",
                                    "Mike"
                                ],
                                "metadata": {}
                            },
                            {
                                "id": "fd44fa9c7dae47b6877901e2654b3026",
                                "timestamp": "20251126061332241",
                                "content": "I need to understand the current state of the Supabase database and help set up the necessary tables for the \"Talk to My Lawyer\" application.\n\n<Terminal.run>\n<cmd>cd /workspace/ttml_app/main-main && find . -type f -name \"*.sql\" -o -name \"*migration*\" -o -name \"*schema*\" | grep -E \"\\.(sql|ts|js)$\" | head -20</cmd>\n</Terminal.run>\n\n<Terminal.run>\n<cmd>cd /workspace/ttml_app/main-main && find supabase -type f 2>/dev/null | head -20</cmd>\n</Terminal.run>\n\n<Editor.read>\n<path>/workspace/ttml_app/main-main/README.md</path>\n</Editor.read>\n\n<RoleZero.reply_to_human>\n<content>\nI understand you're seeing 0 tables in your Supabase database. Let me investigate the database schema and migration files to help you set up the necessary tables for the \"Talk to My Lawyer\" application. I'm checking the project structure to locate the database setup files.\n</content>\n</RoleZero.reply_to_human>",
                                "instruct_content": null,
                                "role": "assistant",
                                "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                                "sent_from": "",
                                "send_to": [
                                    "<all>"
                                ],
                                "metadata": {}
                            },
                            {
                                "id": "f5190485b8f64a9da3300715da24a645",
                                "timestamp": "20251126061338398",
                                "content": "I can see the project has comprehensive SQL migration files. Let me check the main migration file and help you set up the database tables.\n\n<Editor.read>\n<path>/workspace/ttml_app/main-main/supabase/migrations/COMPLETE_MIGRATION.sql</path>\n</Editor.read>\n\n<Terminal.run>\n<cmd>cd /workspace/ttml_app/main-main && ls -la supabase/migrations/ | head -20</cmd>\n</Terminal.run>\n\n<RoleZero.reply_to_human>\n<content>\nI found your database migration files! The project has a comprehensive set of SQL scripts to set up all necessary tables. There's a `COMPLETE_MIGRATION.sql` file that should contain all the schema setup.\n\nLet me read the migration file to understand the complete database structure, and then I'll guide you on how to apply these migrations to your Supabase database to create all the required tables.\n</content>\n</RoleZero.reply_to_human>",
                                "instruct_content": null,
                                "role": "assistant",
                                "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                                "sent_from": "",
                                "send_to": [
                                    "<all>"
                                ],
                                "metadata": {}
                            },
                            {
                                "id": "0939603518ca44b4adb3cfe0a84b2876",
                                "timestamp": "20251126061348530",
                                "content": "I can see you have a comprehensive `COMPLETE_MIGRATION.sql` file that contains all the necessary database schema. This file will create all 7 tables needed for your \"Talk to My Lawyer\" application. Let me help you apply this migration to your Supabase database.\n\n<RoleZero.reply_to_human>\n<content>\nI found your database migration file! The `COMPLETE_MIGRATION.sql` contains the complete schema that will create all 7 tables:\n\n1. **profiles** - User accounts with roles (subscriber, employee, admin)\n2. **employee_coupons** - Employee discount codes and tracking\n3. **subscriptions** - User subscriptions and letter credits\n4. **commissions** - Employee earnings tracking\n5. **letters** - Legal letters with status tracking\n6. **coupon_usage** - Tracks all coupon code usage\n7. **letter_audit_trail** - Complete audit logging\n\n**To set up your database, you have two options:**\n\n**Option 1: Using Supabase SQL Editor (Recommended)**\n1. Go to your Supabase project dashboard\n2. Click on \"SQL Editor\" in the left sidebar\n3. Click \"New Query\"\n4. Copy the entire contents of `/workspace/ttml_app/main-main/supabase/migrations/COMPLETE_MIGRATION.sql`\n5. Paste it into the SQL Editor\n6. Click \"Run\" to execute\n\n**Option 2: Using Supabase CLI**\nIf you have Supabase CLI installed locally:\n```bash\ncd /workspace/ttml_app/main-main\nsupabase db push supabase/migrations/COMPLETE_MIGRATION.sql\n```\n\nAfter running the migration, you'll have all tables created with:\n- Proper indexes for performance\n- Row Level Security (RLS) policies for data protection\n- Database functions for business logic\n- Triggers for automatic profile creation\n- Complete audit trail system\n\nWould you like me to help you with any specific part of the setup, or do you need the SQL file content to copy?\n</content>\n</RoleZero.reply_to_human>\n\n<end></end>",
                                "instruct_content": null,
                                "role": "assistant",
                                "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                                "sent_from": "",
                                "send_to": [
                                    "<all>"
                                ],
                                "metadata": {}
                            }
                        ],
                        "metagpt.actions.di.run_command.RunCommand": [
                            {
                                "id": "f53d7801471a4f0093be3647ae85c40e",
                                "timestamp": "20251126061332242",
                                "content": "Command Terminal.run executed:\n[command]: cd /workspace/ttml_app/main-main && find . -type f -name \"*.sql\" -o -name \"*migration*\" -o -name \"*schema*\" | grep -E \"\\.(sql|ts|js)$\" | head -20 \n[command output] : ./COMPLETE_MIGRATION.sql\n./fix_permissions.sql\n./main-main/COMPLETE_MIGRATION.sql\n./main-main/fix_permissions.sql\n./main-main/scripts/001_setup_schema.sql\n./main-main/scripts/002_setup_rls.sql\n./main-main/scripts/003_seed_data.sql\n./main-main/scripts/004_create_functions.sql\n./main-main/scripts/005_letter_allowance_system.sql\n./main-main/scripts/006_audit_trail.sql\n./main-main/scripts/007_add_missing_letter_statuses.sql\n./main-main/scripts/008_employee_coupon_auto_generation.sql\n./main-main/scripts/009_add_missing_subscription_fields.sql\n./main-main/scripts/010_add_missing_functions.sql\n./main-main/scripts/011_security_hardening.sql\n./main-main/supabase/migrations/001_setup_schema.sql\n./main-main/supabase/migrations/002_setup_rls.sql\n./main-main/supabase/migrations/003_seed_data.sql\n./main-main/supabase/migrations/004_create_functions.sql\n./main-main/supabase/migrations/005_letter_allowance_system.sql\n\n\nCommand Terminal.run executed:\n[command]: cd /workspace/ttml_app/main-main && find supabase -type f 2>/dev/null | head -20 \n[command output] : \nsupabase/functions/generate-letter/index.ts\nsupabase/migrations/001_setup_schema.sql\nsupabase/migrations/002_setup_rls.sql\nsupabase/migrations/003_seed_data.sql\nsupabase/migrations/004_create_functions.sql\nsupabase/migrations/005_letter_allowance_system.sql\nsupabase/migrations/006_audit_trail.sql\nsupabase/migrations/007_add_missing_letter_statuses.sql\nsupabase/migrations/008_employee_coupon_auto_generation.sql\nsupabase/migrations/009_add_missing_subscription_fields.sql\nsupabase/migrations/010_add_missing_functions.sql\nsupabase/migrations/010_admin_role_rls.sql\nsupabase/migrations/011_security_hardening.sql\nsupabase/migrations/012_create_coupon_usage_table.sql\nsupabase/migrations/20251122000003_add_missing_functions.sql\nsupabase/migrations/20251124000001_finalize_rls_policies.sql\nsupabase/migrations/COMPLETE_MIGRATION.sql\n\n\nCommand Editor.read executed: path='/workspace/ttml_app/main-main/README.md' content='001|# Talk-To-My-Lawyer - Legal Letter Generation Platform\\n002|\\n003|A production-ready SaaS platform for generating legal letters with AI assistance and attorney review. Built with Next.js 16, Supabase, and OpenAI GPT-4.\\n004|\\n005|## \ud83c\udfd7\ufe0f Architecture\\n006|\\n007|### Tech Stack\\n008|- **Frontend**: Next.js 16 (App Router), TypeScript, Tailwind CSS\\n009|- **Backend**: Supabase (PostgreSQL), Next.js API Routes\\n010|- **AI**: OpenAI GPT-4 for letter generation and improvement\\n011|- **Authentication**: Supabase Auth with role-based access control\\n012|- **Database**: PostgreSQL with comprehensive RLS policies\\n013|- **Deployment**: Vercel-ready with CI/CD support\\n014|\\n015|### User Roles\\n016|1. **Subscriber**: Creates and manages legal letters\\n017|2. **Employee**: Manages coupon codes and earns commissions\\n018|3. **Admin**: Reviews letters, manages users and commissions\\n019|\\n020|## \ud83d\ude80 Quick Start\\n021|\\n022|### Prerequisites\\n023|- Node.js 18+\\n024|- Supabase account and project\\n025|- OpenAI API key\\n026|\\n027|### Installation\\n028|\\n029|1. **Clone and install dependencies**\\n030|```bash\\n031|git clone <repository-url>\\n032|cd talk-to-my-lawyer\\n033|npm install\\n034|```\\n035|\\n036|2. **Configure environment variables**\\n037|```bash\\n038|cp .env.example .env.local\\n039|# Fill in your environment variables\\n040|```\\n041|\\n042|3. **Set up database**\\n043|```bash\\n044|# Run SQL scripts in order against your Supabase project\\n045|supabase db push scripts/001_setup_schema.sql\\n046|supabase db push scripts/002_setup_rls.sql\\n047|supabase db push scripts/003_seed_data.sql\\n048|supabase db push scripts/004_create_functions.sql\\n049|supabase db push scripts/005_letter_allowance_system.sql\\n050|supabase db push scripts/006_audit_trail.sql\\n051|supabase db push scripts/007_add_missing_letter_statuses.sql\\n052|supabase db push scripts/008_employee_coupon_auto_generation.sql\\n053|supabase db push scripts/009_add_missing_subscription_fields.sql\\n054|supabase db push scripts/010_add_missing_functions.sql\\n055|supabase db push scripts/011_security_hardening.sql\\n056|```\\n057|\\n058|4. **Start development server**\\n059|```bash\\n060|npm run dev\\n061|```\\n062|\\n063|Visit `http://localhost:3000` to see the application.\\n064|\\n065|## \ud83d\udccb Environment Variables\\n066|\\n067|Required environment variables (see `.env.example`):\\n068|\\n069|```bash\\n070|# Base URLs\\n071|NEXT_PUBLIC_APP_URL=https://your-domain.com\\n072|NEXT_PUBLIC_DEV_SUPABASE_REDIRECT_URL=http://localhost:3000\\n073|\\n074|# Supabase Configuration\\n075|NEXT_PUBLIC_SUPABASE_URL=your-supabase-url\\n076|NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key\\n077|SUPABASE_SERVICE_ROLE_KEY=your-service-role-key\\n078|\\n079|# AI Configuration\\n080|OPENAI_API_KEY=your-openai-api-key\\n081|\\n082|# Security\\n083|CRON_SECRET=your-cron-secret-key\\n084|```\\n085|\\n086|## \ud83d\udcb3 Pricing Model\\n087|\\n088|- **Single Letter**: $299 (one-time)\\n089|- **Monthly Plan**: $299/month (4 letters)\\n090|- **Yearly Plan**: $599/year (8 letters)\\n091|\\n092|## \ud83d\udc65 User Flow\\n093|\\n094|### Subscriber Journey\\n095|1. **Free Trial**: First letter is free with AI generation\\n096|2. **Subscription Purchase**: Choose plan via Stripe checkout\\n097|3. **Letter Creation**: Fill intake form, AI generates draft\\n098|4. **Attorney Review**: Admin reviews and improves with AI assistance\\n099|5. **Delivery**: Download PDF or send via email\\n100|\\n101|### Employee Journey\\n102|1. **Coupon Assignment**: Auto-generated unique coupon code\\n103|2. **Sharing**: Distribute 20% discount codes to potential customers\\n104|3. **Commission**: Earn 5% on each successful subscription\\n105|4. **Tracking**: Monitor usage, earnings, and points\\n106|\\n107|### Admin Journey\\n108|1. **Letter Review**: Review and approve/reject subscriber letters\\n109|2. **AI Improvement**: Use OpenAI GPT-4 to enhance letter quality\\n110|3. **User Management**: Manage subscribers, employees, and roles\\n111|4. **Commission Management**: Track and process employee commissions\\n112|5. **Analytics**: Monitor platform usage and revenue\\n113|\\n114|## \ud83d\udd12 Security Features\\n115|\\n116|- **Row Level Security**: Complete data isolation between roles\\n117|- **Employee Isolation**: Employees cannot access any letter content\\n118|- **Audit Trail**: Complete logging of all letter status changes\\n119|- **Input Validation**: Comprehensive data sanitization and validation\\n120|- **API Security**: Authentication on all endpoints, proper error handling\\n121|- **Environment Security**: Proper secrets management and configuration\\n122|\\n123|## \ud83d\udcca Key Features\\n124|\\n125|### Core Functionality\\n126|- \u2705 AI-powered letter generation (OpenAI GPT-4)\\n127|- \u2705 Professional attorney review and AI improvement\\n128|- \u2705 PDF generation and email delivery\\n129|- \u2705 Complete audit trail and logging\\n130|- \u2705 Role-based dashboards and access control\\n131|\\n132|### Business Features\\n133|- \u2705 Free trial system\\n134|- \u2705 Subscription management with Stripe\\n135|- \u2705 Employee coupon and commission system\\n136|- \u2705 Monthly letter allowance tracking\\n137|- \u2705 Comprehensive analytics and reporting\\n138|\\n139|### Technical Features\\n140|- \u2705 TypeScript throughout\\n141|- \u2705 Responsive design with Tailwind CSS\\n142|- \u2705 Database functions and triggers\\n143|- \u2705 Security hardening and best practices\\n144|- \u2705 Production-ready deployment setup\\n145|\\n146|## \ud83d\uddc4\ufe0f Database Schema\\n147|\\n148|### Main Tables\\n149|- `profiles`: User accounts with roles\\n150|- `letters`: Legal letters with status tracking\\n151|- `subscriptions`: User subscriptions and credits\\n152|- `employee_coupons`: Employee discount codes\\n153|- `commissions`: Employee earnings tracking\\n154|- `letter_audit_trail`: Complete audit logging\\n155|\\n156|### Key Functions\\n157|- `deduct_letter_allowance`: Manage letter credits\\n158|- `add_letter_allowances`: Add credits on subscription\\n159|- `reset_monthly_allowances`: Monthly credit reset\\n160|- `log_letter_audit`: Audit trail logging\\n161|- `validate_coupon`: Coupon validation\\n162|\\n163|## \ud83d\ude80 Deployment\\n164|\\n165|### Vercel Deployment (Recommended)\\n166|\\n167|1. **Connect to Vercel**\\n168|```bash\\n169|npm i -g vercel\\n170|vercel\\n171|```\\n172|\\n173|2. **Configure Environment Variables**\\n174|Add all required environment variables to Vercel dashboard\\n175|\\n176|3. **Deploy**\\n177|```bash\\n178|vercel --prod\\n179|```\\n180|\\n181|### Manual Deployment\\n182|\\n183|1. **Build application**\\n184|```bash\\n185|npm run build\\n186|```\\n187|\\n188|2. **Start production server**\\n189|```bash\\n190|npm start\\n191|```\\n192|\\n193|## \ud83d\udd04 Cron Jobs\\n194|\\n195|Set up monthly subscription reset:\\n196|\\n197|```bash\\n198|# Add to your cron tab (runs 1st of each month at midnight UTC)\\n199|0 0 1 * * curl -X POST https://your-domain.com/api/subscriptions/reset-monthly \\\\\\n200|  -H \"Authorization: Bearer $CRON_SECRET\"\\n201|```\\n202|\\n203|## \ud83d\udcc8 Monitoring & Analytics\\n204|\\n205|- **User Analytics**: Track registration, subscription, and usage metrics\\n206|- **Revenue Analytics**: Monitor subscription revenue and commission payouts\\n207|- **Letter Analytics**: Track letter generation, approval rates, and types\\n208|- **Security Monitoring**: Comprehensive audit trail and security event logging\\n209|\\n210|## \ud83d\udee0\ufe0f Development\\n211|\\n212|### Database Migrations\\n213|All database changes are managed through SQL scripts in the `scripts/` directory. Scripts must be run in numerical order.\\n214|\\n215|### Adding New Features\\n216|1. Follow the established patterns in existing components\\n217|2. Ensure proper RLS policies for any new tables\\n218|3. Add comprehensive error handling and logging\\n219|4. Update audit trails for any status changes\\n220|5. Test with all user roles (subscriber, employee, admin)\\n221|\\n222|### Code Quality\\n223|- TypeScript for type safety\\n224|- Comprehensive error handling\\n225|- Security-first development approach\\n226|- Responsive design principles\\n227|- Performance optimization\\n228|\\n229|## \ud83e\udd1d Support\\n230|\\n231|For technical support or questions:\\n232|- Review the comprehensive documentation in the `/docs` directory\\n233|- Check the security checklist for security-related questions\\n234|- Monitor audit logs for troubleshooting user issues\\n235|\\n236|---\\n237|\\n238|**Built with \u2764\ufe0f for democratizing access to professional legal services.**'\n\nCommand RoleZero.reply_to_human executed: SUCCESS, human has received your reply. Refrain from resending duplicate messages.  If you no longer need to take action, use the command \u2018end\u2019 to stop.",
                                "instruct_content": null,
                                "role": "user",
                                "cause_by": "metagpt.actions.di.run_command.RunCommand",
                                "sent_from": "",
                                "send_to": [
                                    "<all>"
                                ],
                                "metadata": {}
                            },
                            {
                                "id": "d48aa9c8d4f44aef89c1a1c8a755e2c1",
                                "timestamp": "20251126061338399",
                                "content": "Command Editor.read executed: path='/workspace/ttml_app/main-main/supabase/migrations/COMPLETE_MIGRATION.sql' content='001|-- ============================================================================\\n002|-- COMPLETE MIGRATION FOR TALK-TO-MY-LAWYER\\n003|-- ============================================================================\\n004|-- This file combines all migrations in the correct order for easy deployment\\n005|-- Run this on a fresh Supabase database to set up the complete schema\\n006|-- ============================================================================\\n007|\\n008|-- TASK 9: Complete migration file combining all migrations\\n009|\\n010|-- ============================================================================\\n011|-- 001: SETUP SCHEMA\\n012|-- ============================================================================\\n013|\\n014|-- Enable extensions\\n015|CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\\n016|\\n017|-- Create ENUM types for type safety\\n018|DO $$ BEGIN\\n019|    CREATE TYPE user_role AS ENUM (\\'subscriber\\', \\'employee\\', \\'admin\\');\\n020|EXCEPTION\\n021|    WHEN duplicate_object THEN null;\\n022|END $$;\\n023|\\n024|DO $$ BEGIN\\n025|    CREATE TYPE letter_status AS ENUM (\\n026|        \\'draft\\', \\n027|        \\'generating\\', \\n028|        \\'pending_review\\', \\n029|        \\'under_review\\', \\n030|        \\'approved\\', \\n031|        \\'rejected\\', \\n032|        \\'completed\\', \\n033|        \\'failed\\'\\n034|    );\\n035|EXCEPTION\\n036|    WHEN duplicate_object THEN null;\\n037|END $$;\\n038|\\n039|DO $$ BEGIN\\n040|    CREATE TYPE subscription_status AS ENUM (\\'active\\', \\'canceled\\', \\'past_due\\');\\n041|EXCEPTION\\n042|    WHEN duplicate_object THEN null;\\n043|END $$;\\n044|\\n045|DO $$ BEGIN\\n046|    CREATE TYPE commission_status AS ENUM (\\'pending\\', \\'paid\\');\\n047|EXCEPTION\\n048|    WHEN duplicate_object THEN null;\\n049|END $$;\\n050|\\n051|-- Create profiles table\\n052|CREATE TABLE IF NOT EXISTS profiles (\\n053|    id UUID PRIMARY KEY REFERENCES auth.users(id) ON DELETE CASCADE,\\n054|    email TEXT NOT NULL UNIQUE,\\n055|    full_name TEXT,\\n056|    role user_role DEFAULT \\'subscriber\\',\\n057|    phone TEXT,\\n058|    company_name TEXT,\\n059|    is_super_user BOOLEAN DEFAULT false,\\n060|    created_at TIMESTAMPTZ DEFAULT NOW(),\\n061|    updated_at TIMESTAMPTZ DEFAULT NOW()\\n062|);\\n063|\\n064|-- Create employee_coupons table\\n065|CREATE TABLE IF NOT EXISTS employee_coupons (\\n066|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n067|    employee_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n068|    code TEXT NOT NULL UNIQUE,\\n069|    discount_percent INT CHECK (discount_percent BETWEEN 0 AND 100) DEFAULT 20,\\n070|    is_active BOOLEAN DEFAULT true,\\n071|    usage_count INT DEFAULT 0,\\n072|    created_at TIMESTAMPTZ DEFAULT NOW(),\\n073|    updated_at TIMESTAMPTZ DEFAULT NOW()\\n074|);\\n075|\\n076|-- Create subscriptions table\\n077|CREATE TABLE IF NOT EXISTS subscriptions (\\n078|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n079|    user_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n080|    status subscription_status DEFAULT \\'active\\',\\n081|    coupon_code TEXT,\\n082|    plan TEXT DEFAULT \\'single_letter\\',\\n083|    plan_type TEXT,\\n084|    price NUMERIC(10,2) DEFAULT 299.00,\\n085|    discount NUMERIC(10,2) DEFAULT 0.00,\\n086|    stripe_subscription_id TEXT,\\n087|    remaining_letters INT DEFAULT 0,\\n088|    credits_remaining INT DEFAULT 0,\\n089|    last_reset_at TIMESTAMPTZ,\\n090|    current_period_start TIMESTAMPTZ,\\n091|    current_period_end TIMESTAMPTZ,\\n092|    created_at TIMESTAMPTZ DEFAULT NOW(),\\n093|    updated_at TIMESTAMPTZ DEFAULT NOW()\\n094|);\\n095|\\n096|-- Create commissions table\\n097|CREATE TABLE IF NOT EXISTS commissions (\\n098|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n099|    employee_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n100|    subscription_id UUID NOT NULL REFERENCES subscriptions(id) ON DELETE CASCADE,\\n101|    commission_rate NUMERIC(5,4) DEFAULT 0.05,\\n102|    subscription_amount NUMERIC(10,2) NOT NULL DEFAULT 0,\\n103|    commission_amount NUMERIC(10,2) NOT NULL DEFAULT 0,\\n104|    status commission_status DEFAULT \\'pending\\',\\n105|    paid_at TIMESTAMPTZ,\\n106|    created_at TIMESTAMPTZ DEFAULT NOW()\\n107|);\\n108|\\n109|-- Create letters table\\n110|CREATE TABLE IF NOT EXISTS letters (\\n111|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n112|    user_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n113|    title TEXT NOT NULL,\\n114|    status letter_status DEFAULT \\'draft\\',\\n115|    letter_type TEXT,\\n116|    intake_data JSONB DEFAULT \\'{}\\',\\n117|    ai_draft_content TEXT,\\n118|    final_content TEXT,\\n119|    reviewed_by UUID REFERENCES profiles(id),\\n120|    reviewed_at TIMESTAMPTZ,\\n121|    review_notes TEXT,\\n122|    rejection_reason TEXT,\\n123|    approved_at TIMESTAMPTZ,\\n124|    created_at TIMESTAMPTZ DEFAULT NOW(),\\n125|    updated_at TIMESTAMPTZ DEFAULT NOW()\\n126|);\\n127|\\n128|-- Create coupon_usage table (TASK 2)\\n129|CREATE TABLE IF NOT EXISTS coupon_usage (\\n130|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n131|    user_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n132|    coupon_code TEXT NOT NULL,\\n133|    employee_id UUID REFERENCES profiles(id) ON DELETE SET NULL,\\n134|    discount_percent INT CHECK (discount_percent BETWEEN 0 AND 100),\\n135|    amount_before NUMERIC(10,2) NOT NULL,\\n136|    amount_after NUMERIC(10,2) NOT NULL,\\n137|    created_at TIMESTAMPTZ DEFAULT NOW()\\n138|);\\n139|\\n140|-- Create letter_audit_trail table\\n141|CREATE TABLE IF NOT EXISTS letter_audit_trail (\\n142|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n143|    letter_id UUID NOT NULL REFERENCES letters(id) ON DELETE CASCADE,\\n144|    action TEXT NOT NULL,\\n145|    old_status TEXT,\\n146|    new_status TEXT,\\n147|    performed_by UUID REFERENCES profiles(id),\\n148|    notes TEXT,\\n149|    created_at TIMESTAMPTZ DEFAULT NOW()\\n150|);\\n151|\\n152|-- Create indexes for performance\\n153|CREATE INDEX IF NOT EXISTS idx_profiles_role ON profiles(role);\\n154|CREATE INDEX IF NOT EXISTS idx_profiles_email ON profiles(email);\\n155|CREATE INDEX IF NOT EXISTS idx_employee_coupons_code ON employee_coupons(code);\\n156|CREATE INDEX IF NOT EXISTS idx_employee_coupons_employee ON employee_coupons(employee_id);\\n157|CREATE INDEX IF NOT EXISTS idx_letters_user_id ON letters(user_id);\\n158|CREATE INDEX IF NOT EXISTS idx_letters_status ON letters(status);\\n159|CREATE INDEX IF NOT EXISTS idx_commissions_employee ON commissions(employee_id);\\n160|CREATE INDEX IF NOT EXISTS idx_subscriptions_user ON subscriptions(user_id);\\n161|CREATE INDEX IF NOT EXISTS idx_coupon_usage_user ON coupon_usage(user_id);\\n162|CREATE INDEX IF NOT EXISTS idx_coupon_usage_code ON coupon_usage(coupon_code);\\n163|CREATE INDEX IF NOT EXISTS idx_coupon_usage_employee ON coupon_usage(employee_id);\\n164|CREATE INDEX IF NOT EXISTS idx_letter_audit_letter ON letter_audit_trail(letter_id);\\n165|\\n166|-- ============================================================================\\n167|-- 002: SETUP RLS POLICIES\\n168|-- ============================================================================\\n169|\\n170|-- Enable RLS on all tables\\n171|ALTER TABLE profiles ENABLE ROW LEVEL SECURITY;\\n172|ALTER TABLE employee_coupons ENABLE ROW LEVEL SECURITY;\\n173|ALTER TABLE subscriptions ENABLE ROW LEVEL SECURITY;\\n174|ALTER TABLE commissions ENABLE ROW LEVEL SECURITY;\\n175|ALTER TABLE letters ENABLE ROW LEVEL SECURITY;\\n176|ALTER TABLE coupon_usage ENABLE ROW LEVEL SECURITY;\\n177|ALTER TABLE letter_audit_trail ENABLE ROW LEVEL SECURITY;\\n178|\\n179|-- Helper function to get user role\\n180|CREATE OR REPLACE FUNCTION get_user_role(user_id UUID)\\n181|RETURNS TEXT AS $$\\n182|  SELECT role::TEXT FROM profiles WHERE id = user_id;\\n183|$$ LANGUAGE SQL STABLE SECURITY DEFINER;\\n184|\\n185|-- Profiles policies\\n186|CREATE POLICY \"Users can view own profile\" ON profiles FOR SELECT USING (auth.uid() = id);\\n187|CREATE POLICY \"Users can update own profile\" ON profiles FOR UPDATE USING (auth.uid() = id);\\n188|CREATE POLICY \"Admins can view all profiles\" ON profiles FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n189|\\n190|-- Letters policies\\n191|CREATE POLICY \"Users can view own letters\" ON letters FOR SELECT USING (auth.uid() = user_id);\\n192|CREATE POLICY \"Users can insert own letters\" ON letters FOR INSERT WITH CHECK (auth.uid() = user_id);\\n193|CREATE POLICY \"Users can update own letters\" ON letters FOR UPDATE USING (auth.uid() = user_id);\\n194|CREATE POLICY \"Admins can view all letters\" ON letters FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n195|CREATE POLICY \"Admins can update all letters\" ON letters FOR UPDATE USING (get_user_role(auth.uid()) = \\'admin\\');\\n196|\\n197|-- Subscriptions policies\\n198|CREATE POLICY \"Users can view own subscriptions\" ON subscriptions FOR SELECT USING (auth.uid() = user_id);\\n199|CREATE POLICY \"Admins can view all subscriptions\" ON subscriptions FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n200|\\n201|-- Employee coupons policies\\n202|CREATE POLICY \"Employees can view own coupons\" ON employee_coupons FOR SELECT USING (auth.uid() = employee_id);\\n203|CREATE POLICY \"Anyone can check coupon validity\" ON employee_coupons FOR SELECT USING (true);\\n204|CREATE POLICY \"Admins can manage all coupons\" ON employee_coupons FOR ALL USING (get_user_role(auth.uid()) = \\'admin\\');\\n205|\\n206|-- Commissions policies\\n207|CREATE POLICY \"Employees can view own commissions\" ON commissions FOR SELECT USING (auth.uid() = employee_id);\\n208|CREATE POLICY \"Admins can view all commissions\" ON commissions FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n209|\\n210|-- Coupon usage policies\\n211|CREATE POLICY \"Users can view own coupon usage\" ON coupon_usage FOR SELECT USING (auth.uid() = user_id);\\n212|CREATE POLICY \"Admins can view all coupon usage\" ON coupon_usage FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n213|\\n214|-- Audit trail policies\\n215|CREATE POLICY \"Users can view audit for own letters\" ON letter_audit_trail FOR SELECT \\n216|  USING (EXISTS (SELECT 1 FROM letters WHERE letters.id = letter_audit_trail.letter_id AND letters.user_id = auth.uid()));\\n217|CREATE POLICY \"Admins can view all audit trails\" ON letter_audit_trail FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n218|\\n219|-- ============================================================================\\n220|-- 004: CREATE FUNCTIONS\\n221|-- ============================================================================\\n222|\\n223|-- Function to check letter allowance\\n224|CREATE OR REPLACE FUNCTION check_letter_allowance(u_id UUID)\\n225|RETURNS TABLE (\\n226|  has_allowance BOOLEAN,\\n227|  remaining INT,\\n228|  plan_name TEXT,\\n229|  is_super BOOLEAN\\n230|) AS $$\\n231|DECLARE\\n232|  v_is_super BOOLEAN;\\n233|  v_remaining INT;\\n234|  v_plan TEXT;\\n235|BEGIN\\n236|  -- Check if user is super user\\n237|  SELECT is_super_user INTO v_is_super FROM profiles WHERE id = u_id;\\n238|  \\n239|  IF v_is_super THEN\\n240|    RETURN QUERY SELECT true, 999999, \\'Super User\\'::TEXT, true;\\n241|    RETURN;\\n242|  END IF;\\n243|  \\n244|  -- Check active subscription\\n245|  SELECT \\n246|    COALESCE(credits_remaining, 0),\\n247|    COALESCE(plan, \\'None\\')\\n248|  INTO v_remaining, v_plan\\n249|  FROM subscriptions\\n250|  WHERE user_id = u_id AND status = \\'active\\'\\n251|  ORDER BY created_at DESC\\n252|  LIMIT 1;\\n253|  \\n254|  IF v_remaining > 0 THEN\\n255|    RETURN QUERY SELECT true, v_remaining, v_plan, false;\\n256|  ELSE\\n257|    RETURN QUERY SELECT false, 0, COALESCE(v_plan, \\'None\\'::TEXT), false;\\n258|  END IF;\\n259|END;\\n260|$$ LANGUAGE plpgsql SECURITY DEFINER;\\n261|\\n262|-- Function to log audit trail\\n263|CREATE OR REPLACE FUNCTION log_letter_audit(\\n264|  p_letter_id UUID,\\n265|  p_action TEXT,\\n266|  p_old_status TEXT DEFAULT NULL,\\n267|  p_new_status TEXT DEFAULT NULL,\\n268|  p_notes TEXT DEFAULT NULL\\n269|)\\n270|RETURNS UUID AS $$\\n271|DECLARE\\n272|  v_audit_id UUID;\\n273|BEGIN\\n274|  INSERT INTO letter_audit_trail (\\n275|    letter_id,\\n276|    action,\\n277|    old_status,\\n278|    new_status,\\n279|    performed_by,\\n280|    notes\\n281|  ) VALUES (\\n282|    p_letter_id,\\n283|    p_action,\\n284|    p_old_status,\\n285|    p_new_status,\\n286|    auth.uid(),\\n287|    p_notes\\n288|  ) RETURNING id INTO v_audit_id;\\n289|  \\n290|  RETURN v_audit_id;\\n291|END;\\n292|$$ LANGUAGE plpgsql SECURITY DEFINER;\\n293|\\n294|-- ============================================================================\\n295|-- TRIGGERS\\n296|-- ============================================================================\\n297|\\n298|-- Trigger to auto-create profile on signup\\n299|CREATE OR REPLACE FUNCTION public.handle_new_user()\\n300|RETURNS TRIGGER AS $$\\n301|BEGIN\\n302|    INSERT INTO public.profiles (id, email, full_name)\\n303|    VALUES (NEW.id, NEW.email, NEW.raw_user_meta_data->>\\'full_name\\');\\n304|    RETURN NEW;\\n305|END;\\n306|$$ LANGUAGE plpgsql SECURITY DEFINER;\\n307|\\n308|DROP TRIGGER IF EXISTS on_auth_user_created ON auth.users;\\n309|CREATE TRIGGER on_auth_user_created\\n310|    AFTER INSERT ON auth.users\\n311|    FOR EACH ROW EXECUTE FUNCTION public.handle_new_user();\\n312|\\n313|-- ============================================================================\\n314|-- COMMENTS\\n315|-- ============================================================================\\n316|\\n317|COMMENT ON TABLE coupon_usage IS \\'Tracks all coupon code usage including TALK3 and employee coupons\\';\\n318|COMMENT ON COLUMN coupon_usage.employee_id IS \\'Employee who owns the coupon, NULL for special codes like TALK3\\';\\n319|COMMENT ON COLUMN coupon_usage.discount_percent IS \\'Percentage discount applied (0-100)\\';\\n320|COMMENT ON COLUMN profiles.is_super_user IS \\'Business flag for unlimited letter allowances, NOT an admin role\\';\\n321|\\n322|-- ============================================================================\\n323|-- COMPLETION\\n324|-- ============================================================================\\n325|\\n326|-- Migration complete\\n327|SELECT \\'Talk-To-My-Lawyer database schema setup complete!\\' as status;'\n\nCommand Terminal.run executed:\n[command]: cd /workspace/ttml_app/main-main && ls -la supabase/migrations/ | head -20 \n[command output] : \ntotal 300\ndrwxrwxr-x 18 root root 110592 Nov 26 01:34 .\ndrwxrwxr-x 22 root root 126976 Nov 26 01:34 ..\n-rw-rw-r--  1 root root   4661 Nov 26 01:34 001_setup_schema.sql\n-rw-rw-r--  1 root root   5010 Nov 26 01:34 002_setup_rls.sql\n-rw-rw-r--  1 root root    662 Nov 26 01:34 003_seed_data.sql\n-rw-rw-r--  1 root root   1635 Nov 26 01:34 004_create_functions.sql\n-rw-rw-r--  1 root root   2966 Nov 26 01:34 005_letter_allowance_system.sql\n-rw-rw-r--  1 root root   1748 Nov 26 01:34 006_audit_trail.sql\n-rw-rw-r--  1 root root    761 Nov 26 01:34 007_add_missing_letter_statuses.sql\n-rw-rw-r--  1 root root    769 Nov 26 01:34 008_employee_coupon_auto_generation.sql\n-rw-rw-r--  1 root root   1443 Nov 26 01:34 009_add_missing_subscription_fields.sql\n-rw-rw-r--  1 root root   2692 Nov 26 01:34 010_add_missing_functions.sql\n-rw-rw-r--  1 root root   7813 Nov 26 01:34 010_admin_role_rls.sql\n-rw-rw-r--  1 root root   4539 Nov 26 01:34 011_security_hardening.sql\n-rw-rw-r--  1 root root   1257 Nov 26 01:34 012_create_coupon_usage_table.sql\n-rw-rw-r--  1 root root   8705 Nov 26 01:34 20251122000003_add_missing_functions.sql\n-rw-rw-r--  1 root root   7412 Nov 26 01:34 20251124000001_finalize_rls_policies.sql\n-rw-rw-r--  1 root root  12409 Nov 26 01:34 COMPLETE_MIGRATION.sql\n\n\nCommand RoleZero.reply_to_human executed: SUCCESS, human has received your reply. Refrain from resending duplicate messages.  If you no longer need to take action, use the command \u2018end\u2019 to stop.",
                                "instruct_content": null,
                                "role": "user",
                                "cause_by": "metagpt.actions.di.run_command.RunCommand",
                                "sent_from": "",
                                "send_to": [
                                    "<all>"
                                ],
                                "metadata": {}
                            },
                            {
                                "id": "36fc78b2042f457db7c7aa3e3a8defbc",
                                "timestamp": "20251126061348531",
                                "content": "Command RoleZero.reply_to_human executed: SUCCESS, human has received your reply. Refrain from resending duplicate messages.  If you no longer need to take action, use the command \u2018end\u2019 to stop.\n\nCommand end executed:",
                                "instruct_content": null,
                                "role": "user",
                                "cause_by": "metagpt.actions.di.run_command.RunCommand",
                                "sent_from": "",
                                "send_to": [
                                    "<all>"
                                ],
                                "metadata": {}
                            }
                        ]
                    },
                    "ignore_id": false,
                    "appendix": null,
                    "buffer": [
                        {
                            "id": "35cca740059444188b1a9c2c91664ce9",
                            "timestamp": "20251126060539155",
                            "content": "# Project Summary\nThe \"Talk to My Lawyer\" project is a SaaS application designed to facilitate legal document generation and review. It supports three user roles: Subscribers who generate letters, Employees who assist with sales and earn commissions through coupon codes, and Admins who review and approve the generated letters. The platform leverages AI for drafting legal documents and integrates with payment processors for subscriptions.\n\n# Project Module Description\nThe application comprises several functional modules:\n- **User Authentication**: Manages user roles and sessions.\n- **Letter Generation**: Allows subscribers to create legal letters using AI.\n- **Subscription Management**: Handles user subscriptions and coupon codes.\n- **Admin Review**: Provides a dashboard for admins to review, edit, and approve letters.\n- **Email Notifications**: Sends emails to users regarding their letters and subscriptions.\n- **PDF Generation**: Converts approved letters into downloadable PDF format.\n\n# Directory Tree\n```\nttml_app/\n\u251c\u2500\u2500 main-main/\n\u2502   \u251c\u2500\u2500 app/\n\u2502   \u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 dashboard/\n\u2502   \u2502   \u251c\u2500\u2500 globals.css\n\u2502   \u2502   \u2514\u2500\u2500 layout.tsx\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u251c\u2500\u2500 supabase/\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 styles/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 package.json\n```\n\n# File Description Inventory\n- `app/api/`: Contains API routes for handling requests related to user authentication, letter generation, and subscriptions.\n- `app/dashboard/`: Contains components for the admin and user dashboards, including letter management and analytics.\n- `components/`: Reusable UI components used throughout the application.\n- `supabase/`: Contains database migrations and functions related to the Supabase backend.\n- `scripts/`: Includes scripts for database setup and administration tasks.\n- `styles/`: Contains global styles for the application.\n- `README.md`: Documentation for the project setup and usage.\n\n# Technology Stack\n- **Frontend**: Next.js 14, TypeScript, Tailwind CSS\n- **Backend**: Supabase, OpenAI GPT-4\n- **Payment Processing**: Stripe\n- **Email Service**: Resend\n\n# Usage\n1. **Install Dependencies**: Run the command to install required packages.\n2. **Build the Application**: Use the build command to compile the application.\n3. **Run the Application**: Start the application server for local development.\n\n\n# INSTRUCTION\n- You can search for the file path in the `/workspace/`\n- After modifying the project files, if this project can be previewed, then you need to reinstall dependencies, restart service and preview;\n\n\nChange the working directory to `/workspace/ttml_app/main-main`.",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "df14fbea5cb742d489435448bacf7ffe",
                            "timestamp": "20251126061240991",
                            "content": "[Message] from User to Mike: there are 0 tables ",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>",
                                "Mike"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "fd44fa9c7dae47b6877901e2654b3026",
                            "timestamp": "20251126061332241",
                            "content": "I need to understand the current state of the Supabase database and help set up the necessary tables for the \"Talk to My Lawyer\" application.\n\n<Terminal.run>\n<cmd>cd /workspace/ttml_app/main-main && find . -type f -name \"*.sql\" -o -name \"*migration*\" -o -name \"*schema*\" | grep -E \"\\.(sql|ts|js)$\" | head -20</cmd>\n</Terminal.run>\n\n<Terminal.run>\n<cmd>cd /workspace/ttml_app/main-main && find supabase -type f 2>/dev/null | head -20</cmd>\n</Terminal.run>\n\n<Editor.read>\n<path>/workspace/ttml_app/main-main/README.md</path>\n</Editor.read>\n\n<RoleZero.reply_to_human>\n<content>\nI understand you're seeing 0 tables in your Supabase database. Let me investigate the database schema and migration files to help you set up the necessary tables for the \"Talk to My Lawyer\" application. I'm checking the project structure to locate the database setup files.\n</content>\n</RoleZero.reply_to_human>",
                            "instruct_content": null,
                            "role": "assistant",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "f53d7801471a4f0093be3647ae85c40e",
                            "timestamp": "20251126061332242",
                            "content": "Command Terminal.run executed:\n[command]: cd /workspace/ttml_app/main-main && find . -type f -name \"*.sql\" -o -name \"*migration*\" -o -name \"*schema*\" | grep -E \"\\.(sql|ts|js)$\" | head -20 \n[command output] : ./COMPLETE_MIGRATION.sql\n./fix_permissions.sql\n./main-main/COMPLETE_MIGRATION.sql\n./main-main/fix_permissions.sql\n./main-main/scripts/001_setup_schema.sql\n./main-main/scripts/002_setup_rls.sql\n./main-main/scripts/003_seed_data.sql\n./main-main/scripts/004_create_functions.sql\n./main-main/scripts/005_letter_allowance_system.sql\n./main-main/scripts/006_audit_trail.sql\n./main-main/scripts/007_add_missing_letter_statuses.sql\n./main-main/scripts/008_employee_coupon_auto_generation.sql\n./main-main/scripts/009_add_missing_subscription_fields.sql\n./main-main/scripts/010_add_missing_functions.sql\n./main-main/scripts/011_security_hardening.sql\n./main-main/supabase/migrations/001_setup_schema.sql\n./main-main/supabase/migrations/002_setup_rls.sql\n./main-main/supabase/migrations/003_seed_data.sql\n./main-main/supabase/migrations/004_create_functions.sql\n./main-main/supabase/migrations/005_letter_allowance_system.sql\n\n\nCommand Terminal.run executed:\n[command]: cd /workspace/ttml_app/main-main && find supabase -type f 2>/dev/null | head -20 \n[command output] : \nsupabase/functions/generate-letter/index.ts\nsupabase/migrations/001_setup_schema.sql\nsupabase/migrations/002_setup_rls.sql\nsupabase/migrations/003_seed_data.sql\nsupabase/migrations/004_create_functions.sql\nsupabase/migrations/005_letter_allowance_system.sql\nsupabase/migrations/006_audit_trail.sql\nsupabase/migrations/007_add_missing_letter_statuses.sql\nsupabase/migrations/008_employee_coupon_auto_generation.sql\nsupabase/migrations/009_add_missing_subscription_fields.sql\nsupabase/migrations/010_add_missing_functions.sql\nsupabase/migrations/010_admin_role_rls.sql\nsupabase/migrations/011_security_hardening.sql\nsupabase/migrations/012_create_coupon_usage_table.sql\nsupabase/migrations/20251122000003_add_missing_functions.sql\nsupabase/migrations/20251124000001_finalize_rls_policies.sql\nsupabase/migrations/COMPLETE_MIGRATION.sql\n\n\nCommand Editor.read executed: path='/workspace/ttml_app/main-main/README.md' content='001|# Talk-To-My-Lawyer - Legal Letter Generation Platform\\n002|\\n003|A production-ready SaaS platform for generating legal letters with AI assistance and attorney review. Built with Next.js 16, Supabase, and OpenAI GPT-4.\\n004|\\n005|## \ud83c\udfd7\ufe0f Architecture\\n006|\\n007|### Tech Stack\\n008|- **Frontend**: Next.js 16 (App Router), TypeScript, Tailwind CSS\\n009|- **Backend**: Supabase (PostgreSQL), Next.js API Routes\\n010|- **AI**: OpenAI GPT-4 for letter generation and improvement\\n011|- **Authentication**: Supabase Auth with role-based access control\\n012|- **Database**: PostgreSQL with comprehensive RLS policies\\n013|- **Deployment**: Vercel-ready with CI/CD support\\n014|\\n015|### User Roles\\n016|1. **Subscriber**: Creates and manages legal letters\\n017|2. **Employee**: Manages coupon codes and earns commissions\\n018|3. **Admin**: Reviews letters, manages users and commissions\\n019|\\n020|## \ud83d\ude80 Quick Start\\n021|\\n022|### Prerequisites\\n023|- Node.js 18+\\n024|- Supabase account and project\\n025|- OpenAI API key\\n026|\\n027|### Installation\\n028|\\n029|1. **Clone and install dependencies**\\n030|```bash\\n031|git clone <repository-url>\\n032|cd talk-to-my-lawyer\\n033|npm install\\n034|```\\n035|\\n036|2. **Configure environment variables**\\n037|```bash\\n038|cp .env.example .env.local\\n039|# Fill in your environment variables\\n040|```\\n041|\\n042|3. **Set up database**\\n043|```bash\\n044|# Run SQL scripts in order against your Supabase project\\n045|supabase db push scripts/001_setup_schema.sql\\n046|supabase db push scripts/002_setup_rls.sql\\n047|supabase db push scripts/003_seed_data.sql\\n048|supabase db push scripts/004_create_functions.sql\\n049|supabase db push scripts/005_letter_allowance_system.sql\\n050|supabase db push scripts/006_audit_trail.sql\\n051|supabase db push scripts/007_add_missing_letter_statuses.sql\\n052|supabase db push scripts/008_employee_coupon_auto_generation.sql\\n053|supabase db push scripts/009_add_missing_subscription_fields.sql\\n054|supabase db push scripts/010_add_missing_functions.sql\\n055|supabase db push scripts/011_security_hardening.sql\\n056|```\\n057|\\n058|4. **Start development server**\\n059|```bash\\n060|npm run dev\\n061|```\\n062|\\n063|Visit `http://localhost:3000` to see the application.\\n064|\\n065|## \ud83d\udccb Environment Variables\\n066|\\n067|Required environment variables (see `.env.example`):\\n068|\\n069|```bash\\n070|# Base URLs\\n071|NEXT_PUBLIC_APP_URL=https://your-domain.com\\n072|NEXT_PUBLIC_DEV_SUPABASE_REDIRECT_URL=http://localhost:3000\\n073|\\n074|# Supabase Configuration\\n075|NEXT_PUBLIC_SUPABASE_URL=your-supabase-url\\n076|NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key\\n077|SUPABASE_SERVICE_ROLE_KEY=your-service-role-key\\n078|\\n079|# AI Configuration\\n080|OPENAI_API_KEY=your-openai-api-key\\n081|\\n082|# Security\\n083|CRON_SECRET=your-cron-secret-key\\n084|```\\n085|\\n086|## \ud83d\udcb3 Pricing Model\\n087|\\n088|- **Single Letter**: $299 (one-time)\\n089|- **Monthly Plan**: $299/month (4 letters)\\n090|- **Yearly Plan**: $599/year (8 letters)\\n091|\\n092|## \ud83d\udc65 User Flow\\n093|\\n094|### Subscriber Journey\\n095|1. **Free Trial**: First letter is free with AI generation\\n096|2. **Subscription Purchase**: Choose plan via Stripe checkout\\n097|3. **Letter Creation**: Fill intake form, AI generates draft\\n098|4. **Attorney Review**: Admin reviews and improves with AI assistance\\n099|5. **Delivery**: Download PDF or send via email\\n100|\\n101|### Employee Journey\\n102|1. **Coupon Assignment**: Auto-generated unique coupon code\\n103|2. **Sharing**: Distribute 20% discount codes to potential customers\\n104|3. **Commission**: Earn 5% on each successful subscription\\n105|4. **Tracking**: Monitor usage, earnings, and points\\n106|\\n107|### Admin Journey\\n108|1. **Letter Review**: Review and approve/reject subscriber letters\\n109|2. **AI Improvement**: Use OpenAI GPT-4 to enhance letter quality\\n110|3. **User Management**: Manage subscribers, employees, and roles\\n111|4. **Commission Management**: Track and process employee commissions\\n112|5. **Analytics**: Monitor platform usage and revenue\\n113|\\n114|## \ud83d\udd12 Security Features\\n115|\\n116|- **Row Level Security**: Complete data isolation between roles\\n117|- **Employee Isolation**: Employees cannot access any letter content\\n118|- **Audit Trail**: Complete logging of all letter status changes\\n119|- **Input Validation**: Comprehensive data sanitization and validation\\n120|- **API Security**: Authentication on all endpoints, proper error handling\\n121|- **Environment Security**: Proper secrets management and configuration\\n122|\\n123|## \ud83d\udcca Key Features\\n124|\\n125|### Core Functionality\\n126|- \u2705 AI-powered letter generation (OpenAI GPT-4)\\n127|- \u2705 Professional attorney review and AI improvement\\n128|- \u2705 PDF generation and email delivery\\n129|- \u2705 Complete audit trail and logging\\n130|- \u2705 Role-based dashboards and access control\\n131|\\n132|### Business Features\\n133|- \u2705 Free trial system\\n134|- \u2705 Subscription management with Stripe\\n135|- \u2705 Employee coupon and commission system\\n136|- \u2705 Monthly letter allowance tracking\\n137|- \u2705 Comprehensive analytics and reporting\\n138|\\n139|### Technical Features\\n140|- \u2705 TypeScript throughout\\n141|- \u2705 Responsive design with Tailwind CSS\\n142|- \u2705 Database functions and triggers\\n143|- \u2705 Security hardening and best practices\\n144|- \u2705 Production-ready deployment setup\\n145|\\n146|## \ud83d\uddc4\ufe0f Database Schema\\n147|\\n148|### Main Tables\\n149|- `profiles`: User accounts with roles\\n150|- `letters`: Legal letters with status tracking\\n151|- `subscriptions`: User subscriptions and credits\\n152|- `employee_coupons`: Employee discount codes\\n153|- `commissions`: Employee earnings tracking\\n154|- `letter_audit_trail`: Complete audit logging\\n155|\\n156|### Key Functions\\n157|- `deduct_letter_allowance`: Manage letter credits\\n158|- `add_letter_allowances`: Add credits on subscription\\n159|- `reset_monthly_allowances`: Monthly credit reset\\n160|- `log_letter_audit`: Audit trail logging\\n161|- `validate_coupon`: Coupon validation\\n162|\\n163|## \ud83d\ude80 Deployment\\n164|\\n165|### Vercel Deployment (Recommended)\\n166|\\n167|1. **Connect to Vercel**\\n168|```bash\\n169|npm i -g vercel\\n170|vercel\\n171|```\\n172|\\n173|2. **Configure Environment Variables**\\n174|Add all required environment variables to Vercel dashboard\\n175|\\n176|3. **Deploy**\\n177|```bash\\n178|vercel --prod\\n179|```\\n180|\\n181|### Manual Deployment\\n182|\\n183|1. **Build application**\\n184|```bash\\n185|npm run build\\n186|```\\n187|\\n188|2. **Start production server**\\n189|```bash\\n190|npm start\\n191|```\\n192|\\n193|## \ud83d\udd04 Cron Jobs\\n194|\\n195|Set up monthly subscription reset:\\n196|\\n197|```bash\\n198|# Add to your cron tab (runs 1st of each month at midnight UTC)\\n199|0 0 1 * * curl -X POST https://your-domain.com/api/subscriptions/reset-monthly \\\\\\n200|  -H \"Authorization: Bearer $CRON_SECRET\"\\n201|```\\n202|\\n203|## \ud83d\udcc8 Monitoring & Analytics\\n204|\\n205|- **User Analytics**: Track registration, subscription, and usage metrics\\n206|- **Revenue Analytics**: Monitor subscription revenue and commission payouts\\n207|- **Letter Analytics**: Track letter generation, approval rates, and types\\n208|- **Security Monitoring**: Comprehensive audit trail and security event logging\\n209|\\n210|## \ud83d\udee0\ufe0f Development\\n211|\\n212|### Database Migrations\\n213|All database changes are managed through SQL scripts in the `scripts/` directory. Scripts must be run in numerical order.\\n214|\\n215|### Adding New Features\\n216|1. Follow the established patterns in existing components\\n217|2. Ensure proper RLS policies for any new tables\\n218|3. Add comprehensive error handling and logging\\n219|4. Update audit trails for any status changes\\n220|5. Test with all user roles (subscriber, employee, admin)\\n221|\\n222|### Code Quality\\n223|- TypeScript for type safety\\n224|- Comprehensive error handling\\n225|- Security-first development approach\\n226|- Responsive design principles\\n227|- Performance optimization\\n228|\\n229|## \ud83e\udd1d Support\\n230|\\n231|For technical support or questions:\\n232|- Review the comprehensive documentation in the `/docs` directory\\n233|- Check the security checklist for security-related questions\\n234|- Monitor audit logs for troubleshooting user issues\\n235|\\n236|---\\n237|\\n238|**Built with \u2764\ufe0f for democratizing access to professional legal services.**'\n\nCommand RoleZero.reply_to_human executed: SUCCESS, human has received your reply. Refrain from resending duplicate messages.  If you no longer need to take action, use the command \u2018end\u2019 to stop.",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.di.run_command.RunCommand",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "f5190485b8f64a9da3300715da24a645",
                            "timestamp": "20251126061338398",
                            "content": "I can see the project has comprehensive SQL migration files. Let me check the main migration file and help you set up the database tables.\n\n<Editor.read>\n<path>/workspace/ttml_app/main-main/supabase/migrations/COMPLETE_MIGRATION.sql</path>\n</Editor.read>\n\n<Terminal.run>\n<cmd>cd /workspace/ttml_app/main-main && ls -la supabase/migrations/ | head -20</cmd>\n</Terminal.run>\n\n<RoleZero.reply_to_human>\n<content>\nI found your database migration files! The project has a comprehensive set of SQL scripts to set up all necessary tables. There's a `COMPLETE_MIGRATION.sql` file that should contain all the schema setup.\n\nLet me read the migration file to understand the complete database structure, and then I'll guide you on how to apply these migrations to your Supabase database to create all the required tables.\n</content>\n</RoleZero.reply_to_human>",
                            "instruct_content": null,
                            "role": "assistant",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "d48aa9c8d4f44aef89c1a1c8a755e2c1",
                            "timestamp": "20251126061338399",
                            "content": "Command Editor.read executed: path='/workspace/ttml_app/main-main/supabase/migrations/COMPLETE_MIGRATION.sql' content='001|-- ============================================================================\\n002|-- COMPLETE MIGRATION FOR TALK-TO-MY-LAWYER\\n003|-- ============================================================================\\n004|-- This file combines all migrations in the correct order for easy deployment\\n005|-- Run this on a fresh Supabase database to set up the complete schema\\n006|-- ============================================================================\\n007|\\n008|-- TASK 9: Complete migration file combining all migrations\\n009|\\n010|-- ============================================================================\\n011|-- 001: SETUP SCHEMA\\n012|-- ============================================================================\\n013|\\n014|-- Enable extensions\\n015|CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\\n016|\\n017|-- Create ENUM types for type safety\\n018|DO $$ BEGIN\\n019|    CREATE TYPE user_role AS ENUM (\\'subscriber\\', \\'employee\\', \\'admin\\');\\n020|EXCEPTION\\n021|    WHEN duplicate_object THEN null;\\n022|END $$;\\n023|\\n024|DO $$ BEGIN\\n025|    CREATE TYPE letter_status AS ENUM (\\n026|        \\'draft\\', \\n027|        \\'generating\\', \\n028|        \\'pending_review\\', \\n029|        \\'under_review\\', \\n030|        \\'approved\\', \\n031|        \\'rejected\\', \\n032|        \\'completed\\', \\n033|        \\'failed\\'\\n034|    );\\n035|EXCEPTION\\n036|    WHEN duplicate_object THEN null;\\n037|END $$;\\n038|\\n039|DO $$ BEGIN\\n040|    CREATE TYPE subscription_status AS ENUM (\\'active\\', \\'canceled\\', \\'past_due\\');\\n041|EXCEPTION\\n042|    WHEN duplicate_object THEN null;\\n043|END $$;\\n044|\\n045|DO $$ BEGIN\\n046|    CREATE TYPE commission_status AS ENUM (\\'pending\\', \\'paid\\');\\n047|EXCEPTION\\n048|    WHEN duplicate_object THEN null;\\n049|END $$;\\n050|\\n051|-- Create profiles table\\n052|CREATE TABLE IF NOT EXISTS profiles (\\n053|    id UUID PRIMARY KEY REFERENCES auth.users(id) ON DELETE CASCADE,\\n054|    email TEXT NOT NULL UNIQUE,\\n055|    full_name TEXT,\\n056|    role user_role DEFAULT \\'subscriber\\',\\n057|    phone TEXT,\\n058|    company_name TEXT,\\n059|    is_super_user BOOLEAN DEFAULT false,\\n060|    created_at TIMESTAMPTZ DEFAULT NOW(),\\n061|    updated_at TIMESTAMPTZ DEFAULT NOW()\\n062|);\\n063|\\n064|-- Create employee_coupons table\\n065|CREATE TABLE IF NOT EXISTS employee_coupons (\\n066|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n067|    employee_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n068|    code TEXT NOT NULL UNIQUE,\\n069|    discount_percent INT CHECK (discount_percent BETWEEN 0 AND 100) DEFAULT 20,\\n070|    is_active BOOLEAN DEFAULT true,\\n071|    usage_count INT DEFAULT 0,\\n072|    created_at TIMESTAMPTZ DEFAULT NOW(),\\n073|    updated_at TIMESTAMPTZ DEFAULT NOW()\\n074|);\\n075|\\n076|-- Create subscriptions table\\n077|CREATE TABLE IF NOT EXISTS subscriptions (\\n078|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n079|    user_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n080|    status subscription_status DEFAULT \\'active\\',\\n081|    coupon_code TEXT,\\n082|    plan TEXT DEFAULT \\'single_letter\\',\\n083|    plan_type TEXT,\\n084|    price NUMERIC(10,2) DEFAULT 299.00,\\n085|    discount NUMERIC(10,2) DEFAULT 0.00,\\n086|    stripe_subscription_id TEXT,\\n087|    remaining_letters INT DEFAULT 0,\\n088|    credits_remaining INT DEFAULT 0,\\n089|    last_reset_at TIMESTAMPTZ,\\n090|    current_period_start TIMESTAMPTZ,\\n091|    current_period_end TIMESTAMPTZ,\\n092|    created_at TIMESTAMPTZ DEFAULT NOW(),\\n093|    updated_at TIMESTAMPTZ DEFAULT NOW()\\n094|);\\n095|\\n096|-- Create commissions table\\n097|CREATE TABLE IF NOT EXISTS commissions (\\n098|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n099|    employee_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n100|    subscription_id UUID NOT NULL REFERENCES subscriptions(id) ON DELETE CASCADE,\\n101|    commission_rate NUMERIC(5,4) DEFAULT 0.05,\\n102|    subscription_amount NUMERIC(10,2) NOT NULL DEFAULT 0,\\n103|    commission_amount NUMERIC(10,2) NOT NULL DEFAULT 0,\\n104|    status commission_status DEFAULT \\'pending\\',\\n105|    paid_at TIMESTAMPTZ,\\n106|    created_at TIMESTAMPTZ DEFAULT NOW()\\n107|);\\n108|\\n109|-- Create letters table\\n110|CREATE TABLE IF NOT EXISTS letters (\\n111|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n112|    user_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n113|    title TEXT NOT NULL,\\n114|    status letter_status DEFAULT \\'draft\\',\\n115|    letter_type TEXT,\\n116|    intake_data JSONB DEFAULT \\'{}\\',\\n117|    ai_draft_content TEXT,\\n118|    final_content TEXT,\\n119|    reviewed_by UUID REFERENCES profiles(id),\\n120|    reviewed_at TIMESTAMPTZ,\\n121|    review_notes TEXT,\\n122|    rejection_reason TEXT,\\n123|    approved_at TIMESTAMPTZ,\\n124|    created_at TIMESTAMPTZ DEFAULT NOW(),\\n125|    updated_at TIMESTAMPTZ DEFAULT NOW()\\n126|);\\n127|\\n128|-- Create coupon_usage table (TASK 2)\\n129|CREATE TABLE IF NOT EXISTS coupon_usage (\\n130|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n131|    user_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,\\n132|    coupon_code TEXT NOT NULL,\\n133|    employee_id UUID REFERENCES profiles(id) ON DELETE SET NULL,\\n134|    discount_percent INT CHECK (discount_percent BETWEEN 0 AND 100),\\n135|    amount_before NUMERIC(10,2) NOT NULL,\\n136|    amount_after NUMERIC(10,2) NOT NULL,\\n137|    created_at TIMESTAMPTZ DEFAULT NOW()\\n138|);\\n139|\\n140|-- Create letter_audit_trail table\\n141|CREATE TABLE IF NOT EXISTS letter_audit_trail (\\n142|    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\\n143|    letter_id UUID NOT NULL REFERENCES letters(id) ON DELETE CASCADE,\\n144|    action TEXT NOT NULL,\\n145|    old_status TEXT,\\n146|    new_status TEXT,\\n147|    performed_by UUID REFERENCES profiles(id),\\n148|    notes TEXT,\\n149|    created_at TIMESTAMPTZ DEFAULT NOW()\\n150|);\\n151|\\n152|-- Create indexes for performance\\n153|CREATE INDEX IF NOT EXISTS idx_profiles_role ON profiles(role);\\n154|CREATE INDEX IF NOT EXISTS idx_profiles_email ON profiles(email);\\n155|CREATE INDEX IF NOT EXISTS idx_employee_coupons_code ON employee_coupons(code);\\n156|CREATE INDEX IF NOT EXISTS idx_employee_coupons_employee ON employee_coupons(employee_id);\\n157|CREATE INDEX IF NOT EXISTS idx_letters_user_id ON letters(user_id);\\n158|CREATE INDEX IF NOT EXISTS idx_letters_status ON letters(status);\\n159|CREATE INDEX IF NOT EXISTS idx_commissions_employee ON commissions(employee_id);\\n160|CREATE INDEX IF NOT EXISTS idx_subscriptions_user ON subscriptions(user_id);\\n161|CREATE INDEX IF NOT EXISTS idx_coupon_usage_user ON coupon_usage(user_id);\\n162|CREATE INDEX IF NOT EXISTS idx_coupon_usage_code ON coupon_usage(coupon_code);\\n163|CREATE INDEX IF NOT EXISTS idx_coupon_usage_employee ON coupon_usage(employee_id);\\n164|CREATE INDEX IF NOT EXISTS idx_letter_audit_letter ON letter_audit_trail(letter_id);\\n165|\\n166|-- ============================================================================\\n167|-- 002: SETUP RLS POLICIES\\n168|-- ============================================================================\\n169|\\n170|-- Enable RLS on all tables\\n171|ALTER TABLE profiles ENABLE ROW LEVEL SECURITY;\\n172|ALTER TABLE employee_coupons ENABLE ROW LEVEL SECURITY;\\n173|ALTER TABLE subscriptions ENABLE ROW LEVEL SECURITY;\\n174|ALTER TABLE commissions ENABLE ROW LEVEL SECURITY;\\n175|ALTER TABLE letters ENABLE ROW LEVEL SECURITY;\\n176|ALTER TABLE coupon_usage ENABLE ROW LEVEL SECURITY;\\n177|ALTER TABLE letter_audit_trail ENABLE ROW LEVEL SECURITY;\\n178|\\n179|-- Helper function to get user role\\n180|CREATE OR REPLACE FUNCTION get_user_role(user_id UUID)\\n181|RETURNS TEXT AS $$\\n182|  SELECT role::TEXT FROM profiles WHERE id = user_id;\\n183|$$ LANGUAGE SQL STABLE SECURITY DEFINER;\\n184|\\n185|-- Profiles policies\\n186|CREATE POLICY \"Users can view own profile\" ON profiles FOR SELECT USING (auth.uid() = id);\\n187|CREATE POLICY \"Users can update own profile\" ON profiles FOR UPDATE USING (auth.uid() = id);\\n188|CREATE POLICY \"Admins can view all profiles\" ON profiles FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n189|\\n190|-- Letters policies\\n191|CREATE POLICY \"Users can view own letters\" ON letters FOR SELECT USING (auth.uid() = user_id);\\n192|CREATE POLICY \"Users can insert own letters\" ON letters FOR INSERT WITH CHECK (auth.uid() = user_id);\\n193|CREATE POLICY \"Users can update own letters\" ON letters FOR UPDATE USING (auth.uid() = user_id);\\n194|CREATE POLICY \"Admins can view all letters\" ON letters FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n195|CREATE POLICY \"Admins can update all letters\" ON letters FOR UPDATE USING (get_user_role(auth.uid()) = \\'admin\\');\\n196|\\n197|-- Subscriptions policies\\n198|CREATE POLICY \"Users can view own subscriptions\" ON subscriptions FOR SELECT USING (auth.uid() = user_id);\\n199|CREATE POLICY \"Admins can view all subscriptions\" ON subscriptions FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n200|\\n201|-- Employee coupons policies\\n202|CREATE POLICY \"Employees can view own coupons\" ON employee_coupons FOR SELECT USING (auth.uid() = employee_id);\\n203|CREATE POLICY \"Anyone can check coupon validity\" ON employee_coupons FOR SELECT USING (true);\\n204|CREATE POLICY \"Admins can manage all coupons\" ON employee_coupons FOR ALL USING (get_user_role(auth.uid()) = \\'admin\\');\\n205|\\n206|-- Commissions policies\\n207|CREATE POLICY \"Employees can view own commissions\" ON commissions FOR SELECT USING (auth.uid() = employee_id);\\n208|CREATE POLICY \"Admins can view all commissions\" ON commissions FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n209|\\n210|-- Coupon usage policies\\n211|CREATE POLICY \"Users can view own coupon usage\" ON coupon_usage FOR SELECT USING (auth.uid() = user_id);\\n212|CREATE POLICY \"Admins can view all coupon usage\" ON coupon_usage FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n213|\\n214|-- Audit trail policies\\n215|CREATE POLICY \"Users can view audit for own letters\" ON letter_audit_trail FOR SELECT \\n216|  USING (EXISTS (SELECT 1 FROM letters WHERE letters.id = letter_audit_trail.letter_id AND letters.user_id = auth.uid()));\\n217|CREATE POLICY \"Admins can view all audit trails\" ON letter_audit_trail FOR SELECT USING (get_user_role(auth.uid()) = \\'admin\\');\\n218|\\n219|-- ============================================================================\\n220|-- 004: CREATE FUNCTIONS\\n221|-- ============================================================================\\n222|\\n223|-- Function to check letter allowance\\n224|CREATE OR REPLACE FUNCTION check_letter_allowance(u_id UUID)\\n225|RETURNS TABLE (\\n226|  has_allowance BOOLEAN,\\n227|  remaining INT,\\n228|  plan_name TEXT,\\n229|  is_super BOOLEAN\\n230|) AS $$\\n231|DECLARE\\n232|  v_is_super BOOLEAN;\\n233|  v_remaining INT;\\n234|  v_plan TEXT;\\n235|BEGIN\\n236|  -- Check if user is super user\\n237|  SELECT is_super_user INTO v_is_super FROM profiles WHERE id = u_id;\\n238|  \\n239|  IF v_is_super THEN\\n240|    RETURN QUERY SELECT true, 999999, \\'Super User\\'::TEXT, true;\\n241|    RETURN;\\n242|  END IF;\\n243|  \\n244|  -- Check active subscription\\n245|  SELECT \\n246|    COALESCE(credits_remaining, 0),\\n247|    COALESCE(plan, \\'None\\')\\n248|  INTO v_remaining, v_plan\\n249|  FROM subscriptions\\n250|  WHERE user_id = u_id AND status = \\'active\\'\\n251|  ORDER BY created_at DESC\\n252|  LIMIT 1;\\n253|  \\n254|  IF v_remaining > 0 THEN\\n255|    RETURN QUERY SELECT true, v_remaining, v_plan, false;\\n256|  ELSE\\n257|    RETURN QUERY SELECT false, 0, COALESCE(v_plan, \\'None\\'::TEXT), false;\\n258|  END IF;\\n259|END;\\n260|$$ LANGUAGE plpgsql SECURITY DEFINER;\\n261|\\n262|-- Function to log audit trail\\n263|CREATE OR REPLACE FUNCTION log_letter_audit(\\n264|  p_letter_id UUID,\\n265|  p_action TEXT,\\n266|  p_old_status TEXT DEFAULT NULL,\\n267|  p_new_status TEXT DEFAULT NULL,\\n268|  p_notes TEXT DEFAULT NULL\\n269|)\\n270|RETURNS UUID AS $$\\n271|DECLARE\\n272|  v_audit_id UUID;\\n273|BEGIN\\n274|  INSERT INTO letter_audit_trail (\\n275|    letter_id,\\n276|    action,\\n277|    old_status,\\n278|    new_status,\\n279|    performed_by,\\n280|    notes\\n281|  ) VALUES (\\n282|    p_letter_id,\\n283|    p_action,\\n284|    p_old_status,\\n285|    p_new_status,\\n286|    auth.uid(),\\n287|    p_notes\\n288|  ) RETURNING id INTO v_audit_id;\\n289|  \\n290|  RETURN v_audit_id;\\n291|END;\\n292|$$ LANGUAGE plpgsql SECURITY DEFINER;\\n293|\\n294|-- ============================================================================\\n295|-- TRIGGERS\\n296|-- ============================================================================\\n297|\\n298|-- Trigger to auto-create profile on signup\\n299|CREATE OR REPLACE FUNCTION public.handle_new_user()\\n300|RETURNS TRIGGER AS $$\\n301|BEGIN\\n302|    INSERT INTO public.profiles (id, email, full_name)\\n303|    VALUES (NEW.id, NEW.email, NEW.raw_user_meta_data->>\\'full_name\\');\\n304|    RETURN NEW;\\n305|END;\\n306|$$ LANGUAGE plpgsql SECURITY DEFINER;\\n307|\\n308|DROP TRIGGER IF EXISTS on_auth_user_created ON auth.users;\\n309|CREATE TRIGGER on_auth_user_created\\n310|    AFTER INSERT ON auth.users\\n311|    FOR EACH ROW EXECUTE FUNCTION public.handle_new_user();\\n312|\\n313|-- ============================================================================\\n314|-- COMMENTS\\n315|-- ============================================================================\\n316|\\n317|COMMENT ON TABLE coupon_usage IS \\'Tracks all coupon code usage including TALK3 and employee coupons\\';\\n318|COMMENT ON COLUMN coupon_usage.employee_id IS \\'Employee who owns the coupon, NULL for special codes like TALK3\\';\\n319|COMMENT ON COLUMN coupon_usage.discount_percent IS \\'Percentage discount applied (0-100)\\';\\n320|COMMENT ON COLUMN profiles.is_super_user IS \\'Business flag for unlimited letter allowances, NOT an admin role\\';\\n321|\\n322|-- ============================================================================\\n323|-- COMPLETION\\n324|-- ============================================================================\\n325|\\n326|-- Migration complete\\n327|SELECT \\'Talk-To-My-Lawyer database schema setup complete!\\' as status;'\n\nCommand Terminal.run executed:\n[command]: cd /workspace/ttml_app/main-main && ls -la supabase/migrations/ | head -20 \n[command output] : \ntotal 300\ndrwxrwxr-x 18 root root 110592 Nov 26 01:34 .\ndrwxrwxr-x 22 root root 126976 Nov 26 01:34 ..\n-rw-rw-r--  1 root root   4661 Nov 26 01:34 001_setup_schema.sql\n-rw-rw-r--  1 root root   5010 Nov 26 01:34 002_setup_rls.sql\n-rw-rw-r--  1 root root    662 Nov 26 01:34 003_seed_data.sql\n-rw-rw-r--  1 root root   1635 Nov 26 01:34 004_create_functions.sql\n-rw-rw-r--  1 root root   2966 Nov 26 01:34 005_letter_allowance_system.sql\n-rw-rw-r--  1 root root   1748 Nov 26 01:34 006_audit_trail.sql\n-rw-rw-r--  1 root root    761 Nov 26 01:34 007_add_missing_letter_statuses.sql\n-rw-rw-r--  1 root root    769 Nov 26 01:34 008_employee_coupon_auto_generation.sql\n-rw-rw-r--  1 root root   1443 Nov 26 01:34 009_add_missing_subscription_fields.sql\n-rw-rw-r--  1 root root   2692 Nov 26 01:34 010_add_missing_functions.sql\n-rw-rw-r--  1 root root   7813 Nov 26 01:34 010_admin_role_rls.sql\n-rw-rw-r--  1 root root   4539 Nov 26 01:34 011_security_hardening.sql\n-rw-rw-r--  1 root root   1257 Nov 26 01:34 012_create_coupon_usage_table.sql\n-rw-rw-r--  1 root root   8705 Nov 26 01:34 20251122000003_add_missing_functions.sql\n-rw-rw-r--  1 root root   7412 Nov 26 01:34 20251124000001_finalize_rls_policies.sql\n-rw-rw-r--  1 root root  12409 Nov 26 01:34 COMPLETE_MIGRATION.sql\n\n\nCommand RoleZero.reply_to_human executed: SUCCESS, human has received your reply. Refrain from resending duplicate messages.  If you no longer need to take action, use the command \u2018end\u2019 to stop.",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.di.run_command.RunCommand",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "0939603518ca44b4adb3cfe0a84b2876",
                            "timestamp": "20251126061348530",
                            "content": "I can see you have a comprehensive `COMPLETE_MIGRATION.sql` file that contains all the necessary database schema. This file will create all 7 tables needed for your \"Talk to My Lawyer\" application. Let me help you apply this migration to your Supabase database.\n\n<RoleZero.reply_to_human>\n<content>\nI found your database migration file! The `COMPLETE_MIGRATION.sql` contains the complete schema that will create all 7 tables:\n\n1. **profiles** - User accounts with roles (subscriber, employee, admin)\n2. **employee_coupons** - Employee discount codes and tracking\n3. **subscriptions** - User subscriptions and letter credits\n4. **commissions** - Employee earnings tracking\n5. **letters** - Legal letters with status tracking\n6. **coupon_usage** - Tracks all coupon code usage\n7. **letter_audit_trail** - Complete audit logging\n\n**To set up your database, you have two options:**\n\n**Option 1: Using Supabase SQL Editor (Recommended)**\n1. Go to your Supabase project dashboard\n2. Click on \"SQL Editor\" in the left sidebar\n3. Click \"New Query\"\n4. Copy the entire contents of `/workspace/ttml_app/main-main/supabase/migrations/COMPLETE_MIGRATION.sql`\n5. Paste it into the SQL Editor\n6. Click \"Run\" to execute\n\n**Option 2: Using Supabase CLI**\nIf you have Supabase CLI installed locally:\n```bash\ncd /workspace/ttml_app/main-main\nsupabase db push supabase/migrations/COMPLETE_MIGRATION.sql\n```\n\nAfter running the migration, you'll have all tables created with:\n- Proper indexes for performance\n- Row Level Security (RLS) policies for data protection\n- Database functions for business logic\n- Triggers for automatic profile creation\n- Complete audit trail system\n\nWould you like me to help you with any specific part of the setup, or do you need the SQL file content to copy?\n</content>\n</RoleZero.reply_to_human>\n\n<end></end>",
                            "instruct_content": null,
                            "role": "assistant",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "36fc78b2042f457db7c7aa3e3a8defbc",
                            "timestamp": "20251126061348531",
                            "content": "Command RoleZero.reply_to_human executed: SUCCESS, human has received your reply. Refrain from resending duplicate messages.  If you no longer need to take action, use the command \u2018end\u2019 to stop.\n\nCommand end executed:",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.di.run_command.RunCommand",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        }
                    ],
                    "memory_k": 30,
                    "compress_k": 15,
                    "prev_summary": null,
                    "compress_model_name": "gpt-4o-mini",
                    "is_xml_format": true,
                    "compress_trigger_token_threshold": 0
                },
                "working_memory": {
                    "storage": [],
                    "index": {},
                    "ignore_id": false,
                    "appendix": null
                },
                "state": -1,
                "watch": [],
                "react_mode": "react",
                "max_react_loop": 10
            },
            "addresses": [
                "metagpt.roles.di.team_leader.TeamLeader",
                "Mike"
            ],
            "planner": {
                "plan": {
                    "goal": "[Message] from User to Mike: there are 0 tables ",
                    "context": "",
                    "tasks": [],
                    "task_map": {},
                    "current_task_id": ""
                },
                "working_memory": {
                    "storage": [],
                    "index": {},
                    "ignore_id": false,
                    "appendix": null
                },
                "auto_run": true
            },
            "recovered": false,
            "latest_observed_msg": null,
            "observe_all_msg_from_buffer": true,
            "system_msg": null,
            "system_prompt": "\n<Basic Info>\n{role_info}\n</Basic Info>\n\n<Domain Info>\n{domain_info}\n</Domain Info>\n\n<Data Structure>\nclass Task(BaseModel):\n    task_id: str = \"\"\n    dependent_task_ids: list[str] = []\n    instruction: str = \"\"\n    task_type: str = \"\"\n    assignee: str = \"\"\n</Data Structure>\n\n<Available Task Types>\n{task_type_desc}\n</Available Task Types>\n\n<Available Commands>\n{available_commands}\nSpecial Command: Use <end></end> to indicate completion of all requirements and termination of the entire workflow.\n</Available Commands>\n\n<Example>\n{example}\n</Example>\n\n<Instruction>\n{instruction}\n\nYou may use any of the available commands. You may output multiple commands, they will be executed sequentially.\nIf the task requires writing multiple files, you should output multiple commands to write files instead of writing one by one.\nIf you finish current task, you will automatically take the next task in the existing plan, use Plan.finish_current_task, DON'T append a new task.\nReview the latest plan's outcome. If your completed task matches the current, consider it finished.\nIn your response, include at least one command. Only use <end></end> when all requirements are met, and please use RoleZero.reply_to_human immediately before <end></end> to report completion.\nDo NOT use <end></end> when waiting for user input or clarification.\n</Instruction>\n\n<Output>\nYou should output a list of commands, follow the format below, replace the <ClassName.method_name> and <args_name> with the actual name.\n\nSome thoughts...\n<ClassName.method_name>\n<args_name1>\nargs_value1\n</args_name1>\n<args_name2>\nargs_value2\n</args_name2>\n</ClassName.method_name>\n\nSome thoughts...\n<ClassName2.method_name2>\n<args_name1>\nargs_value1\n</args_name1>\n<args_name2>\nargs_value2\n</args_name2>\n</ClassName2.method_name2>\n</Output>\n",
            "cmd_prompt": "\n<Current State>\n{current_state}\n</Current State>\n\n<Current Plan>\n{plan_status}\n</Current Plan>\n\n<Current Task>\n{current_task}\n</Current Task>\n\nYour commands (output ONE and ONLY ONE command block, the block can contain one or more commands. Use <end></end> when all requirements are met):\n",
            "instruction": "\nYou are a team leader, and you are responsible for drafting tasks and routing tasks to your team members.\nYour team member:\nMike: Team Leader, Manage a team to assist users\nEmma: Product Manager, Analyze user needs, create a PRD or Product Research report (including competitive analysis, market research) after collecting information and data online.\nBob: Architect, Design a concise, usable, complete software system. Output the system design.\nAlex: Engineer, Take on game, app, web development and deployment.\nDavid: Data Analyst, Take on any data-related tasks, such as machine learning, deep learning, web browsing, web scraping, web searching, terminal operation, etc.\n\nYou should NOT assign consecutive tasks to the same team member, instead, assign an aggregated task (or the complete requirement) and let the team member to decompose it.\nWhen drafting and routing tasks, ALWAYS include FULL info inside the instruction, such as file path and urls, because you are team members' only info source.\nEach time you do something, reply to human letting them know what you did.\nWhen creating a new plan involving multiple members, create all tasks at once.\nUse RoleZero.ask_human to clarify user's requirements only when ambiguity would materially change the plan; do this before drafting. Ask 2-4 critical questions that affect architecture/features, and use continuous incremental numbering (1, 2, 3, \u2026) with no subsections/headers. Skip details that can be inferred. Ask only about features within our team's capabilities; do not ask about or mention complex backends, heavy server-side processing, large-scale real-time systems, video-streaming infrastructure, or enterprise-level systems.\nYou MUST ALWAYS call TeamLeader.draft_plan FIRST to confirm your development plan with the human before distributing tasks to team members. This is MANDATORY - do NOT skip this step. content must be a minimal, flat list of only main features/components, use continuous incremental numbering 1, 2, 3, \u2026 (no headers, grouping, or extra formatting). Exclude assumptions and implementation details. Do not put conversational text in content; keep it outside.\nIf plan is created, you should track the progress based on team member feedback message, and update plan accordingly, such as Plan.finish_current_task, Plan.reset_task, Plan.replace_task, etc.\nYou should use TeamLeader.publish_team_message to team members, asking them to start their task. DON'T omit any necessary info such as path, link, environment, programming language, framework, requirement, constraint from original content to team members because you are their sole info source. When distributing tasks, explicitly tell team members: \"The plan has been confirmed with the user. No need to ask human or draft plan again. Proceed directly.\"\nPay close attention to new user message, review the conversation history, use RoleZero.reply_to_human to respond to the user directly, DON'T ask your team members.\nPay close attention to messages from team members. If a team member has finished a task, mark the current task as completed.\nAvoid using overly absolute or definitive statements such as \u201ceverything is working perfectly,\u201d \u201cthe app is fully developed,\u201d or \u201cthe interface is intuitive and fully functional.\u201d\nWhen all team members have finished their tasks, give recommendation to users to whether: a. preview the application build results in App Viewer, b. click the publish button to deploy, c. share the link, based on the task type. Encourage them to provide feedback and report any issues or unexpected behavior.\nNote:\n1. Assign team members based on their expertise, considering task order and dependency properly. Strictly follow user requirements without adding or modifying them. All tasks must align with the original user requirements exactly.\n2. If the requirement is developing a software, game, app, or website, you should decompose the requirement into one or multiple tasks and assign them to different team members based on their expertise. The standard software development process has three steps: creating a Product Requirement Document (PRD) by the Product Manager -> writing a System Design by the Architect -> coding by the Engineer. You may choose to execute any of these steps.\n2.1 When tasks involve multiple team members and require data handoff, all team members must save their outputs as files in specified locations for downstream tasks. When assigning tasks, always specify these file paths clearly.\n2.2 **IMPORTANT**: When you decide whether to assign to David, you must follow the followings:\n    - Assign the task to David only if the user explicitly or implicitly requests data modeling, machine learning, deep learning, reinforcement learning, or related analysis.\n    - For all other cases, including research data, document analysis, and web development and other similar tasks, assign the task directly to Alex.\n    - Always give a brief reason for your decision.\n2.3. For software development requirement, estimate the complexity of the requirement before assignment, following the common industry practice of t-shirt sizing:\n - XS: snake game, static personal homepage, basic calculator app, personal business card, blog-typed personal website, personal demonstration, PPT creation, data dashboards\n - S: Basic photo gallery, basic file upload system, basic feedback form\n - M: Offline menu ordering system, news aggregator app\n - L: Online booking system, inventory management system\n - XL: Social media platform, e-commerce app, real-time multiplayer game\n - For XS / S / M / L requirements, you don't need the standard software development process, you may directly ask Engineer to write the code. Otherwise, estimate if any part of the standard software development process may contribute to a better final code. If so, assign team members accordingly.\n - Hint: When developing, one ENGINEER IS ENOUGH most of the time. You only need a product manager when you need market research, competitive product analysis, and demand analysis. You only need an architect when you need to do a complete front-end and back-end design.\n8. Do not use escape characters in json data, particularly within file paths.\n11. Instructions and reply must be in the same language.\n12. Default technology stack is Shadcn-ui, TypeScript and Tailwind CSS. Web app is the default option when developing software.\n12.1 Default backend service is Supabase(provides Auth, Database, File Storage, Edge Functions and Real-time Updates features) insead of localStorage.\n14. For presentation project, ask Engineer to use the presentation template. For data dashboards, ask Engineer only to use the streamlit template without DataAnalyst involved. Engineer may perform the data analysis for the dashboard.\n15. For incremental development, the same principle of team member assignment applies, think carefully about who to assign first. This could possibly involve multiple members.\n20. **Exclude dependency folders** when using `find` or `tree`; otherwise the output explodes the token limit.  \n   - `find` example:  \n     `find <dirname> -type d \\( -name \"site-packages\" -o -name \"node_modules\" \\) -prune -o -print`  \n   - `tree` example:  \n     `tree <dirname> -I 'site-packages|node_modules'`\n21. If files like `docs/tasks/Alex_todo.md.part*` exist, once the Engineer informs you that it has completed any work on a `docs/tasks/Alex_todo.md.part(n)` file (e.g., `docs/tasks/Alex_done.md.part1`), you must **IMMEDIATELY** rename that file from `docs/tasks/Alex_todo.md.part(n)` to `docs/tasks/Alex_done.md.part(n)`. And if **all** `todo` files in `docs/tasks/` have been renamed to `done`, **NOTIFY** the user that the entire task list is finished. \n18. If Deep Researcher has already generated a research report AND the user input is related to content modifications of that existing report, then assign to Deep Researcher, which supports only text content additions, deletions, and modifications. Remember, Deep Researcher does not support adding images, charts, multimedia content, or other non-text elements. If users request such features, clearly inform them of the limitations.\n",
            "task_type_desc": null,
            "example": "\n## example 1\nUser Requirement: Create an e-commerce platform.\nExplanation: The requirement is too vague and could mean many different things. Clarify the requirement first, but only ask about features within our capabilities (simple product catalog, basic shopping cart, user accounts).\n<RoleZero.ask_human>\n<question>\nI can help you build an e-commerce website. To create the right plan, I need to clarify:\n\n1. What type of products will you display and sell? (physical products, digital downloads, or both)\n2. Do you need a shopping cart and checkout page, or just a product showcase with inquiry forms?\n3. How many products approximately? (This helps me design the product browsing experience)\n\nI'll focus on creating a clean product display, user-friendly interface, and basic user authentication if needed.\n</question>\n</RoleZero.ask_human>\n\n## example 2\nUser Requirement: Build a website for selling motorcycles.\nExplanation: The requirement is complex and involves multiple features. First, create a draft plan and confirm with the human before distributing tasks to team members.\n\nI will create a structured draft plan for the motorcycle sales website and ask the human to confirm.\n\n<TeamLeader.draft_plan>\n<content>\n1. Product showcase page - Display all available motorcycles with product images, detailed specifications (engine, weight, price, etc.), and filtering options by category/price\n2. Administrator portal - Secure login page and dashboard for administrators to add/edit/delete motorcycle listings, update prices, and manage inventory\n3. Customer inquiry system - Contact form where potential customers can leave messages, select motorcycles they're interested in, and provide their contact information\n4. Shopping cart and checkout - Allow customers to add motorcycles to cart, review their selections, and complete purchase with payment integration\n5. Company information page - About us section, physical location with map, contact details, and business hours\n6. The website is a web application (not mobile app)\n</content>\n</TeamLeader.draft_plan>\n\n## example 3\nContext: After presenting the initial draft plan for a website, the human responds with feedback requesting features beyond our capabilities.\nUser's feedback on draft plan: \"I approve task 1, 2, 5. For task 3, I also need live video chat with sales representatives. For task 4, I need a shopping cart and checkout system that supports hundreds of users checking out at the same time.\"\nExplanation: The user requests live video chat and high-concurrency checkout system, which are beyond our capabilities. Suggest feasible alternatives that achieve similar goals and call draft_plan again.\n\nI understand you want enhanced communication and a robust checkout system. Let me suggest alternatives that we can build well:\n\nFor real-time communication with sales reps, instead of video chat, I can create a live inquiry form where customers can instantly submit questions, and you'll receive email notifications. We can also add a click-to-call button that opens their phone dialer with your sales number.\n\nFor the checkout system, I can build a shopping cart and checkout page that works smoothly for small to medium traffic (5-10 concurrent users). This is perfect for a boutique motorcycle shop. If you expect hundreds of simultaneous checkouts, you'll need enterprise-level infrastructure which is outside our scope.\n\nLet me show you the revised plan:\n\n<TeamLeader.draft_plan>\n<content>\n1. Product showcase page with motorcycle images, specifications, and filtering\n2. Administrator portal for managing motorcycle listings and inventory\n3. Enhanced customer inquiry system with instant notification forms and click-to-call buttons\n4. Shopping cart with checkout page optimized for small to medium traffic\n5. Company information page with contact details and location map\n</content>\n</TeamLeader.draft_plan>\n\n## example 4\nUser Requirement: Create a web snake game.\nExplanation: The requirement is about software development. Assign appropriate team member based on their expertise.\n<Plan.append_task>\n<task_id>\n2\n</task_id>\n<dependent_task_ids>\n['1']\n</dependent_task_ids>\n<instruction>\nImplement the core game logic for the snake game based on the dependent task.\n</instruction>\n<assignee>\nAlex\n</assignee>\n</Plan.append_task>\n\n<TeamLeader.publish_message>\n<content>\nUse Shadcn-ui, TypeScript and Tailwind CSS to create a web snake game. The plan has been confirmed with the user. No need to ask human or draft plan again. Proceed directly.\n</content>\n<send_to>\nAlex\n</send_to>\n</TeamLeader.publish_message>\n\n<RoleZero.reply_to_human>\n<content>\nI have assigned the tasks to the team members. Alex will work on the project accordingly\n</content>\n</RoleZero.reply_to_human>\n\n<end></end>\n\n## example 5\nConversation History:\n[\n    ...,\n    {'role': 'assistant', 'content': 'from Emma(Product Manager) to {'<all>'}: Request is completed, with outputs: Command WritePRD executed: PRD filename: \"/absolute/path/to/snake_game/docs/design/prd.md\"'},\n]\nExplanation: You received a message from Emma, the Product Manager, that she has completed the PRD, use Plan.finish_current_task to mark her task as finished and moves the plan to the next task. Based on plan status, next task is for Bob (Architect), publish a message asking him to start. The message content should contain important path info.\n<Plan.finish_current_task>\n</Plan.finish_current_task>\n\n<TeamLeader.publish_message>\n<content>\nPlease design the software architecture for the snake game based on the PRD created by Emma. The PRD is at '/absolute/path/to/snake_game/docs/design/prd.md'. The plan has been confirmed with the user. Do NOT use ask_human or draft_plan. Proceed directly.\n</content>\n<send_to>\nBob\n</send_to>\n</TeamLeader.publish_message>\n\n<RoleZero.reply_to_human>\n<content>\nEmma has completed the PRD. I have marked her task as finished and sent the PRD to Bob. Bob will work on the software architecture.\n</content>\n</RoleZero.reply_to_human>\n\n<end></end>\n",
            "react_mode": "react",
            "max_react_loop": 10,
            "consecutive_react_cnt": 3,
            "max_consecutive_react_limit": 10,
            "tools": [
                "SearchEnhancedQA",
                "Plan",
                "RoleZero",
                "TeamLeader",
                "Editor:read,similarity_search",
                "Terminal:run"
            ],
            "mcp_tools": [],
            "tool_recommender": {
                "tools": {
                    "SearchEnhancedQA": {
                        "name": "SearchEnhancedQA",
                        "path": "metagpt/actions/search_enhanced_qa.py",
                        "schemas": {
                            "type": "class",
                            "description": "Question answering and info searching through search engine.",
                            "methods": {
                                "run": {
                                    "type": "async_function",
                                    "description": "Answer a query by leveraging web search results. ",
                                    "signature": "(self, query: 'str', rewrite_query: 'bool' = True) -> 'str'",
                                    "parameters": "Args: query (str): The original user query. rewrite_query (bool): Whether to rewrite the query for better web search results. Defaults to True. Returns: str: A detailed answer based on web search results. Raises: ValueError: If the query is invalid."
                                }
                            },
                            "tool_path": "metagpt/actions/search_enhanced_qa.py"
                        },
                        "code": "@register_tool(include_functions=[\"run\"])\nclass SearchEnhancedQA(Action):\n    \"\"\"Question answering and info searching through search engine.\"\"\"\n\n    name: str = \"SearchEnhancedQA\"\n    desc: str = \"Integrating search engine results to anwser the question.\"\n\n    collect_links_action: CollectLinks = Field(\n        default_factory=CollectLinks, description=\"Action to collect relevant links from a search engine.\"\n    )\n    web_browse_and_summarize_action: WebBrowseAndSummarize = Field(\n        default=None,\n        description=\"Action to explore the web and provide summaries of articles and webpages.\",\n    )\n    per_page_timeout: float = Field(\n        default=15, description=\"The maximum time for fetching a single page is in seconds. Defaults to 15s.\"\n    )\n    java_script_enabled: bool = Field(\n        default=False, description=\"Whether or not to enable JavaScript in the web browser context. Defaults to False.\"\n    )\n    user_agent: str = Field(\n        default=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.81\",\n        description=\"Specific user agent to use in browser.\",\n    )\n    extra_http_headers: dict = Field(\n        default={\"sec-ch-ua\": 'Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24'},\n        description=\"An object containing additional HTTP headers to be sent with every request.\",\n    )\n    max_chars_per_webpage_summary: int = Field(\n        default=4000, description=\"Maximum summary length for each web page content.\"\n    )\n    max_search_results: int = Field(\n        default=15,\n        description=\"Maximum number of search results (links) to collect using the collect_links_action. This controls the number of potential sources for answering the question.\",\n    )\n\n    _reporter: ThoughtReporter = PrivateAttr(ThoughtReporter())\n\n    @model_validator(mode=\"after\")\n    def initialize(self):\n        if self.web_browse_and_summarize_action is None:\n            web_browser_engine = WebBrowserEngine.from_browser_config(\n                self.config.browser,\n                proxy=self.config.proxy,\n                java_script_enabled=self.java_script_enabled,\n                extra_http_headers=self.extra_http_headers,\n                user_agent=self.user_agent,\n            )\n\n            self.web_browse_and_summarize_action = WebBrowseAndSummarize(web_browser_engine=web_browser_engine)\n\n        return self\n\n    async def run(self, query: str, rewrite_query: bool = True) -> str:\n        \"\"\"Answer a query by leveraging web search results.\n\n        Args:\n            query (str): The original user query.\n            rewrite_query (bool): Whether to rewrite the query for better web search results. Defaults to True.\n\n        Returns:\n            str: A detailed answer based on web search results.\n\n        Raises:\n            ValueError: If the query is invalid.\n        \"\"\"\n        async with self._reporter:\n            await self._reporter.async_report({\"type\": \"search\", \"stage\": \"init\"})\n            self._validate_query(query)\n\n            processed_query = await self._process_query(query, rewrite_query)\n            context = await self._build_context(processed_query)\n\n            return await self._generate_answer(processed_query, context)\n\n    def _validate_query(self, query: str) -> None:\n        \"\"\"Validate the input query.\n\n        Args:\n            query (str): The query to validate.\n\n        Raises:\n            ValueError: If the query is invalid.\n        \"\"\"\n\n        if not query.strip():\n            raise ValueError(\"Query cannot be empty or contain only whitespace.\")\n\n    async def _process_query(self, query: str, should_rewrite: bool) -> str:\n        \"\"\"Process the query, optionally rewriting it.\"\"\"\n\n        if should_rewrite:\n            return await self._rewrite_query(query)\n\n        return query\n\n    async def _rewrite_query(self, query: str) -> str:\n        \"\"\"Write a better search query for web search engine.\n\n        If the rewrite process fails, the original query is returned.\n\n        Args:\n            query (str): The original search query.\n\n        Returns:\n            str: The rewritten query if successful, otherwise the original query.\n        \"\"\"\n\n        prompt = REWRITE_QUERY_PROMPT.format(q=query)\n\n        try:\n            resp = await self._aask(prompt)\n            rewritten_query = self._extract_rewritten_query(resp)\n\n            logger.info(f\"Query rewritten: '{query}' -> '{rewritten_query}'\")\n            return rewritten_query\n        except Exception as e:\n            logger.warning(f\"Query rewrite failed. Returning original query. Error: {e}\")\n            return query\n\n    def _extract_rewritten_query(self, response: str) -> str:\n        \"\"\"Extract the rewritten query from the LLM's JSON response.\"\"\"\n\n        resp_json = json.loads(CodeParser.parse_code(response, lang=\"json\"))\n        return resp_json[\"query\"]\n\n    async def _build_context(self, query: str) -> str:\n        \"\"\"Construct a context string from web search citations.\n\n        Args:\n            query (str): The search query.\n\n        Returns:\n            str: Formatted context with numbered citations.\n        \"\"\"\n\n        citations = await self._search_citations(query)\n        context = \"\\n\\n\".join([f\"[[citation:{i+1}]] {c}\" for i, c in enumerate(citations)])\n\n        return context\n\n    async def _search_citations(self, query: str) -> list[str]:\n        \"\"\"Perform web search and summarize relevant content.\n\n        Args:\n            query (str): The search query.\n\n        Returns:\n            list[str]: Summaries of relevant web content.\n        \"\"\"\n\n        relevant_urls = await self._collect_relevant_links(query)\n        await self._reporter.async_report({\"type\": \"search\", \"stage\": \"searching\", \"urls\": relevant_urls})\n        if not relevant_urls:\n            logger.warning(f\"No relevant URLs found for query: {query}\")\n            return []\n\n        logger.info(f\"The Relevant links are: {relevant_urls}\")\n\n        web_summaries = await self._summarize_web_content(relevant_urls)\n        if not web_summaries:\n            logger.warning(f\"No summaries generated for query: {query}\")\n            return []\n\n        citations = list(web_summaries.values())\n\n        return citations\n\n    async def _collect_relevant_links(self, query: str) -> list[str]:\n        \"\"\"Search and rank URLs relevant to the query.\n\n        Args:\n            query (str): The search query.\n\n        Returns:\n            list[str]: Ranked list of relevant URLs.\n        \"\"\"\n\n        return await self.collect_links_action._search_and_rank_urls(\n            topic=query, query=query, max_num_results=self.max_search_results\n        )\n\n    async def _summarize_web_content(self, urls: list[str]) -> dict[str, str]:\n        \"\"\"Fetch and summarize content from given URLs.\n\n        Args:\n            urls (list[str]): List of URLs to summarize.\n\n        Returns:\n            dict[str, str]: Mapping of URLs to their summaries.\n        \"\"\"\n\n        contents = await self._fetch_web_contents(urls)\n        logger.info(\"Fetching web contents done\")\n\n        summaries = {}\n        await self._reporter.async_report(\n            {\"type\": \"search\", \"stage\": \"browsing\", \"pages\": [i.model_dump() for i in contents]}\n        )\n        for content in contents:\n            url = content.url\n            inner_text = content.inner_text.replace(\"\\n\", \"\")\n            if self.web_browse_and_summarize_action._is_content_invalid(inner_text):\n                logger.warning(f\"Invalid content detected for URL {url}: {inner_text[:10]}...\")\n                continue\n\n            summary = inner_text[: self.max_chars_per_webpage_summary]\n            summaries[url] = summary\n\n        return summaries\n\n    async def _fetch_web_contents(self, urls: list[str]) -> list[WebPage]:\n        return await self.web_browse_and_summarize_action._fetch_web_contents(\n            *urls, per_page_timeout=self.per_page_timeout\n        )\n\n    async def _generate_answer(self, query: str, context: str) -> str:\n        \"\"\"Generate an answer using the query and context.\n\n        Args:\n            query (str): The user's question.\n            context (str): Relevant information from web search.\n\n        Returns:\n            str: Generated answer based on the context.\n        \"\"\"\n\n        system_prompt = SEARCH_ENHANCED_QA_SYSTEM_PROMPT.format(context=context)\n\n        async with ThoughtReporter(uuid=self._reporter.uuid, enable_llm_stream=True) as reporter:\n            await reporter.async_report({\"type\": \"search\", \"stage\": \"answer\"})\n            rsp = await self._aask(query, [system_prompt])\n        return rsp\n",
                        "tags": []
                    },
                    "Plan": {
                        "name": "Plan",
                        "path": "metagpt/schema.py",
                        "schemas": {
                            "type": "class",
                            "description": "Plan is a sequence of tasks towards a goal.",
                            "methods": {
                                "append_task": {
                                    "type": "function",
                                    "description": "Append a new task with task_id (number) to the end of existing task sequences. If dependent_task_ids is not empty, the task will depend on the tasks with the ids in the list. Note that the assignee should be the 'name' of the role.",
                                    "signature": "(self, task_id: 'str', dependent_task_ids: 'list[str]', instruction: 'str', assignee: 'str', task_type: 'str' = '')",
                                    "parameters": ""
                                },
                                "finish_current_task": {
                                    "type": "function",
                                    "description": "Finish current task, set Task.is_finished=True, set current task to next task",
                                    "signature": "(self)",
                                    "parameters": ""
                                },
                                "replace_task": {
                                    "type": "function",
                                    "description": "Replace an existing task (can be current task) based on task_id, and reset all tasks depending on it.",
                                    "signature": "(self, task_id: 'str', new_dependent_task_ids: 'list[str]', new_instruction: 'str', new_assignee: 'str')",
                                    "parameters": ""
                                },
                                "reset_task": {
                                    "type": "function",
                                    "description": "Reset a task based on task_id, i.e. set Task.is_finished=False and request redo. This also resets all tasks depending on it. ",
                                    "signature": "(self, task_id: 'str')",
                                    "parameters": "Args: task_id (str): The ID of the task to be reset."
                                }
                            },
                            "tool_path": "metagpt/schema.py"
                        },
                        "code": "@register_tool(\n    include_functions=[\n        \"append_task\",\n        \"reset_task\",\n        \"replace_task\",\n        \"finish_current_task\",\n    ]\n)\nclass Plan(BaseModel):\n    \"\"\"Plan is a sequence of tasks towards a goal.\"\"\"\n\n    goal: str\n    context: str = \"\"\n    tasks: list[Task] = []\n    task_map: dict[str, Task] = {}\n    current_task_id: str = \"\"\n\n    def _topological_sort(self, tasks: list[Task]):\n        task_map = {task.task_id: task for task in tasks}\n        dependencies = {task.task_id: set(task.dependent_task_ids) for task in tasks}\n        sorted_tasks = []\n        visited = set()\n\n        def visit(task_id):\n            if task_id in visited:\n                return\n            visited.add(task_id)\n            for dependent_id in dependencies.get(task_id, []):\n                visit(dependent_id)\n            sorted_tasks.append(task_map[task_id])\n\n        for task in tasks:\n            visit(task.task_id)\n\n        return sorted_tasks\n\n    def add_tasks(self, tasks: list[Task]):\n        \"\"\"\n        Integrates new tasks into the existing plan, ensuring dependency order is maintained.\n\n        This method performs two primary functions based on the current state of the task list:\n        1. If there are no existing tasks, it topologically sorts the provided tasks to ensure\n        correct execution order based on dependencies, and sets these as the current tasks.\n        2. If there are existing tasks, it merges the new tasks with the existing ones. It maintains\n        any common prefix of tasks (based on task_id and instruction) and appends the remainder\n        of the new tasks. The current task is updated to the first unfinished task in this merged list.\n\n        Args:\n            tasks (list[Task]): A list of tasks (may be unordered) to add to the plan.\n\n        Returns:\n            None: The method updates the internal state of the plan but does not return anything.\n        \"\"\"\n        if not tasks:\n            return\n\n        # Topologically sort the new tasks to ensure correct dependency order\n        new_tasks = self._topological_sort(tasks)\n\n        if not self.tasks:\n            # If there are no existing tasks, set the new tasks as the current tasks\n            self.tasks = new_tasks\n\n        else:\n            # Find the length of the common prefix between existing and new tasks\n            prefix_length = 0\n            for old_task, new_task in zip(self.tasks, new_tasks):\n                if old_task.task_id != new_task.task_id or old_task.instruction != new_task.instruction:\n                    break\n                prefix_length += 1\n\n            # Combine the common prefix with the remainder of the new tasks\n            final_tasks = self.tasks[:prefix_length] + new_tasks[prefix_length:]\n            self.tasks = final_tasks\n\n        # Update current_task_id to the first unfinished task in the merged list\n        self._update_current_task()\n\n        # Update the task map for quick access to tasks by ID\n        self.task_map = {task.task_id: task for task in self.tasks}\n\n    def reset_task(self, task_id: str):\n        \"\"\"\n        Reset a task based on task_id, i.e. set Task.is_finished=False and request redo. This also resets all tasks depending on it.\n\n        Args:\n            task_id (str): The ID of the task to be reset.\n        \"\"\"\n        if task_id in self.task_map:\n            task = self.task_map[task_id]\n            task.reset()\n            # reset all downstream tasks that are dependent on the reset task\n            for dep_task in self.tasks:\n                if task_id in dep_task.dependent_task_ids:\n                    # FIXME: if LLM generates cyclic tasks, this will result in infinite recursion\n                    self.reset_task(dep_task.task_id)\n\n        self._update_current_task()\n\n    def _replace_task(self, new_task: Task):\n        \"\"\"\n        Replace an existing task with the new input task based on task_id, and reset all tasks depending on it.\n\n        Args:\n            new_task (Task): The new task that will replace an existing one.\n\n        Returns:\n            None\n        \"\"\"\n        assert new_task.task_id in self.task_map\n        # Replace the task in the task map and the task list\n        self.task_map[new_task.task_id] = new_task\n        for i, task in enumerate(self.tasks):\n            if task.task_id == new_task.task_id:\n                self.tasks[i] = new_task\n                break\n\n        # Reset dependent tasks\n        for task in self.tasks:\n            if new_task.task_id in task.dependent_task_ids:\n                self.reset_task(task.task_id)\n\n        self._update_current_task()\n\n    def _append_task(self, new_task: Task):\n        \"\"\"\n        Append a new task to the end of existing task sequences\n\n        Args:\n            new_task (Task): The new task to be appended to the existing task sequence\n\n        Returns:\n            None\n        \"\"\"\n        # assert not self.has_task_id(new_task.task_id), \"Task already in current plan, use replace_task instead\"\n        if self.has_task_id(new_task.task_id):\n            logger.warning(\n                \"Task already in current plan, should use replace_task instead. Overwriting the existing task.\"\n            )\n\n        assert all(\n            [self.has_task_id(dep_id) for dep_id in new_task.dependent_task_ids]\n        ), \"New task has unknown dependencies\"\n\n        # Existing tasks do not depend on the new task, it's fine to put it to the end of the sorted task sequence\n        self.tasks.append(new_task)\n        self.task_map[new_task.task_id] = new_task\n        self._update_current_task()\n\n    def has_task_id(self, task_id: str) -> bool:\n        return task_id in self.task_map\n\n    def _update_current_task(self):\n        self.tasks = self._topological_sort(self.tasks)\n        # Update the task map for quick access to tasks by ID\n        self.task_map = {task.task_id: task for task in self.tasks}\n\n        current_task_id = \"\"\n        for task in self.tasks:\n            if not task.is_finished:\n                current_task_id = task.task_id\n                break\n        self.current_task_id = current_task_id\n        TaskReporter().report({\"tasks\": [i.model_dump() for i in self.tasks], \"current_task_id\": current_task_id})\n\n    @property\n    def current_task(self) -> Task:\n        \"\"\"Find current task to execute\n\n        Returns:\n            Task: the current task to be executed\n        \"\"\"\n        return self.task_map.get(self.current_task_id, None)\n\n    def finish_current_task(self):\n        \"\"\"Finish current task, set Task.is_finished=True, set current task to next task\"\"\"\n        if self.current_task_id:\n            self.current_task.is_finished = True\n            self._update_current_task()  # set to next task\n\n    def finish_all_tasks(self):\n        \"Finish all tasks.\"\n        while self.current_task:\n            self.finish_current_task()\n\n    def is_plan_finished(self) -> bool:\n        \"\"\"Check if all tasks are finished\"\"\"\n        return all(task.is_finished for task in self.tasks)\n\n    def get_finished_tasks(self) -> list[Task]:\n        \"\"\"return all finished tasks in correct linearized order\n\n        Returns:\n            list[Task]: list of finished tasks\n        \"\"\"\n        return [task for task in self.tasks if task.is_finished]\n\n    def append_task(\n        self, task_id: str, dependent_task_ids: list[str], instruction: str, assignee: str, task_type: str = \"\"\n    ):\n        \"\"\"\n        Append a new task with task_id (number) to the end of existing task sequences.\n        If dependent_task_ids is not empty, the task will depend on the tasks with the ids in the list.\n        Note that the assignee should be the 'name' of the role.\n        \"\"\"\n        new_task = Task(\n            task_id=task_id,\n            dependent_task_ids=dependent_task_ids,\n            instruction=instruction,\n            assignee=assignee,\n            task_type=task_type,\n        )\n        return self._append_task(new_task)\n\n    def replace_task(self, task_id: str, new_dependent_task_ids: list[str], new_instruction: str, new_assignee: str):\n        \"\"\"Replace an existing task (can be current task) based on task_id, and reset all tasks depending on it.\"\"\"\n        new_task = Task(\n            task_id=task_id,\n            dependent_task_ids=new_dependent_task_ids,\n            instruction=new_instruction,\n            assignee=new_assignee,\n        )\n        return self._replace_task(new_task)\n",
                        "tags": []
                    },
                    "RoleZero": {
                        "name": "RoleZero",
                        "path": "metagpt/roles/di/role_zero.py",
                        "schemas": {
                            "type": "class",
                            "description": "A role who can think and act dynamically",
                            "methods": {
                                "ask_human": {
                                    "type": "async_function",
                                    "description": "Use this when you fail the current task or if you are unsure of the situation encountered. Your response should contain a brief summary of your situation, ended with a clear and concise question.",
                                    "signature": "(self, question: 'str') -> 'str'",
                                    "parameters": ""
                                },
                                "reply_to_human": {
                                    "type": "async_function",
                                    "description": "Reply to human user with the content provided. Use this when you have a clear answer or solution to the user's question.",
                                    "signature": "(self, content: 'str') -> 'str'",
                                    "parameters": ""
                                }
                            },
                            "tool_path": "metagpt/roles/di/role_zero.py"
                        },
                        "code": "@register_tool(include_functions=[\"ask_human\", \"reply_to_human\"])\nclass RoleZero(Role):\n    \"\"\"A role who can think and act dynamically\"\"\"\n\n    # Basic Info\n    name: str = \"Zero\"\n    profile: str = \"RoleZero\"\n    goal: str = \"\"\n    system_msg: Optional[list[str]] = None  # Use None to conform to the default value at llm.aask\n    system_prompt: str = SYSTEM_PROMPT  # Use None to conform to the default value at llm.aask\n    cmd_prompt: str = CMD_PROMPT\n    instruction: str = ROLE_INSTRUCTION\n    task_type_desc: Optional[str] = None\n    example: str = \"\"  # global think & act demonstration for the role\n\n    # React Mode\n    react_mode: Literal[\"react\"] = \"react\"\n    max_react_loop: int = 50\n    # consecutive react count after receiving a message, used for triggering human intervention\n    consecutive_react_cnt: int = 0\n    max_consecutive_react_limit: int = 10\n\n    # Tools\n    tools: list[str] = []  # Use special symbol [\"<all>\"] to indicate use of all registered tools\n    mcp_tools: list[str] = []\n    tool_recommender: Optional[ToolRecommender] = None\n    tool_execution_map: Annotated[dict[str, Callable], Field(exclude=True)] = {}\n    special_tool_commands: list[str] = [\"Plan.finish_current_task\", \"end\", \"Terminal.run\", \"RoleZero.ask_human\"]\n\n    # Equipped with three basic tools by default for optional use\n    editor: Annotated[Union[Editor, RemoteProxy], BeforeValidator(ensure_editor_instance)] = Field(\n        default_factory=resolve_editor\n    )\n    browser: Annotated[Union[Browser, RemoteProxy], BeforeValidator(ensure_browser_instance)] = Field(\n        default_factory=resolve_browser\n    )\n    terminal: Annotated[Union[Terminal, RemoteProxy], BeforeValidator(ensure_terminal_instance)] = Field(\n        default_factory=resolve_terminal\n    )\n    working_dir: str = Field(default_factory=lambda: str(DEFAULT_WORKSPACE_ROOT.resolve()))\n\n    # Others\n    observe_all_msg_from_buffer: bool = True\n    command_rsp: str = \"\"  # the raw string containing the commands\n    commands: list[dict] = []  # commands to be executed\n    memory_k: int = 30  # number of memories (messages) to use as historical context\n    use_fixed_sop: bool = False\n    use_summary: bool = True  # whether to summarize at the end\n    browser_is_active: bool = False  # Track browser state to check if it is open\n    think_task: Optional[asyncio.Task] = Field(default=None, exclude=True)  # Track the background thinking task\n\n    # @model_validator(mode=\"after\")\n    # def validate_and_init_mcp_tools(self) -> \"RoleZero\":\n    #     \"\"\"Initialize mcp_tools if empty and MCP servers are enabled\"\"\"\n    #     if not self.mcp_tools:\n    #         if self.config.mcp.has_enabled_servers():\n    #             self.mcp_tools = mcp_manager.list_tool_names()\n    #     return self\n\n    @model_validator(mode=\"after\")\n    def set_plan_and_tool(self) -> \"RoleZero\":\n        # We force using this parameter for DataAnalyst\n        assert self.react_mode == \"react\"\n\n        # Roughly the same part as DataInterpreter.set_plan_and_tool\n        self._set_react_mode(react_mode=self.react_mode, max_react_loop=self.max_react_loop)\n\n        tools = self.tools  # + self.mcp_tools\n        if tools and not self.tool_recommender:\n            self.tool_recommender = BM25ToolRecommender(tools=tools, force=True)\n        self.set_actions([RunCommand])\n\n        # HACK: Init Planner, control it through dynamic thinking; Consider formalizing as a react mode\n        self.planner = Planner(goal=\"\", working_memory=self.rc.working_memory, auto_run=True)\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_tool_execution(self) -> \"RoleZero\":\n        # default map, includes all special tool commands\n        self.tool_execution_map = {\n            \"Plan.append_task\": self.planner.plan.append_task,\n            \"Plan.reset_task\": self.planner.plan.reset_task,\n            \"Plan.replace_task\": self.planner.plan.replace_task,\n            \"Terminal.run\": self.terminal.run,\n            \"Terminal\": self.terminal.run,  # alias\n            \"Terminal.run_command\": self.terminal.run,  # alias\n            \"Terminal.preview\": self.terminal.preview,\n            \"RoleZero.ask_human\": self.ask_human,\n            \"RoleZero.reply_to_human\": self.reply_to_human,\n            \"SearchEnhancedQA.run\": SearchEnhancedQA().run,\n            \"Plan.finish_current_task\": self.planner.plan.finish_current_task,\n            \"end\": self._end,\n        }\n        self.tool_execution_map.update(\n            {\n                f\"Browser.{i}\": getattr(self.browser, i)\n                for i in [\n                    \"click\",\n                    \"close_tab\",\n                    \"go_back\",\n                    \"go_forward\",\n                    \"goto\",\n                    \"hover\",\n                    \"press\",\n                    \"scroll\",\n                    \"tab_focus\",\n                    \"type\",\n                    \"take_screenshot\",\n                ]\n            }\n        )\n        self.tool_execution_map.update(\n            {\n                f\"Editor.{i}\": getattr(self.editor, i)\n                for i in [\n                    \"append_file\",\n                    \"create_file\",\n                    \"edit_file_by_replace\",\n                    \"find_file\",\n                    \"goto_line\",\n                    \"insert_content_at_line\",\n                    \"open_file\",\n                    \"read\",\n                    \"scroll_down\",\n                    \"scroll_up\",\n                    \"search_dir\",\n                    \"search_file\",\n                    \"similarity_search\",\n                    # \"set_workdir\",\n                    \"write\",\n                ]\n            }\n        )\n\n        for tool_name in self.mcp_tools:\n            self.tool_execution_map[tool_name] = mcp_manager.get_tool_executor(tool_name)\n\n        # can be updated by subclass\n        self._update_tool_execution()\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_tool_working_dir(self) -> \"RoleZero\":\n        asyncio.create_task(self._update_workdir_for_tools(self.working_dir))\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_longterm_memory(self) -> \"RoleZero\":\n        \"\"\"Set up special memory for the role if enabled in the configuration.\"\"\"\n\n        # We set Role.rc.memory as SerializeAsAny to retain all fields in Memory subclasses, ensuring model_dump() includes subclass fields.\n        # However, for some of the fields, we want to use the latest values from the config.\n        # To avoid repeated args, we init the subclass first, then update the fields with the config\n        if self.config.role_zero.enable_compressable_memory:\n            kvs = self.rc.memory.model_dump()\n            kvs[\"compress_model_name\"] = self.config.role_zero.compress_model_name\n            self.rc.memory = CompressableMemory(**kvs)\n            self.rc.memory.memory_k = self.config.role_zero.memory_k\n            self.rc.memory.compress_k = self.config.role_zero.compress_k\n            self.rc.memory.compress_trigger_token_threshold = self.config.role_zero.compress_trigger_token_threshold\n            logger.info(f\"Compressable memory set for role '{self.name}'\")\n\n        elif self.config.role_zero.enable_longterm_memory:\n            # Use config.role_zero to initialize long-term memory\n            # The role name will be used as the collection name.\n            self.rc.memory = RoleZeroLongTermMemory(**self.rc.memory.model_dump())\n            memory = self.rc.memory\n            memory.persist_path = self.config.role_zero.longterm_memory_persist_path\n            memory.collection_name = self.name.replace(\" \", \"\")\n            memory.memory_k = self.config.role_zero.memory_k\n            memory.similarity_top_k = self.config.role_zero.similarity_top_k\n            memory.use_llm_ranker = self.config.role_zero.use_llm_ranker\n            logger.info(f\"Long-term memory set for role '{self.name}'\")\n\n        return self\n\n    def _update_tool_execution(self):\n        pass\n\n    def _retrieve_example(self) -> str:\n        \"\"\"Default implementation of example retrieval. Can be overwritten in subclasses.\"\"\"\n        return self.example\n\n    def _get_goal_from_memories(self):\n        \"\"\"Get goal from memories by finding the latest message from TL or UserRequirement\"\"\"\n        memories = self.get_memories()\n        for msg in reversed(memories):\n            if msg.sent_from == \"Mike\" or \"UserRequirement\" in msg.cause_by:\n                return msg.content\n        # Fallback to last message if no matching message found\n        return memories[-1].content if memories else \"\"\n\n    async def _think(self) -> bool:\n        \"\"\"Useful in 'react' mode. Use LLM to decide whether and what to do next.\"\"\"\n        # Compatibility\n        if self.use_fixed_sop:\n            return await super()._think()\n\n        ### 0. Preparation ###\n        if not self.rc.todo:\n            return False\n\n        if not self.planner.plan.goal:\n            self.planner.plan.goal = self._get_goal_from_memories()\n\n        ### 1. Example ###\n        example = self._retrieve_example()\n\n        ### 2. Plan Status ###\n        plan_status, current_task = get_plan_status(planner=self.planner)\n\n        ### 3. Tool/Command Info ###\n        tools = await self.tool_recommender.recommend_tools()\n        tool_info = json.dumps({tool.name: tool.schemas for tool in tools})\n\n        ### Role Instruction ###\n        instruction = self.instruction.strip()\n        system_prompt = self.system_prompt.format(\n            role_info=self._get_prefix(),\n            domain_info=self._get_domain_info(),\n            task_type_desc=self.task_type_desc,\n            available_commands=tool_info,\n            example=example,\n            instruction=instruction,\n        )\n\n        ### Make Decision Dynamically ###\n        prompt = self.cmd_prompt.format(\n            current_state=f\"current directory: {self.working_dir}\",\n            plan_status=plan_status,\n            current_task=current_task,\n        )\n\n        ### Recent Observation ###\n        if self.config.role_zero.enable_compressable_memory:\n            await self.rc.memory.compress(extra_context=super()._get_prefix())\n        memory = self.rc.memory.get(self.memory_k)\n        if self.browser_is_active:\n            memory = await parse_browser_actions(memory, browser=self.browser)\n        # memory = await parse_images(memory, llm=self.llm)\n\n        req = self.llm.format_msg(memory + [UserMessage(content=prompt)])\n\n        state_data = dict(\n            plan_status=plan_status,\n            current_task=current_task,\n            instruction=instruction,\n        )\n        logger.info(f\"{self.name}, ready to think\")\n        # background task to handle LLM thinking process\n        self.create_think_background_task(req, system_prompt, state_data)\n        return True\n\n    def create_think_background_task(self, req, system_prompt, state_data):\n        self.think_task = asyncio.create_task(self._think_background(req, system_prompt, state_data))\n\n    async def _think_background(self, req, system_prompt, state_data):\n        \"\"\"Background task to handle LLM thinking process.\"\"\"\n        async with ThoughtReporter(enable_llm_stream=True) as reporter:\n            await reporter.async_report({\"type\": \"react\"})\n            self.command_rsp = await self.llm_cached_aask(req=req, system_msgs=[system_prompt], state_data=state_data)\n        logger.info(f\"Command response:\\n{self.command_rsp}\")\n        rsp_hist = [mem.content for mem in self.rc.memory.get()]\n        self.command_rsp = await check_duplicates(\n            req=req, command_rsp=self.command_rsp, rsp_hist=rsp_hist, llm=self.llm\n        )\n\n    @exp_cache(context_builder=RoleZeroContextBuilder(), serializer=RoleZeroSerializer())\n    async def llm_cached_aask(self, *, req: list[dict], system_msgs: list[str], **kwargs) -> str:\n        \"\"\"Use `exp_cache` to automatically manage experiences.\n\n        The `RoleZeroContextBuilder` attempts to add experiences to `req`.\n        The `RoleZeroSerializer` extracts essential parts of `req` for the experience pool, trimming lengthy entries to retain only necessary parts.\n        \"\"\"\n        return await self.llm.aask(req, system_msgs=system_msgs)\n\n    def _get_domain_info(self) -> str:\n        return get_time_info() + MGX_INFO + get_enable_supabase_guidance()\n\n    async def _act(self) -> Message:\n        if self.use_fixed_sop:\n            return await super()._act()\n\n        command_outputs = []\n        async for cmd in parse_stream_commands2(role_instance=self, tool_execution_map=self.tool_execution_map):\n            is_success, output = await self._run_command(cmd)\n            command_outputs.append(output)\n            if not is_success:\n                break\n        if not command_outputs:\n            command_outputs = [\"No valid commands found for execution, pay attention to the output format.\"]\n        self.rc.memory.add(AIMessage(content=self.command_rsp))\n        outputs = \"\\n\\n\".join(command_outputs)\n        logger.info(f\"Commands outputs: \\n{outputs}\")\n        self.rc.memory.add(UserMessage(content=outputs, cause_by=RunCommand))\n\n        think_task = self.think_task\n        self.think_task = None\n        await think_task\n\n        return AIMessage(\n            content=f\"I have finished the task, please mark my task as finished. Outputs: {outputs}\",\n            sent_from=self.name,\n            cause_by=RunCommand,\n        )\n\n    async def _react(self) -> Message:\n        # NOTE: Diff 1: Each time landing here means news is observed, set todo to allow news processing in _think\n        self._set_state(0)\n\n        actions_taken = 0\n        self.consecutive_react_cnt = 0\n        rsp = AIMessage(content=\"No actions taken yet\", cause_by=Action)  # will be overwritten after Role _act\n        while actions_taken < self.rc.max_react_loop:\n            # NOTE: Diff 2: Keep observing within _react, news will go into memory, allowing adapting to new info\n            if await self._observe():\n                self.consecutive_react_cnt = 0\n                # Reset state to 0 to override potential `end` command (state = -1) and ensure the loop continues.\n                self._set_state(0)\n            # think\n            has_todo = await self._think()\n            if not has_todo:\n                break\n            # act\n            logger.debug(f\"{self._setting}: {self.rc.state=}, will do {self.rc.todo}\")\n            rsp = await self._act()\n            actions_taken += 1\n            self.consecutive_react_cnt += 1\n\n            # post-check\n            if self.rc.max_react_loop >= 10 and actions_taken >= self.rc.max_react_loop:\n                # If max_react_loop is a small value (e.g. < 10), it is intended to be reached and make the agent stop\n                logger.warning(f\"reached max_react_loop: {actions_taken}\")\n                human_rsp = await self.ask_human(\n                    \"I have reached my max action rounds, do you want me to continue? Yes or no\"\n                )\n                if \"yes\" in human_rsp.lower():\n                    actions_taken = 0\n            if self.consecutive_react_cnt >= self.max_consecutive_react_limit:\n                memory = self.get_memories(k=self.memory_k)\n                context = self.llm.format_msg(memory + [UserMessage(content=SUMMARIZE_STATUS_WHEN_CONSECUTIVE)])\n                question = await self.llm.aask(context)\n                human_rsp = await self.ask_human(question)\n                self.rc.memory.add(UserMessage(content=\"User's extra instruction: \" + human_rsp, cause_by=RunCommand))\n                self.consecutive_react_cnt = 0\n\n        return rsp  # return output from the last action\n\n    async def _run_command(self, cmd) -> tuple[bool, str]:\n        \"\"\"stream command execution\"\"\"\n        output = f\"Command {cmd['command_name']} executed\"\n        try:\n            # handle special command first\n            if self._is_special_command(cmd):\n                special_command_output = await self._run_special_command(cmd)\n                output += \":\" + special_command_output\n                cmd[\"status\"] = \"success\"  # mgx_backend need these fields\n                cmd[\"error_msg\"] = \"\"\n                return True, output\n\n            # run command as specified by tool_execute_map\n            if cmd[\"command_name\"] in self.tool_execution_map:\n                tool_obj = self.tool_execution_map[cmd[\"command_name\"]]\n                if inspect.iscoroutinefunction(tool_obj):\n                    tool_output = await tool_obj(**cmd[\"args\"])\n                else:\n                    tool_output = tool_obj(**cmd[\"args\"])\n                self._try_update_browser_state(cmd[\"command_name\"])\n                if tool_output:\n                    output += f\": {str(tool_output)}\"\n                cmd[\"status\"] = \"success\"\n                cmd[\"error_msg\"] = \"\"\n            else:\n                output = f\"Command {cmd['command_name']} not found.\"\n                cmd[\"status\"] = \"failed\"\n                cmd[\"error_msg\"] = f\"Command {cmd['command_name']} not found.\"\n                return False, output\n\n        except Exception:\n            tb = traceback.format_exc()\n            output += f\": {tb}\"\n            cmd[\"status\"] = \"failed\"\n            cmd[\"error_msg\"] = f\"Command {cmd['command_name']} failed: {tb}\"\n            return False, output\n\n        return True, output\n\n    def _is_special_command(self, cmd) -> bool:\n        return cmd[\"command_name\"] in self.special_tool_commands\n\n    async def _run_special_command(self, cmd) -> str:\n        \"\"\"command requiring special check or parsing\"\"\"\n        command_output = \"\"\n\n        if cmd[\"command_name\"] == \"Plan.finish_current_task\":\n            if not self.planner.plan.is_plan_finished():\n                self.planner.plan.finish_current_task()\n            command_output = (\n                \"Current task is finished. If you no longer need to take action, use the command 'end' to stop.\"\n            )\n\n        elif cmd[\"command_name\"] == \"end\":\n            command_output = await self._end()\n        elif cmd[\"command_name\"] == \"RoleZero.ask_human\":\n            args = cmd[\"args\"]\n            # fault tolerance for llm hallucination\n            question = args.get(\"question\") or args.get(\"content\") or args.get(\"message\")\n            assert question, \"Use kwarg 'question' to provide the question.\"\n            human_response = await self.ask_human(question=question)\n            self.consecutive_react_cnt = 0\n            if human_response.strip().lower().endswith((\"stop\", \"<stop>\")):\n                human_response += \"The user has asked me to stop because I have encountered a problem.\"\n                self.rc.memory.add(UserMessage(content=human_response, cause_by=RunCommand))\n                end_output = \"\\nCommand end executed:\"\n                end_output += await self._end()\n                return end_output\n            return human_response\n        # output from bash.run may be empty, add decorations to the output to ensure visibility.\n        elif \"Terminal\" in cmd[\"command_name\"]:  # Terminal.run and its alias\n            tool_obj = self.tool_execution_map[cmd[\"command_name\"]]\n            tool_output = await tool_obj(**cmd[\"args\"])\n            command_output = format_terminal_output(cmd=cmd, raw_output=tool_output)\n            self.working_dir = await self.terminal.get_cwd()\n            await self._update_workdir_for_tools(self.working_dir)\n            token_count = count_string_tokens(command_output, model_name=\"default\")\n            if token_count >= self.config.llm.max_token * 0.9:\n                command_output = \"OutputTooLongException: the content is too long to use. Please optimize the command parameters to reduce the amount of output.\"\n\n        return command_output\n\n    async def _update_workdir_for_tools(self, new_workdir: str) -> None:\n        \"\"\"Update working directory for all relevant tools.\n        This method can be overridden by subclasses to update additional tools.\n        \"\"\"\n        await self.editor.set_workdir(new_workdir)\n\n    async def ask_human(self, question: str) -> str:\n        \"\"\"Use this when you fail the current task or if you are unsure of the situation encountered. Your response should contain a brief summary of your situation, ended with a clear and concise question.\"\"\"\n        # NOTE: Can be overwritten in remote setting\n        from metagpt.environment.mgx.mgx_env import MGXEnv  # avoid circular import\n\n        if not isinstance(self.rc.env, MGXEnv):\n            return \"Not in MGXEnv, command will not be executed.\"\n        return await self.rc.env.ask_human(question, sent_from=self)\n\n    async def reply_to_human(self, content: str) -> str:\n        \"\"\"Reply to human user with the content provided. Use this when you have a clear answer or solution to the user's question.\"\"\"\n        # NOTE: Can be overwritten in remote setting\n        from metagpt.environment.mgx.mgx_env import MGXEnv  # avoid circular import\n\n        if not isinstance(self.rc.env, MGXEnv):\n            return \"Not in MGXEnv, command will not be executed.\"\n        return await self.rc.env.reply_to_human(content, sent_from=self)\n\n    async def _end(self, **kwarg):\n        self._set_state(-1)\n        memory = self.rc.memory.get(self.memory_k)\n        outputs = \"\"\n        # Summary of the Completed Task and Deliverables\n        if self.use_summary:\n            logger.info(\"end current run and summarize\")\n            async with ThoughtReporter() as reporter:\n                await reporter.async_report({\"type\": \"summary\"})\n                outputs = await self.llm.aask(self.llm.format_msg(memory + [UserMessage(SUMMARY_PROMPT)]))\n        return outputs\n\n    def _try_update_browser_state(self, command_name: str) -> None:\n        \"\"\"Update browser_is_active based on the command_name.\"\"\"\n\n        if command_name == \"Browser.goto\":\n            self.browser_is_active = True\n        elif command_name == \"Browser.close_tab\":\n            self.browser_is_active = False\n",
                        "tags": []
                    },
                    "TeamLeader": {
                        "name": "TeamLeader",
                        "path": "metagpt/roles/di/team_leader.py",
                        "schemas": {
                            "type": "class",
                            "description": "A role who can think and act dynamically",
                            "methods": {
                                "draft_plan": {
                                    "type": "async_function",
                                    "description": "Present a development plan to human for confirmation before distributing tasks to team members. ",
                                    "signature": "(self, content: 'str') -> 'str'",
                                    "parameters": "Args: content: A string containing numbered plan items (e.g., \"1. Task A 2. Task B 3. Task C\") Returns: str: The human's response/confirmed plan, or error message if format is incorrect"
                                },
                                "publish_team_message": {
                                    "type": "function",
                                    "description": "Publish a message to a team member, use member name to fill send_to args. You may copy the full original content or add additional information from upstream. This will make team members start their work. DONT omit any necessary info such as path, link, environment, programming language, framework, requirement, constraint from original content to team members because you are their sole info source.",
                                    "signature": "(self, content: 'str', send_to: 'str')",
                                    "parameters": ""
                                }
                            },
                            "tool_path": "metagpt/roles/di/team_leader.py"
                        },
                        "code": "@register_tool(include_functions=[\"publish_team_message\", \"draft_plan\"])\nclass TeamLeader(RoleZero):\n    name: str = TEAMLEADER_NAME\n    profile: str = \"Team Leader\"\n    goal: str = \"Manage a team to assist users\"\n    example: str = TL_EXAMPLE\n\n    # TeamLeader only reacts once each time, but may encounter errors or need to ask human, thus allowing 2 more turns\n    max_react_loop: int = 10\n\n    tools: list[str] = [\n        \"SearchEnhancedQA\",\n        \"Plan\",\n        \"RoleZero\",\n        \"TeamLeader\",\n        \"Editor:read,similarity_search\",\n        \"Terminal:run\",\n    ]\n\n    use_summary: bool = False\n\n    def _update_tool_execution(self):\n        self.tool_execution_map.update(\n            {\n                \"TeamLeader.publish_team_message\": self.publish_team_message,\n                \"TeamLeader.publish_message\": self.publish_team_message,  # alias\n                \"TeamLeader.draft_plan\": self.draft_plan,\n            }\n        )\n\n    def _get_team_info(self) -> str:\n        if not self.rc.env:\n            return \"\"\n        team_info = \"\"\n        for role in self.rc.env.roles.values():\n            # if role.profile == \"Team Leader\":\n            #     continue\n            team_info += f\"{role.name}: {role.profile}, {role.goal}\\n\"\n\n        return team_info\n\n    def _get_prefix(self) -> str:\n        role_info = super()._get_prefix()\n        team_info = self._get_team_info()\n        return TL_INFO.format(role_info=role_info, team_info=team_info)\n\n    async def _think(self) -> bool:\n        self.instruction = TL_INSTRUCTION.format(\n            team_info=self._get_team_info(), backend_info=get_backend_prompt_for_tl()\n        )\n        return await super()._think()\n\n    def publish_message(self, msg: Message, send_to=\"no one\"):\n        \"\"\"Overwrite Role.publish_message, send to no one if called within Role.run, send to the specified role if called dynamically.\"\"\"\n        if not msg:\n            return\n        if not self.rc.env:\n            # If env does not exist, do not publish the message\n            return\n        msg.send_to = send_to\n        self.rc.env.publish_message(msg, publicer=self.profile)\n\n    def publish_team_message(self, content: str, send_to: str):\n        \"\"\"\n        Publish a message to a team member, use member name to fill send_to args. You may copy the full original content or add additional information from upstream. This will make team members start their work.\n        DONT omit any necessary info such as path, link, environment, programming language, framework, requirement, constraint from original content to team members because you are their sole info source.\n        \"\"\"\n        self._set_state(-1)  # each time publishing a message, pause to wait for the response\n        if send_to == self.name:\n            return  # Avoid sending message to self\n        # Specify the outer send_to to overwrite the default \"no one\" value. Use UserMessage because message from self is like a user request for others.\n        self.publish_message(\n            UserMessage(content=content, sent_from=self.name, send_to=send_to, cause_by=RunCommand), send_to=send_to\n        )\n\n    async def draft_plan(self, content: str) -> str:\n        \"\"\"\n        Present a development plan to human for confirmation before distributing tasks to team members.\n\n        Args:\n            content: A string containing numbered plan items (e.g., \"1. Task A\\n2. Task B\\n3. Task C\")\n\n        Returns:\n            str: The human's response/confirmed plan, or error message if format is incorrect\n        \"\"\"\n\n        # Extract lines without numbers for task list\n        lines_without_number = re.findall(r\"^\\d+\\.\\s+(.+)$\", content, re.MULTILINE)\n        user_response = await get_human_input(prompt=content, tasks=lines_without_number)\n        return parse_plan_confirmation(user_response, role=\"TeamLeader\")\n\n    def finish_current_task(self):\n        self.planner.plan.finish_current_task()\n        self.rc.memory.add(AIMessage(content=FINISH_CURRENT_TASK_CMD))\n",
                        "tags": []
                    },
                    "Editor": {
                        "name": "Editor",
                        "path": "metagpt/tools/libs/editor.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool for reading, understanding, writing, and editing files. Support local file including text-based files (txt, md, json, py, html, js, css, etc.), pdf, docx, excluding images, csv, excel, or online links For agent: Do NOT initiate multiple Editor.insert_content_at_line calls at the same time, since the line number will change starting with the first execution, making line number of the subsequent calls incorrect. Split the calls into separate responses. For the same reason, Editor.insert_content_at_line should NOT go behind Editor.edit_file_by_replace in the same response. Perform insert operation in a separate response.",
                            "methods": {
                                "read": {
                                    "type": "async_function",
                                    "description": "Read the whole content of a file. Using an absolute path as the argument for specifying the file location.",
                                    "signature": "(self, path: str) -> metagpt.tools.libs.editor.FileBlock",
                                    "parameters": ""
                                },
                                "similarity_search": {
                                    "type": "async_function",
                                    "description": "Given a filename or a pathname, performs a similarity search for a given query across the specified file or path. This method searches the index repository for the provided query, classifying the specified files or paths. It performs a search on each cluster of files and handles non-indexed files separately, merging results from structured indices with any direct results from non-indexed files. This function call does not depend on other functions. ",
                                    "signature": "(query: str, path: Union[str, pathlib.Path]) -> List[str]",
                                    "parameters": "Args: query (str): The search query string to look for in the indexed files. path (Union[str, Path]): A pathname or filename to search within. Returns: List[str]: A list of results as strings, containing the text from the merged results and any direct results from non-indexed files. Example: >>> query = \"The problem to be analyzed from the document\" >>> file_or_path = \"The pathname or filename you want to search within\" >>> texts: List[str] = await Editor.similarity_search(query=query, path=file_or_path) >>> print(texts)"
                                }
                            },
                            "tool_path": "metagpt/tools/libs/editor.py"
                        },
                        "code": "@register_tool(\n    include_functions=[\n        \"write\",\n        \"read\",\n        \"open_file\",\n        \"goto_line\",\n        \"scroll_down\",\n        \"scroll_up\",\n        \"create_file\",\n        \"edit_file_by_replace\",\n        \"insert_content_at_line\",\n        \"append_file\",\n        \"search_dir\",\n        \"search_file\",\n        \"find_file\",\n        \"similarity_search\",\n    ]\n)\nclass Editor(BaseModel):\n    \"\"\"\n    A tool for reading, understanding, writing, and editing files.\n    Support local file including text-based files (txt, md, json, py, html, js, css, etc.), pdf, docx, excluding images, csv, excel, or online links\n    For agent: Do NOT initiate multiple Editor.insert_content_at_line calls at the same time, since the line number will change starting with the first execution, making line number of the subsequent calls incorrect. Split the calls into separate responses.\n               For the same reason, Editor.insert_content_at_line should NOT go behind Editor.edit_file_by_replace in the same response. Perform insert operation in a separate response.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    resource: EditorReporter = EditorReporter()\n    current_file: Optional[Path] = None\n    current_line: int = 1\n    window: int = 50\n    enable_auto_lint: bool = True\n    working_dir: Path = DEFAULT_WORKSPACE_ROOT\n\n    async def write(self, path: str, content: str):\n        \"\"\"Write the whole content to a file. When used, make sure content arg contains the full content of the file.\"\"\"\n\n        path = self._try_fix_path(path)\n        content = ensure_string(content)  # Undo possible MCP conversion\n\n        if \"\\n\" not in content and \"\\\\n\" in content:\n            # A very raw rule to correct the content: If 'content' lacks actual newlines ('\\n') but includes '\\\\n', consider\n            # replacing them with '\\n' to potentially correct mistaken representations of newline characters.\n            content = content.replace(\"\\\\n\", \"\\n\")\n        directory = os.path.dirname(path)\n        if directory and not os.path.exists(directory):\n            os.makedirs(directory)\n        async with aiofiles.open(path, \"w\", encoding=\"utf-8\") as f:\n            await f.write(content)\n        # self.resource.report(path, \"path\")\n        return f\"File successfully written and saved to {path}.\"\n\n    def _read_structural_data(self, path: str) -> FileBlock:\n        \"\"\"Read the whole content of a csv file. Using an absolute path as the argument for specifying the file location.\"\"\"\n        if path.endswith(\".csv\"):\n            df = pd.read_csv(path)\n        elif path.endswith(\".xlsx\"):\n            df = pd.read_excel(path)\n        else:\n            raise ValueError(\"The file must be a csv or excel file.\")\n        return FileBlock(path=path, content=f\"Showing head of the data:\\n{df.head().to_string()}\")\n\n    async def read(self, path: str) -> FileBlock:\n        \"\"\"Read the whole content of a file. Using an absolute path as the argument for specifying the file location.\"\"\"\n\n        path = self._try_fix_path(path)\n\n        if path.suffix in STRUCTURAL_DATA_SUFFIX:\n            result = self._read_structural_data(str(path))\n            self.resource.report(str(path), \"path\")\n            return result\n\n        error = FileBlock(\n            path=str(path),\n            content=\"The file is too large to read. Use `Editor.similarity_search` to read the file instead.\",\n        )\n        path = Path(path)\n\n        # Limit the size only for text documents\n        # TODO: Formulate a complete mechanism to handle files of all types and of any size\n        if path.stat().st_size > 5 * DEFAULT_MIN_TOKEN_COUNT and path.suffix in TEXT_DOC_SUFFIX:\n            return error\n\n        content = await File.read_text_file(path)\n        if not content:\n            return FileBlock(path=str(path), content=\"\")\n        if self.is_large_file(content=content):\n            return error\n        self.resource.report(str(path), \"path\")\n\n        result = FileBlock(\n            path=str(path),\n            content=content,\n        )\n        return result\n\n    @staticmethod\n    def _is_valid_filename(file_name: str) -> bool:\n        if not file_name or not file_name.strip():\n            return False\n        invalid_chars = '<>:\"/\\\\|?*'\n        if os.name == \"nt\":  # Windows\n            invalid_chars = '<>:\"/\\\\|?*'\n        elif os.name == \"posix\":  # Unix-like systems\n            invalid_chars = \"\\0\"\n\n        for char in invalid_chars:\n            if char in file_name:\n                return False\n        return True\n\n    @staticmethod\n    def _is_valid_path(path: Path) -> bool:\n        try:\n            return path.exists()\n        except PermissionError:\n            return False\n\n    @staticmethod\n    def _create_paths(file_path: Path) -> bool:\n        try:\n            if file_path.parent:\n                file_path.parent.mkdir(parents=True, exist_ok=True)\n            return True\n        except PermissionError:\n            return False\n\n    def _check_current_file(self, file_path: Optional[Path] = None) -> bool:\n        if file_path is None:\n            file_path = self.current_file\n        if not file_path or not file_path.is_file():\n            raise ValueError(\"No file open. Use the open_file function first.\")\n        return True\n\n    @staticmethod\n    def _clamp(value, min_value, max_value):\n        return max(min_value, min(value, max_value))\n\n    def _lint_file(self, file_path: Path) -> tuple[Optional[str], Optional[int]]:\n        \"\"\"Lint the file at the given path and return a tuple with a boolean indicating if there are errors,\n        and the line number of the first error, if any.\n\n        Returns:\n            tuple[str | None, int | None]: (lint_error, first_error_line_number)\n        \"\"\"\n\n        linter = Linter(root=self.working_dir)\n        lint_error = linter.lint(str(file_path))\n        if not lint_error:\n            # Linting successful. No issues found.\n            return None, None\n        return \"ERRORS:\\n\" + lint_error.text, lint_error.lines[0]\n\n    async def _print_window(self, file_path: Path, targeted_line: int, window: int):\n        self._check_current_file(file_path)\n        async with aiofiles.open(file_path, \"r\") as file:\n            content = await file.read()\n\n            # Ensure the content ends with a newline character\n            if not content.endswith(\"\\n\"):\n                content += \"\\n\"\n\n            lines = content.splitlines(True)  # Keep all line ending characters\n            total_lines = len(lines)\n\n            # cover edge cases\n            self.current_line = self._clamp(targeted_line, 1, total_lines)\n            half_window = max(1, window // 2)\n\n            # Ensure at least one line above and below the targeted line\n            start = max(1, self.current_line - half_window)\n            end = min(total_lines, self.current_line + half_window)\n\n            # Adjust start and end to ensure at least one line above and below\n            if start == 1:\n                end = min(total_lines, start + window - 1)\n            if end == total_lines:\n                start = max(1, end - window + 1)\n\n            output = \"\"\n\n            # only display this when there's at least one line above\n            if start > 1:\n                output += f\"({start - 1} more lines above)\\n\"\n            else:\n                output += \"(this is the beginning of the file)\\n\"\n            for i in range(start, end + 1):\n                _new_line = f\"{i:03d}|{lines[i - 1]}\"\n                if not _new_line.endswith(\"\\n\"):\n                    _new_line += \"\\n\"\n                output += _new_line\n            if end < total_lines:\n                output += f\"({total_lines - end} more lines below)\\n\"\n            else:\n                output += \"(this is the end of the file)\\n\"\n            output = output.rstrip()\n\n            return output\n\n    @staticmethod\n    def _cur_file_header(current_file: Path, total_lines: int) -> str:\n        if not current_file:\n            return \"\"\n        return f\"[File: {current_file.resolve()} ({total_lines} lines total)]\\n\"\n\n    async def set_workdir(self, path: str) -> None:\n        \"\"\"\n        Sets the working directory to the given path. eg: repo directory.\n        You MUST to set it up before open the file.\n\n        Args:\n            path: str: The path to set as the working directory.\n        \"\"\"\n        self.working_dir = Path(path)\n\n    async def open_file(\n        self, path: Union[Path, str], line_number: Optional[int] = 1, context_lines: Optional[int] = None\n    ) -> str:\n        \"\"\"Opens the file at the given path in the editor. If line_number is provided, the window will be moved to include that line.\n        It only shows the first 100 lines by default! Max `context_lines` supported is 2000, use `scroll up/down`\n        to view the file if you want to see more.\n\n        Args:\n            path: str: The path to the file to open, preferred absolute path.\n            line_number: int | None = 1: The line number to move to. Defaults to 1.\n            context_lines: int | None = 100: Only shows this number of lines in the context window (usually from line 1), with line_number as the center (if possible). Defaults to 100.\n        \"\"\"\n        if context_lines is None:\n            context_lines = self.window\n\n        path = self._try_fix_path(path)\n\n        if not path.is_file():\n            raise FileNotFoundError(f\"File {path} not found\")\n\n        self.current_file = path\n\n        total_lines = await self._count_file_lines(file_path=path)\n\n        if not isinstance(line_number, int) or line_number < 1 or line_number > total_lines:\n            raise ValueError(f\"Line number must be between 1 and {total_lines}\")\n        self.current_line = line_number\n\n        # Override WINDOW with context_lines\n        if context_lines is None or context_lines < 1:\n            context_lines = self.window\n\n        output = self._cur_file_header(path, total_lines)\n        output += await self._print_window(path, self.current_line, self._clamp(context_lines, 1, 2000))\n        self.resource.report(path, \"path\")\n        return output\n\n    async def goto_line(self, line_number: int) -> str:\n        \"\"\"Moves the window to show the specified line number.\n\n        Args:\n            line_number: int: The line number to move to.\n        \"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        if not isinstance(line_number, int) or line_number < 1 or line_number > total_lines:\n            raise ValueError(f\"Line number must be between 1 and {total_lines}\")\n\n        self.current_line = self._clamp(line_number, 1, total_lines)\n\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def scroll_down(self) -> str:\n        \"\"\"Moves the window down by 100 lines.\"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        self.current_line = self._clamp(self.current_line + self.window, 1, total_lines)\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def scroll_up(self) -> str:\n        \"\"\"Moves the window up by 100 lines.\"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        self.current_line = self._clamp(self.current_line - self.window, 1, total_lines)\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def create_file(self, filename: str) -> str:\n        \"\"\"Creates and opens a new file with the given name.\n\n        Args:\n            filename: str: The name of the file to create. If the parent directory does not exist, it will be created.\n        \"\"\"\n        filename = self._try_fix_path(filename)\n\n        if filename.exists():\n            raise FileExistsError(f\"File '{filename}' already exists.\")\n        await awrite(filename, \"\\n\")\n\n        self.open_file(filename)\n        return f\"[File {filename} created.]\"\n\n    @staticmethod\n    def _append_impl(lines, content):\n        \"\"\"Internal method to handle appending to a file.\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            content: str: The content to append to the file.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n        \"\"\"\n        content_lines = content.splitlines(keepends=True)\n        n_added_lines = len(content_lines)\n        if lines and not (len(lines) == 1 and lines[0].strip() == \"\"):\n            # file is not empty\n            if not lines[-1].endswith(\"\\n\"):\n                lines[-1] += \"\\n\"\n            new_lines = lines + content_lines\n            content = \"\".join(new_lines)\n        else:\n            # file is empty\n            content = \"\".join(content_lines)\n\n        return content, n_added_lines\n\n    @staticmethod\n    def _insert_impl(lines, start, content):\n        \"\"\"Internal method to handle inserting to a file.\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            start: int: The start line number for inserting.\n            content: str: The content to insert to the file.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n\n        Raises:\n            LineNumberError: If the start line number is invalid.\n        \"\"\"\n        inserted_lines = [content + \"\\n\" if not content.endswith(\"\\n\") else content]\n        if len(lines) == 0:\n            new_lines = inserted_lines\n        elif start is not None:\n            if len(lines) == 1 and lines[0].strip() == \"\":\n                # if the file with only 1 line and that line is empty\n                lines = []\n\n            if len(lines) == 0:\n                new_lines = inserted_lines\n            else:\n                new_lines = lines[: start - 1] + inserted_lines + lines[start - 1 :]\n        else:\n            raise LineNumberError(\n                f\"Invalid line number: {start}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n\n        content = \"\".join(new_lines)\n        n_added_lines = len(inserted_lines)\n        return content, n_added_lines\n\n    @staticmethod\n    def _edit_impl(lines, start, end, content):\n        \"\"\"Internal method to handle editing a file.\n\n        REQUIRES (should be checked by caller):\n            start <= end\n            start and end are between 1 and len(lines) (inclusive)\n            content ends with a newline\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            start: int: The start line number for editing.\n            end: int: The end line number for editing.\n            content: str: The content to replace the lines with.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n        \"\"\"\n        # Handle cases where start or end are None\n        if start is None:\n            start = 1  # Default to the beginning\n        if end is None:\n            end = len(lines)  # Default to the end\n        # Check arguments\n        if not (1 <= start <= len(lines)):\n            raise LineNumberError(\n                f\"Invalid start line number: {start}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n        if not (1 <= end <= len(lines)):\n            raise LineNumberError(\n                f\"Invalid end line number: {end}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n        if start > end:\n            raise LineNumberError(f\"Invalid line range: {start}-{end}. Start must be less than or equal to end.\")\n\n        # Split content into lines and ensure it ends with a newline\n        if not content.endswith(\"\\n\"):\n            content += \"\\n\"\n        content_lines = content.splitlines(True)\n\n        # Calculate the number of lines to be added\n        n_added_lines = len(content_lines)\n\n        # Remove the specified range of lines and insert the new content\n        new_lines = lines[: start - 1] + content_lines + lines[end:]\n\n        # Handle the case where the original lines are empty\n        if len(lines) == 0:\n            new_lines = content_lines\n\n        # Join the lines to create the new content\n        content = \"\".join(new_lines)\n        return content, n_added_lines\n\n    def _get_indentation_info(self, content, first_line):\n        \"\"\"\n        The indentation of the first insert line and the previous line, along with guidance for the next attempt.\n        \"\"\"\n        content_lines = content.split(\"\\n\")\n        pre_line = content_lines[first_line - 2] if first_line - 2 >= 0 else \"\"\n        pre_line_indent = len(pre_line) - len(pre_line.lstrip())\n        insert_line = content_lines[first_line - 1]\n        insert_line_indent = len(insert_line) - len(insert_line.lstrip())\n        ret_str = INDENTATION_INFO.format(\n            pre_line=pre_line,\n            pre_line_indent=pre_line_indent,\n            insert_line=insert_line,\n            insert_line_indent=insert_line_indent,\n            sub_4_space=max(insert_line_indent - 4, 0),\n            add_4_space=insert_line_indent + 4,\n        )\n        return ret_str\n\n    async def _edit_file_impl(\n        self,\n        file_name: Path,\n        start: Optional[int] = None,\n        end: Optional[int] = None,\n        content: str = \"\",\n        is_insert: bool = False,\n        is_append: bool = False,\n    ) -> str:\n        \"\"\"Internal method to handle common logic for edit_/append_file methods.\n\n        Args:\n            file_name: Path: The name of the file to edit or append to.\n            start: int | None = None: The start line number for editing. Ignored if is_append is True.\n            end: int | None = None: The end line number for editing. Ignored if is_append is True.\n            content: str: The content to replace the lines with or to append.\n            is_insert: bool = False: Whether to insert content at the given line number instead of editing.\n            is_append: bool = False: Whether to append content to the file instead of editing.\n        \"\"\"\n\n        ERROR_MSG = f\"[Error editing file {file_name}. Please confirm the file is correct.]\"\n        ERROR_MSG_SUFFIX = (\n            \"Your changes have NOT been applied. Please fix your edit command and try again.\\n\"\n            \"You either need to 1) Open the correct file and try again or 2) Specify the correct line number arguments.\\n\"\n            \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n        )\n\n        if not self._is_valid_filename(file_name.name):\n            raise FileNotFoundError(\"Invalid file name.\")\n\n        if not self._is_valid_path(file_name):\n            raise FileNotFoundError(\"Invalid path or file name.\")\n\n        if not self._create_paths(file_name):\n            raise PermissionError(\"Could not access or create directories.\")\n\n        if not file_name.is_file():\n            raise FileNotFoundError(f\"File {file_name} not found.\")\n\n        if is_insert and is_append:\n            raise ValueError(\"Cannot insert and append at the same time.\")\n\n        # Use a temporary file to write changes\n        content = str(content or \"\")\n        temp_file_path = \"\"\n        src_abs_path = file_name.resolve()\n        first_error_line = None\n        # The file to store previous content and will be removed automatically.\n        temp_backup_file = tempfile.NamedTemporaryFile(\"w\", delete=True)\n\n        try:\n            # lint the original file\n            if self.enable_auto_lint:\n                original_lint_error, _ = self._lint_file(file_name)\n\n            # Create a temporary file\n            with tempfile.NamedTemporaryFile(\"w\", delete=False) as temp_file:\n                temp_file_path = temp_file.name\n\n                # Read the original file and check if empty and for a trailing newline\n                async with aiofiles.open(file_name, \"r\") as original_file:\n                    lines = await original_file.readlines()\n\n                if is_append:\n                    content, n_added_lines = self._append_impl(lines, content)\n                elif is_insert:\n                    try:\n                        content, n_added_lines = self._insert_impl(lines, start, content)\n                    except LineNumberError as e:\n                        return (f\"{ERROR_MSG}\\n\" f\"{e}\\n\" f\"{ERROR_MSG_SUFFIX}\") + \"\\n\"\n                else:\n                    try:\n                        content, n_added_lines = self._edit_impl(lines, start, end, content)\n                    except LineNumberError as e:\n                        return (f\"{ERROR_MSG}\\n\" f\"{e}\\n\" f\"{ERROR_MSG_SUFFIX}\") + \"\\n\"\n\n                if not content.endswith(\"\\n\"):\n                    content += \"\\n\"\n\n                # Write the new content to the temporary file\n                temp_file.write(content)\n\n            # Replace the original file with the temporary file atomically\n            shutil.move(temp_file_path, src_abs_path)\n\n            # Handle linting\n            # NOTE: we need to get env var inside this function\n            # because the env var will be set AFTER the agentskills is imported\n            if self.enable_auto_lint:\n                # BACKUP the original file\n                temp_backup_file.writelines(lines)\n                temp_backup_file.flush()\n                lint_error, first_error_line = self._lint_file(file_name)\n\n                # Select the errors caused by the modification\n                def extract_last_part(line):\n                    parts = line.split(\":\")\n                    if len(parts) > 1:\n                        return parts[-1].strip()\n                    return line.strip()\n\n                def subtract_strings(str1, str2) -> str:\n                    lines1 = str1.splitlines()\n                    lines2 = str2.splitlines()\n\n                    last_parts1 = [extract_last_part(line) for line in lines1]\n\n                    remaining_lines = [line for line in lines2 if extract_last_part(line) not in last_parts1]\n\n                    result = \"\\n\".join(remaining_lines)\n                    return result\n\n                if original_lint_error and lint_error:\n                    lint_error = subtract_strings(original_lint_error, lint_error)\n                    if lint_error == \"\":\n                        lint_error = None\n                        first_error_line = None\n\n                if lint_error is not None:\n                    # show the first insert line.\n                    if is_append:\n                        # original end-of-file\n                        show_line = len(lines)\n                    # insert OR edit WILL provide meaningful line numbers\n                    elif start is not None and end is not None:\n                        show_line = int((start + end) / 2)\n                    else:\n                        raise ValueError(\"Invalid state. This should never happen.\")\n\n                    guidance_message = self._get_indentation_info(content, start or len(lines))\n                    guidance_message += (\n                        \"You either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\\n\"\n                        \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n                    )\n                    lint_error_info = ERROR_GUIDANCE.format(\n                        linter_error_msg=LINTER_ERROR_MSG + lint_error,\n                        window_after_applied=await self._print_window(file_name, show_line, n_added_lines + 20),\n                        window_before_applied=await self._print_window(\n                            Path(temp_backup_file.name), show_line, n_added_lines + 20\n                        ),\n                        guidance_message=guidance_message,\n                    ).strip()\n\n                    # recover the original file\n                    shutil.move(temp_backup_file.name, src_abs_path)\n                    return lint_error_info\n\n        except FileNotFoundError as e:\n            return f\"File not found: {e}\\n\"\n        except IOError as e:\n            return f\"An error occurred while handling the file: {e}\\n\"\n        except ValueError as e:\n            return f\"Invalid input: {e}\\n\"\n        except Exception as e:\n            guidance_message = self._get_indentation_info(content, start or len(lines))\n            guidance_message += (\n                \"You either need to 1) Specify the correct start/end line arguments or 2) Enlarge the range of original code.\\n\"\n                \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n            )\n            error_info = ERROR_GUIDANCE.format(\n                linter_error_msg=LINTER_ERROR_MSG + str(e),\n                window_after_applied=await self._print_window(file_name, start or len(lines), 100),\n                window_before_applied=await self._print_window(Path(temp_backup_file.name), start or len(lines), 100),\n                guidance_message=guidance_message,\n            ).strip()\n            # Clean up the temporary file if an error occurs\n            shutil.move(temp_backup_file.name, src_abs_path)\n            if temp_file_path and Path(temp_file_path).exists():\n                Path(temp_file_path).unlink()\n\n            raise Exception(f\"{error_info}\") from e\n\n        # Update the file information and print the updated content\n        n_total_lines = await self._count_file_lines(file_path=file_name, encoding=\"utf-8\")\n\n        if first_error_line is not None and int(first_error_line) > 0:\n            self.current_line = first_error_line\n        else:\n            if is_append:\n                self.current_line = max(1, len(lines))  # end of original file\n            else:\n                self.current_line = start or n_total_lines or 1\n\n        success_edit_info = SUCCESS_EDIT_INFO.format(\n            file_name=file_name.resolve(),\n            n_total_lines=n_total_lines,\n            window_after_applied=await self._print_window(file_name, self.current_line, self.window),\n            line_number=self.current_line,\n        ).strip()\n\n        return success_edit_info\n\n    async def edit_file_by_replace(self, file_name: str, to_replace: str, new_content: str) -> str:\n        \"\"\"Edit a file. This will search for `to_replace` in the given file and replace it with `new_content`.\n\n        Every *to_replace* must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n        * The `to_replace` parameter should match EXACTLY one or more consecutive lines from the original file. Be mindful of whitespaces!\n        * If the `to_replace` parameter is not unique in the file, the replacement will not be performed. Make sure to include enough context in `to_replace` to make it unique\n        * The `new_content` parameter should contain the edited lines that should replace the `to_replace`\n\n        For example, given a file \"path/to/example.txt\" with the following content:\n        ```\n        line 1\n        line 2\n        line 2\n        line 3\n        ```\n\n        EDITING: If you want to replace the second occurrence of \"line 2\", you can make `to_replace` unique:\n\n        edit_file_by_replace(\n            'path/to/example.txt',\n            to_replace='line 2\\nline 3',\n            new_content='new line\\nline 3',\n        )\n\n        This will replace only the second \"line 2\" with \"new line\". The first \"line 2\" will remain unchanged.\n\n        The resulting file will be:\n        ```\n        line 1\n        line 2\n        new line\n        line 3\n        ```\n\n        REMOVAL: If you want to remove \"line 2\" and \"line 3\", you can set `new_content` to an empty string:\n\n        edit_file_by_replace(\n            'path/to/example.txt',\n            to_replace='line 2\\nline 3',\n            new_content='',\n        )\n        \"\"\"\n        # search for `to_replace` in the file\n        # if found, replace it with `new_content`\n        # if not found, perform a fuzzy search to find the closest match and replace it with `new_content`\n        file_name = self._try_fix_path(file_name)\n\n        async with aiofiles.open(file_name, \"r\") as file:\n            file_content = await file.read()\n\n        if to_replace.strip() == \"\":\n            if file_content.strip() == \"\":\n                raise ValueError(f\"The file '{file_name}' is empty. Please use the append method to add content.\")\n            raise ValueError(\"`to_replace` must not be empty.\")\n\n        to_replace = self._rm_line_no(to_replace)\n        new_content = self._rm_line_no(new_content)\n\n        if file_content.count(to_replace) > 1:\n            raise ValueError(\n                f\"`to_replace` ```{to_replace}``` appears more than once, please include enough lines to make code in `to_replace` unique.\"\n            )\n        elif file_content.count(to_replace) == 0:\n            # If no exact match, try to find close matches using LCS\n            if len(to_replace) > 20:\n                # white spaces are usually missing in multi-line content, thus hard coding a threshold of 20\n                _, similarity, file_content_match = self.lcs(to_replace, file_content)\n                if similarity > 0.9:  # High similarity suggests missing whitespace\n                    raise ValueError(\n                        f\"`to_replace` ```{to_replace}``` not found in {file_name}. Pay attention to spaces and line breaks! Did you actually mean ```{file_content_match}```\"\n                    )\n            raise ValueError(\n                f\"`to_replace` ```{to_replace}``` not found in {file_name}. Read the file carefully with Editor.read and make sure you give the right content to replace. If you want to insert new content, use Editor.insert_content_at_line instead.\"\n            )\n\n        ### Take a easy way to replace the content with direct string operation. Also disable linting for now ###\n        ### TODO: unittest to cover this part ###\n        new_content = file_content.replace(to_replace, new_content)\n        async with aiofiles.open(file_name, \"w\") as file:\n            await file.write(new_content)\n\n        start = file_content.find(to_replace) + 1  # index of starting char\n        start_line_number = file_content[:start].count(\"\\n\") + 1\n        window_after_applied = await self._print_window(file_name, start_line_number, 30)\n        success_edit_info = SUCCESS_EDIT_INFO.format(\n            file_name=file_name.resolve(),\n            n_total_lines=\"\",\n            window_after_applied=window_after_applied,\n            line_number=start_line_number,\n        ).strip()\n        self.resource.report(file_name, \"path\")\n        return success_edit_info\n\n    async def insert_content_at_line(self, file_name: str, line_number: int, insert_content: str) -> str:\n        \"\"\"Insert a complete block of code before the given line number in a file. That is, the new content will start at the beginning of the specified line, and the existing content of that line will be moved down.\n        This operation will NOT modify the content of the lines before or after the given line number.\n        This function can not insert content the end of the file. Please use append_file instead,\n        For example, if the file has the following content:\n        ```\n        001|contain g\n        002|contain h\n        003|contain i\n        004|contain j\n        ```\n        and you call\n        insert_content_at_line(\n            file_name='file.txt',\n            line_number=2,\n            insert_content='new line'\n        )\n        the file will be updated to:\n        ```\n        001|contain g\n        002|new line\n        003|contain h\n        004|contain i\n        005|contain j\n        ```\n\n        Args:\n            file_name: (str): The name of the file to edit.\n            line_number (int): The line number (starting from 1) to insert the content after. The insert content will be add between the line of line_number-1 and line_number\n            insert_content (str): The content to insert betweed the previous_line_content and current_line_content.The insert_content must be a complete block of code at.\n        \"\"\"\n        file_name = self._try_fix_path(file_name)\n        ret_str = await self._edit_file_impl(\n            file_name,\n            start=line_number,\n            end=line_number,\n            content=insert_content,\n            is_insert=True,\n            is_append=False,\n        )\n        self.resource.report(file_name, \"path\")\n        return ret_str\n\n    async def append_file(self, file_name: str, content: str) -> str:\n        \"\"\"Append content to the given file.\n        It appends text `content` to the end of the specified file.\n\n        Args:\n            file_name: str: The name of the file to edit.\n            content: str: The content to insert.\n        \"\"\"\n        file_name = self._try_fix_path(file_name)\n        ret_str = await self._edit_file_impl(\n            file_name,\n            start=None,\n            end=None,\n            content=content,\n            is_insert=False,\n            is_append=True,\n        )\n        self.resource.report(file_name, \"path\")\n        return ret_str\n\n    async def search_dir(self, search_term: str, dir_path: str = \"./\") -> str:\n        \"\"\"Searches for search_term in all files in dir. If dir is not provided, searches in the current directory.\n\n        Args:\n            search_term: str: The term to search for.\n            dir_path: str: The path to the directory to search.\n        \"\"\"\n        dir_path = self._try_fix_path(dir_path)\n        if not dir_path.is_dir():\n            raise FileNotFoundError(f\"Directory {dir_path} not found\")\n\n        matches = []\n        for root, _, files in os.walk(dir_path):\n            for file in files:\n                if file.startswith(\".\"):\n                    continue\n                file_path = Path(root) / file\n                try:\n                    async with aiofiles.open(file_path, \"r\", errors=\"ignore\") as f:\n                        line_num = 0\n                        async for line in f:\n                            line_num += 1\n                            if search_term in line:\n                                matches.append((file_path, line_num, line.strip()))\n                except Exception:\n                    # Skip files that can't be read\n                    continue\n\n        if not matches:\n            return f'No matches found for \"{search_term}\" in {dir_path}'\n\n        num_matches = len(matches)\n        num_files = len(set(match[0] for match in matches))\n\n        if num_files > 100:\n            return f'More than {num_files} files matched for \"{search_term}\" in {dir_path}. Please narrow your search.'\n\n        res_list = [f'[Found {num_matches} matches for \"{search_term}\" in {dir_path}]']\n        for file_path, line_num, line in matches:\n            res_list.append(f\"{file_path} (Line {line_num}): {line}\")\n        res_list.append(f'[End of matches for \"{search_term}\" in {dir_path}]')\n        return \"\\n\".join(res_list)\n\n    async def search_file(self, search_term: str, file_path: Optional[str] = None) -> str:\n        \"\"\"Searches for search_term in file. If file is not provided, searches in the current open file.\n\n        Args:\n            search_term: str: The term to search for.\n            file_path: str | None: The path to the file to search.\n        \"\"\"\n        if file_path is None:\n            file_path = self.current_file\n        else:\n            file_path = self._try_fix_path(file_path)\n        if file_path is None:\n            raise FileNotFoundError(\"No file specified or open. Use the open_file function first.\")\n        if not file_path.is_file():\n            raise FileNotFoundError(f\"File {file_path} not found\")\n\n        matches = []\n        async with aiofiles.open(file_path, \"r\") as file:\n            i = 0\n            async for line in file:\n                i += 1\n                if search_term in line:\n                    matches.append((i, line.strip()))\n\n        res_list = []\n        if matches:\n            res_list.append(f'[Found {len(matches)} matches for \"{search_term}\" in {file_path}]')\n            for match in matches:\n                res_list.append(f\"Line {match[0]}: {match[1]}\")\n            res_list.append(f'[End of matches for \"{search_term}\" in {file_path}]')\n        else:\n            res_list.append(f'[No matches found for \"{search_term}\" in {file_path}]')\n\n        extra = {\"type\": \"search\", \"symbol\": search_term, \"lines\": [i[0] - 1 for i in matches]} if matches else None\n        self.resource.report(file_path, \"path\", extra=extra)\n        return \"\\n\".join(res_list)\n\n    async def find_file(self, file_name: str, dir_path: str = \"./\") -> str:\n        \"\"\"Finds all files with the given name or path in the specified directory.\n\n        Args:\n            file_name: str: The name or path of the file to find.\n                           If it's an absolute path, will match the exact path.\n                           If it's a relative path, will match the path suffix.\n                           If it's just a name, will match file names.\n            dir_path: str: The path to the directory to search.\n        \"\"\"\n        dir_path = self._try_fix_path(dir_path)\n        if not dir_path.is_dir():\n            raise FileNotFoundError(f\"Directory {dir_path} not found\")\n\n        file_path = Path(file_name)\n\n        # TODO: use asyncio.gather to concurrently check multiple paths\n        matches = []\n        for root, _, files in os.walk(dir_path):\n            for file in files:\n                current_path = Path(root) / file\n\n                if file_path.is_absolute():\n                    if current_path == file_path:\n                        matches.append(current_path)\n                elif \"/\" in str(file_path) or \"\\\\\" in str(file_path):\n                    if str(current_path).endswith(str(file_path)):\n                        matches.append(current_path)\n                else:\n                    if current_path.name == file_name:\n                        matches.append(current_path)\n\n        res_list = []\n        if matches:\n            res_list.append(f'[Found {len(matches)} matches for \"{file_name}\" in {dir_path}]')\n            for match in matches:\n                res_list.append(f\"{match}\")\n            res_list.append(f'[End of matches for \"{file_name}\" in {dir_path}]')\n        else:\n            res_list.append(f'[No matches found for \"{file_name}\" in {dir_path}]')\n        return \"\\n\".join(res_list)\n\n    def _try_fix_path(self, path: Union[Path, str]) -> Path:\n        \"\"\"Tries to fix the path if it is not absolute.\"\"\"\n        if not isinstance(path, Path):\n            path = Path(path)\n        if not path.is_absolute():\n            path = self.working_dir / path\n        return path\n\n    @staticmethod\n    def _rm_line_no(content: str) -> str:\n        line_no_pattern = \"\\n[0-9]{3}\\|\"\n        if re.search(line_no_pattern, content):\n            logger.warning(\"Line numbers detected in content and will be removed\")\n            content = re.sub(line_no_pattern, \"\\n\", content)\n        return content\n\n    @staticmethod\n    async def similarity_search(query: str, path: Union[str, Path]) -> List[str]:\n        \"\"\"Given a filename or a pathname, performs a similarity search for a given query across the specified file or path.\n\n        This method searches the index repository for the provided query, classifying the specified\n        files or paths. It performs a search on each cluster of files and handles non-indexed files\n        separately, merging results from structured indices with any direct results from non-indexed files.\n        This function call does not depend on other functions.\n\n        Args:\n            query (str): The search query string to look for in the indexed files.\n            path (Union[str, Path]): A pathname or filename to search within.\n\n        Returns:\n            List[str]: A list of results as strings, containing the text from the merged results\n                        and any direct results from non-indexed files.\n\n        Example:\n            >>> query = \"The problem to be analyzed from the document\"\n            >>> file_or_path = \"The pathname or filename you want to search within\"\n            >>> texts: List[str] = await Editor.similarity_search(query=query, path=file_or_path)\n            >>> print(texts)\n        \"\"\"\n        return await IndexRepo.cross_repo_search(query=query, file_or_path=path)\n\n    @staticmethod\n    def is_large_file(content: str, mix_token_count: int = 0) -> bool:\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n        token_count = len(encoding.encode(content))\n        mix_token_count = mix_token_count or DEFAULT_MIN_TOKEN_COUNT\n        return token_count >= mix_token_count\n\n    @staticmethod\n    def lcs(s1: str, s2: str) -> tuple[str, float, str]:\n        \"\"\"Find the longest common subsequence between two strings and the matching section from s2. Assuming s1 is always a subsequence of s2.\n\n        Args:\n            s1: First string (typically the to_replace content)\n            s2: Second string (typically the file content)\n\n        Returns:\n            tuple[str, float, str]: The LCS string, its similarity score (0-1), and the matching section from s2\n        \"\"\"\n        # Create DP table\n        m, n = len(s1), len(s2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n\n        # Fill DP table\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if s1[i - 1] == s2[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n\n        # Reconstruct LCS and track positions in s2\n        lcs = []\n        i, j = m, n\n        positions = []  # Store positions of matches in s2\n        while i > 0 and j > 0:\n            if s1[i - 1] == s2[j - 1]:\n                lcs.append(s1[i - 1])\n                positions.append(j - 1)\n                i -= 1\n                j -= 1\n            elif dp[i - 1][j] > dp[i][j - 1]:\n                i -= 1\n            else:\n                j -= 1\n\n        lcs = \"\".join(reversed(lcs))\n        positions = list(reversed(positions))\n\n        # Calculate similarity score\n        score = len(lcs) / len(s1)\n\n        # Extract the matching section from s2\n        if positions:\n            start = min(positions)\n            end = max(positions) + 1\n            matching_section = s2[start:end]\n        else:\n            matching_section = \"\"\n        matching_section = matching_section[\n            : int(1.2 * len(s1))\n        ]  # hard truncate to avoid greedily match too long string\n\n        return lcs, score, matching_section\n\n    async def _count_file_lines(self, file_path: Union[Path, str], encoding: Optional[str] = None) -> int:\n        \"\"\"Asynchronously count the total number of lines in a file.\n\n        Args:\n            file_path: Path to the file.\n            encoding: The encoding used to open the file. Defaults to None (system default encoding).\n\n        Returns:\n            int: Total number of lines in the file, minimum 1.\n        \"\"\"\n        total_lines = 0\n        async with aiofiles.open(file_path, \"r\", encoding=encoding) as file:\n            async for _ in file:\n                total_lines += 1\n        return max(1, total_lines)\n\n    async def remove_file(self, file_path: Union[Path, str], missing_ok: bool = True) -> str:\n        \"\"\"Remove a file from the file system.\n\n        Args:\n            file_path: Path to the file to remove.\n            missing_ok: If True, don't raise error if file doesn't exist.\n        \"\"\"\n        path = self._try_fix_path(file_path)\n        path.unlink(missing_ok=missing_ok)\n        await self.resource.async_report(file_path, \"delete\")\n        return f\"File {path} removed successfully\"\n",
                        "tags": []
                    },
                    "Terminal": {
                        "name": "Terminal",
                        "path": "metagpt/tools/libs/terminal.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool for running terminal commands. Don't initialize a new instance of this class if one already exists.",
                            "methods": {
                                "run": {
                                    "type": "async_function",
                                    "description": "Executes a specified command in the terminal and streams the output back in real time. ",
                                    "signature": "(self, cmd: str) -> str",
                                    "parameters": "Args: cmd (str): The command to execute in the terminal. Returns: str: The command's output."
                                }
                            },
                            "tool_path": "metagpt/tools/libs/terminal.py"
                        },
                        "code": "@register_tool(include_functions=[\"run\"])\nclass Terminal(BaseModel):\n    \"\"\"A tool for running terminal commands. Don't initialize a new instance of this class if one already exists.\"\"\"\n\n    tabs: dict[str, Tab] = {}\n    current_tab_id: str = \"\"\n    current_tab: Optional[Tab] = Field(None, exclude=True)\n    #  The cmd in forbidden_terminal_commands will be replace by pass ana return the advise. example:{\"cmd\":\"forbidden_reason/advice\"}\n    forbidden_commands: dict[str, str] = {\n        # \"run dev\": \"Use Deployer.deploy_to_public instead.\",\n        \"run preview\": \"Use Deployer.deploy_to_public instead.\",\n        # serve cmd have a space behind it,\n        \"serve \": \"Use Deployer.deploy_to_public instead.\",\n        \"python -m http.server\": \"Use python -u -m http.server port_number instead.\",  # force buffer flush for http.server\n    }\n    timeout: float = 300.0  # timeout for reading output\n    working_dir: str = Field(default_factory=lambda: str(DEFAULT_WORKSPACE_ROOT.absolute()))\n\n    @model_validator(mode=\"after\")\n    def valid_current_tab(self):\n        if self.current_tab_id:\n            self.current_tab = self.tabs[self.current_tab_id]\n        return self\n\n    async def switch_tab(self, tab_id: str = \"\") -> str:\n        \"\"\"Switch tab based on tab_id. Useful for checking out new output from detached tabs or typing on desired tabs.\"\"\"\n        # NOTE: Hide from agent for now (not registered), need to solve the echo END_MARKER_VALUE problem\n        if tab_id in self.tabs:\n            self.current_tab = self.tabs[tab_id]\n            self.current_tab_id = tab_id\n            tab_new_output = self.current_tab.read_background_output()\n            return f\"Switched to tab {tab_id}, pwd is {self.current_tab.cwd}, the tab has new output: {tab_new_output}\"\n        return f\"Tab {tab_id} not found, created tabs are {list(self.tabs.keys())}\"\n\n    async def _create_new_tab(self) -> str:\n        \"\"\"create a new tab and switch to it\"\"\"\n        new_tab_id = f\"{len(self.tabs):02}\"\n        new_tab = Tab(tab_id=new_tab_id, cwd=self.cwd)\n        await new_tab.start()\n        self.tabs.update({new_tab_id: new_tab})\n        switch_tab_info = await self.switch_tab(new_tab_id)\n        return f\"Tab {new_tab_id} created. \" + switch_tab_info\n\n    @property\n    def cwd(self):\n        return self.current_tab.cwd if self.current_tab else self.working_dir\n\n    async def get_cwd(self):\n        return self.cwd\n\n    async def run(self, cmd: str) -> str:\n        \"\"\"\n        Executes a specified command in the terminal and streams the output back in real time.\n\n        Args:\n            cmd (str): The command to execute in the terminal.\n\n        Returns:\n            str: The command's output.\n        \"\"\"\n        if not self.current_tab:\n            await self._create_new_tab()\n\n        output = \"\"\n        # Precheck for forbidden commands\n        commands = re.split(r\"\\s*&&\\s*\", cmd)\n        for cmd_name, reason in self.forbidden_commands.items():\n            for command in commands:\n                if cmd_name in command:\n                    output += f\"{command} is forbidden. {reason}\\n\"\n        if output:\n            output = f\"Command not executed due to forbidden syntax:\\n{output}\\nPlease rewrite the whole command for execution.\"\n            return output\n\n        cmd = \" && \".join(commands)\n\n        # Send the command\n        current_tab = self.current_tab\n        if current_tab.is_running:\n            return (\n                \"Cannot execute the command because a previous one is still running in the current terminal tab\"\n                f\" (ID: {self.current_tab_id}).\"\n            )\n\n        # clear the output queue before execute command\n        output_queue = current_tab.output_queue\n        while not output_queue.empty():\n            output_queue.get_nowait()\n\n        await current_tab.execute(cmd)\n\n        tmp = []\n        is_service_flag = False\n        while True:\n            try:\n                # shorter timeout for service process by detecting output patterns such as localhost:port, ip:port\n                timeout = self.timeout if not is_service_flag else 3\n                line = await asyncio.wait_for(output_queue.get(), timeout=timeout)\n                if line is None:\n                    # ensure the read_and_process_output task is done\n                    await current_tab.task\n                    break\n                is_service_flag = is_service_flag or is_service_process(line)  # if True already, skip checking\n                tmp.append(line)\n            except asyncio.TimeoutError:\n                output_so_far = \"\".join(tmp)\n                if (returncode := current_tab.process.returncode) is not None:\n                    msg = f\"The terminal has exited with code {returncode}\"\n                    logger.warning(msg)\n                    return f\"{msg}, currently with output: {output_so_far}\"\n\n                current_tab.update_cwd()  # update cwd for the command still running\n                logger.info(f\"No more output after {timeout}s, detached from current tab and switched to a new tab\")\n\n                detached_tab_id = self.current_tab_id\n                new_tab_info = await self._create_new_tab()\n                instruction = DETACH_PROMPT.format(\n                    detached_tab_id=detached_tab_id,\n                    output_so_far=output_so_far,\n                    new_tab_info=new_tab_info,\n                )\n                # print(instruction)\n                return instruction\n        return \"\".join(tmp)\n\n    async def preview(self, tab_id: str, port: int, proj_name: str) -> str:\n        \"\"\"Preview a web project by forwarding a local port to public. Specify the id of the tab that runs the service, which is usually not the current tab but some detached tab.\"\"\"\n        if tab_id not in self.tabs:\n            return f\"Tab {tab_id} not found, created tabs are {list(self.tabs.keys())}, specify the correct tab_id that runs the service.\"\n        return await self.tabs[tab_id].preview(port, proj_name)\n\n    async def execute_in_conda_env(self, cmd: str, env) -> str:\n        \"\"\"\n        Executes a given command within a specified Conda environment automatically without\n        the need for manual activation. Users just need to provide the name of the Conda\n        environment and the command to execute.\n\n        Args:\n            cmd (str): The command to execute within the Conda environment.\n            env (str, optional): The name of the Conda environment to activate before executing the command.\n                                 If not specified, the command will run in the current active environment.\n\n        Returns:\n            str: The command's output, or an empty string if `daemon` is True, with output processed\n                 asynchronously in that case.\n\n        Note:\n            This function wraps `run`, prepending the necessary Conda activation commands\n            to ensure the specified environment is active for the command's execution.\n        \"\"\"\n        cmd = f\"conda run -n {env} {cmd}\"\n        return await self.run(cmd)\n\n    @handle_exception\n    async def close(self):\n        for tab in self.tabs.values():\n            await tab.close()\n        self.tabs = {}\n\n    @handle_exception\n    def __del__(self):\n        \"\"\"Close tabls when Terminal instance is destroyed\"\"\"\n        if self.tabs:\n            loop = asyncio.get_running_loop()\n            loop.create_task(self.close())\n",
                        "tags": []
                    }
                },
                "force": true
            },
            "special_tool_commands": [
                "Plan.finish_current_task",
                "end",
                "Terminal.run",
                "RoleZero.ask_human"
            ],
            "editor": {
                "resource": {
                    "block": "Editor",
                    "uuid": "97aad5e8-9fa3-46bd-b6f2-ccd0778a4e14",
                    "enable_llm_stream": false,
                    "callback_url": ""
                },
                "current_file": null,
                "current_line": 1,
                "window": 50,
                "enable_auto_lint": true,
                "working_dir": "/workspace/ttml_app/main-main"
            },
            "browser": {
                "rp_meta": {
                    "uuid": "96e1bac465054a86849285604e6ddc27",
                    "storage_dir": "/data/chats/28286616f10743a6baaf2bd913b64f39/workspace/.MGXTools",
                    "user_id": 644050,
                    "chat_id": "28286616f10743a6baaf2bd913b64f39",
                    "role_name": "Mike"
                }
            },
            "terminal": {
                "rp_meta": {
                    "uuid": "a2f7db5646aa4051a3747541cd7aefbe",
                    "storage_dir": "/data/chats/28286616f10743a6baaf2bd913b64f39/workspace/.MGXTools",
                    "user_id": 644050,
                    "chat_id": "28286616f10743a6baaf2bd913b64f39",
                    "role_name": "Mike"
                }
            },
            "working_dir": "/workspace/ttml_app/main-main",
            "command_rsp": "I can see you have a comprehensive `COMPLETE_MIGRATION.sql` file that contains all the necessary database schema. This file will create all 7 tables needed for your \"Talk to My Lawyer\" application. Let me help you apply this migration to your Supabase database.\n\n<RoleZero.reply_to_human>\n<content>\nI found your database migration file! The `COMPLETE_MIGRATION.sql` contains the complete schema that will create all 7 tables:\n\n1. **profiles** - User accounts with roles (subscriber, employee, admin)\n2. **employee_coupons** - Employee discount codes and tracking\n3. **subscriptions** - User subscriptions and letter credits\n4. **commissions** - Employee earnings tracking\n5. **letters** - Legal letters with status tracking\n6. **coupon_usage** - Tracks all coupon code usage\n7. **letter_audit_trail** - Complete audit logging\n\n**To set up your database, you have two options:**\n\n**Option 1: Using Supabase SQL Editor (Recommended)**\n1. Go to your Supabase project dashboard\n2. Click on \"SQL Editor\" in the left sidebar\n3. Click \"New Query\"\n4. Copy the entire contents of `/workspace/ttml_app/main-main/supabase/migrations/COMPLETE_MIGRATION.sql`\n5. Paste it into the SQL Editor\n6. Click \"Run\" to execute\n\n**Option 2: Using Supabase CLI**\nIf you have Supabase CLI installed locally:\n```bash\ncd /workspace/ttml_app/main-main\nsupabase db push supabase/migrations/COMPLETE_MIGRATION.sql\n```\n\nAfter running the migration, you'll have all tables created with:\n- Proper indexes for performance\n- Row Level Security (RLS) policies for data protection\n- Database functions for business logic\n- Triggers for automatic profile creation\n- Complete audit trail system\n\nWould you like me to help you with any specific part of the setup, or do you need the SQL file content to copy?\n</content>\n</RoleZero.reply_to_human>\n\n<end></end>",
            "commands": [],
            "memory_k": 30,
            "use_fixed_sop": false,
            "use_summary": false,
            "browser_is_active": false,
            "__module_class_name": "metagpt.roles.di.team_leader.TeamLeader"
        },
        "Emma": {
            "name": "Emma",
            "profile": "Product Manager",
            "goal": "Analyze user needs, create a PRD or Product Research report (including competitive analysis, market research) after collecting information and data online.",
            "constraints": "utilize the same language as the user requirements for seamless communication",
            "desc": "",
            "is_human": false,
            "enable_memory": true,
            "role_id": "",
            "states": [
                "0. RunCommand"
            ],
            "actions": [
                {
                    "name": "RunCommand",
                    "i_context": "",
                    "prefix": "You are a Product Manager, named Emma, your goal is Analyze user needs, create a PRD or Product Research report (including competitive analysis, market research) after collecting information and data online.. the constraint is utilize the same language as the user requirements for seamless communication. ",
                    "desc": "",
                    "__module_class_name": "metagpt.actions.di.run_command.RunCommand"
                }
            ],
            "rc": {
                "memory": {
                    "storage": [
                        {
                            "id": "35cca740059444188b1a9c2c91664ce9",
                            "timestamp": "20251126060539195",
                            "content": "# Project Summary\nThe \"Talk to My Lawyer\" project is a SaaS application designed to facilitate legal document generation and review. It supports three user roles: Subscribers who generate letters, Employees who assist with sales and earn commissions through coupon codes, and Admins who review and approve the generated letters. The platform leverages AI for drafting legal documents and integrates with payment processors for subscriptions.\n\n# Project Module Description\nThe application comprises several functional modules:\n- **User Authentication**: Manages user roles and sessions.\n- **Letter Generation**: Allows subscribers to create legal letters using AI.\n- **Subscription Management**: Handles user subscriptions and coupon codes.\n- **Admin Review**: Provides a dashboard for admins to review, edit, and approve letters.\n- **Email Notifications**: Sends emails to users regarding their letters and subscriptions.\n- **PDF Generation**: Converts approved letters into downloadable PDF format.\n\n# Directory Tree\n```\nttml_app/\n\u251c\u2500\u2500 main-main/\n\u2502   \u251c\u2500\u2500 app/\n\u2502   \u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 dashboard/\n\u2502   \u2502   \u251c\u2500\u2500 globals.css\n\u2502   \u2502   \u2514\u2500\u2500 layout.tsx\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u251c\u2500\u2500 supabase/\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 styles/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 package.json\n```\n\n# File Description Inventory\n- `app/api/`: Contains API routes for handling requests related to user authentication, letter generation, and subscriptions.\n- `app/dashboard/`: Contains components for the admin and user dashboards, including letter management and analytics.\n- `components/`: Reusable UI components used throughout the application.\n- `supabase/`: Contains database migrations and functions related to the Supabase backend.\n- `scripts/`: Includes scripts for database setup and administration tasks.\n- `styles/`: Contains global styles for the application.\n- `README.md`: Documentation for the project setup and usage.\n\n# Technology Stack\n- **Frontend**: Next.js 14, TypeScript, Tailwind CSS\n- **Backend**: Supabase, OpenAI GPT-4\n- **Payment Processing**: Stripe\n- **Email Service**: Resend\n\n# Usage\n1. **Install Dependencies**: Run the command to install required packages.\n2. **Build the Application**: Use the build command to compile the application.\n3. **Run the Application**: Start the application server for local development.\n\n\n# INSTRUCTION\n- You can search for the file path in the `/workspace/`\n- After modifying the project files, if this project can be previewed, then you need to reinstall dependencies, restart service and preview;\n\n\nChange the working directory to `/workspace/ttml_app/main-main`.",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "df14fbea5cb742d489435448bacf7ffe",
                            "timestamp": "20251126061240991",
                            "content": "[Message] from User to Mike: there are 0 tables ",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>",
                                "Mike"
                            ],
                            "metadata": {}
                        }
                    ],
                    "index": {
                        "metagpt.actions.add_requirement.UserRequirement": [
                            {
                                "id": "df14fbea5cb742d489435448bacf7ffe",
                                "timestamp": "20251126061240991",
                                "content": "[Message] from User to Mike: there are 0 tables ",
                                "instruct_content": null,
                                "role": "user",
                                "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                                "sent_from": "",
                                "send_to": [
                                    "<all>",
                                    "Mike"
                                ],
                                "metadata": {}
                            }
                        ]
                    },
                    "ignore_id": false,
                    "appendix": null,
                    "buffer": [
                        {
                            "id": "35cca740059444188b1a9c2c91664ce9",
                            "timestamp": "20251126060539195",
                            "content": "# Project Summary\nThe \"Talk to My Lawyer\" project is a SaaS application designed to facilitate legal document generation and review. It supports three user roles: Subscribers who generate letters, Employees who assist with sales and earn commissions through coupon codes, and Admins who review and approve the generated letters. The platform leverages AI for drafting legal documents and integrates with payment processors for subscriptions.\n\n# Project Module Description\nThe application comprises several functional modules:\n- **User Authentication**: Manages user roles and sessions.\n- **Letter Generation**: Allows subscribers to create legal letters using AI.\n- **Subscription Management**: Handles user subscriptions and coupon codes.\n- **Admin Review**: Provides a dashboard for admins to review, edit, and approve letters.\n- **Email Notifications**: Sends emails to users regarding their letters and subscriptions.\n- **PDF Generation**: Converts approved letters into downloadable PDF format.\n\n# Directory Tree\n```\nttml_app/\n\u251c\u2500\u2500 main-main/\n\u2502   \u251c\u2500\u2500 app/\n\u2502   \u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 dashboard/\n\u2502   \u2502   \u251c\u2500\u2500 globals.css\n\u2502   \u2502   \u2514\u2500\u2500 layout.tsx\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u251c\u2500\u2500 supabase/\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 styles/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 package.json\n```\n\n# File Description Inventory\n- `app/api/`: Contains API routes for handling requests related to user authentication, letter generation, and subscriptions.\n- `app/dashboard/`: Contains components for the admin and user dashboards, including letter management and analytics.\n- `components/`: Reusable UI components used throughout the application.\n- `supabase/`: Contains database migrations and functions related to the Supabase backend.\n- `scripts/`: Includes scripts for database setup and administration tasks.\n- `styles/`: Contains global styles for the application.\n- `README.md`: Documentation for the project setup and usage.\n\n# Technology Stack\n- **Frontend**: Next.js 14, TypeScript, Tailwind CSS\n- **Backend**: Supabase, OpenAI GPT-4\n- **Payment Processing**: Stripe\n- **Email Service**: Resend\n\n# Usage\n1. **Install Dependencies**: Run the command to install required packages.\n2. **Build the Application**: Use the build command to compile the application.\n3. **Run the Application**: Start the application server for local development.\n\n\n# INSTRUCTION\n- You can search for the file path in the `/workspace/`\n- After modifying the project files, if this project can be previewed, then you need to reinstall dependencies, restart service and preview;\n\n\nChange the working directory to `/workspace/ttml_app/main-main`.",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "df14fbea5cb742d489435448bacf7ffe",
                            "timestamp": "20251126061240991",
                            "content": "[Message] from User to Mike: there are 0 tables ",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>",
                                "Mike"
                            ],
                            "metadata": {}
                        }
                    ],
                    "memory_k": 30,
                    "compress_k": 15,
                    "prev_summary": null,
                    "compress_model_name": "gpt-4o-mini",
                    "is_xml_format": null,
                    "compress_trigger_token_threshold": 0
                },
                "working_memory": {
                    "storage": [],
                    "index": {},
                    "ignore_id": false,
                    "appendix": null
                },
                "state": -1,
                "watch": [],
                "react_mode": "react",
                "max_react_loop": 50
            },
            "addresses": [
                "Emma",
                "metagpt.roles.product_manager.ProductManager"
            ],
            "planner": {
                "plan": {
                    "goal": "",
                    "context": "",
                    "tasks": [],
                    "task_map": {},
                    "current_task_id": ""
                },
                "working_memory": {
                    "storage": [],
                    "index": {},
                    "ignore_id": false,
                    "appendix": null
                },
                "auto_run": true
            },
            "recovered": false,
            "latest_observed_msg": null,
            "observe_all_msg_from_buffer": true,
            "system_msg": null,
            "system_prompt": "\n<Basic Info>\n{role_info}\n</Basic Info>\n\n<Domain Info>\n{domain_info}\n</Domain Info>\n\n<Data Structure>\nclass Task(BaseModel):\n    task_id: str = \"\"\n    dependent_task_ids: list[str] = []\n    instruction: str = \"\"\n    task_type: str = \"\"\n    assignee: str = \"\"\n</Data Structure>\n\n<Available Task Types>\n{task_type_desc}\n</Available Task Types>\n\n<Available Commands>\n{available_commands}\nSpecial Command: Use <end></end> to indicate completion of all requirements and termination of the entire workflow.\n</Available Commands>\n\n<Example>\n{example}\n</Example>\n\n<Instruction>\n{instruction}\n\nYou may use any of the available commands. You may output multiple commands, they will be executed sequentially.\nIf the task requires writing multiple files, you should output multiple commands to write files instead of writing one by one.\nIf you finish current task, you will automatically take the next task in the existing plan, use Plan.finish_current_task, DON'T append a new task.\nReview the latest plan's outcome. If your completed task matches the current, consider it finished.\nIn your response, include at least one command. Only use <end></end> when all requirements are met, and please use RoleZero.reply_to_human immediately before <end></end> to report completion.\nDo NOT use <end></end> when waiting for user input or clarification.\n</Instruction>\n\n<Output>\nYou should output a list of commands, follow the format below, replace the <ClassName.method_name> and <args_name> with the actual name.\n\nSome thoughts...\n<ClassName.method_name>\n<args_name1>\nargs_value1\n</args_name1>\n<args_name2>\nargs_value2\n</args_name2>\n</ClassName.method_name>\n\nSome thoughts...\n<ClassName2.method_name2>\n<args_name1>\nargs_value1\n</args_name1>\n<args_name2>\nargs_value2\n</args_name2>\n</ClassName2.method_name2>\n</Output>\n",
            "cmd_prompt": "\n<Current State>\n{current_state}\n</Current State>\n\n<Current Plan>\n{plan_status}\n</Current Plan>\n\n<Current Task>\n{current_task}\n</Current Task>\n\nYour commands (output ONE and ONLY ONE command block, the block can contain one or more commands. Use <end></end> when all requirements are met):\n",
            "instruction": "\nBased on the context, write a plan or modify an existing plan to achieve the goal. A plan consists of one to 3 tasks.\nIf plan is created, you should track the progress and update the plan accordingly, such as Plan.finish_current_task, Plan.append_task, Plan.reset_task, Plan.replace_task, etc.\nWhen presented a current task, tackle the task using the available commands.\nPay close attention to new user message, review the conversation history, use RoleZero.reply_to_human to respond to new user requirement.\nNote:\n1. If you keeping encountering errors, unexpected situation, or you are not sure of proceeding, use RoleZero.ask_human to ask for help.\n2. Carefully review your progress at the current task, if your actions so far has not fulfilled the task instruction, you should continue with current task. Otherwise, finish current task by Plan.finish_current_task explicitly.\n3. Each time you finish a task, use RoleZero.reply_to_human to report your progress.\n4. Don't forget to append task first when all existing tasks are finished and new tasks are required.\n5. Avoid repeating tasks you have already completed. And end loop when all requirements are met.\n",
            "task_type_desc": null,
            "example": "\n## Example 1\nwrite a PRD document for user's request and reply to human\n\n<SearchEnhancedQA.run>\n<query>\nSEARCH_KEY_WORDS\n</query>\n</SearchEnhancedQA.run>\n\n<Editor.write>\n<path>\nprd.md\n</path>\n<content>\nYour PRD document content\n</content>\n</Editor.write>\n\n<RoleZero.reply_to_human>\n<content>\nsome reply text\n</content>\n</RoleZero.reply_to_human>\n\n<end></end>\n",
            "react_mode": "react",
            "max_react_loop": 50,
            "consecutive_react_cnt": 0,
            "max_consecutive_react_limit": 10,
            "tools": [
                "Editor:read,write,edit_file_by_replace,append_file,similarity_search",
                "RoleZero",
                "SearchEnhancedQA",
                "Terminal:run"
            ],
            "mcp_tools": [],
            "tool_recommender": {
                "tools": {
                    "Editor": {
                        "name": "Editor",
                        "path": "metagpt/tools/libs/editor.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool for reading, understanding, writing, and editing files. Support local file including text-based files (txt, md, json, py, html, js, css, etc.), pdf, docx, excluding images, csv, excel, or online links For agent: Do NOT initiate multiple Editor.insert_content_at_line calls at the same time, since the line number will change starting with the first execution, making line number of the subsequent calls incorrect. Split the calls into separate responses. For the same reason, Editor.insert_content_at_line should NOT go behind Editor.edit_file_by_replace in the same response. Perform insert operation in a separate response.",
                            "methods": {
                                "read": {
                                    "type": "async_function",
                                    "description": "Read the whole content of a file. Using an absolute path as the argument for specifying the file location.",
                                    "signature": "(self, path: str) -> metagpt.tools.libs.editor.FileBlock",
                                    "parameters": ""
                                },
                                "write": {
                                    "type": "async_function",
                                    "description": "Write the whole content to a file. When used, make sure content arg contains the full content of the file.",
                                    "signature": "(self, path: str, content: str)",
                                    "parameters": ""
                                },
                                "edit_file_by_replace": {
                                    "type": "async_function",
                                    "description": "Edit a file. This will search for `to_replace` in the given file and replace it with `new_content`. Every *to_replace* must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc. * The `to_replace` parameter should match EXACTLY one or more consecutive lines from the original file. Be mindful of whitespaces! * If the `to_replace` parameter is not unique in the file, the replacement will not be performed. Make sure to include enough context in `to_replace` to make it unique * The `new_content` parameter should contain the edited lines that should replace the `to_replace` For example, given a file \"path/to/example.txt\" with the following content: ``` line 1 line 2 line 2 line 3 ``` EDITING: If you want to replace the second occurrence of \"line 2\", you can make `to_replace` unique: edit_file_by_replace( 'path/to/example.txt', to_replace='line 2 line 3', new_content='new line line 3', ) This will replace only the second \"line 2\" with \"new line\". The first \"line 2\" will remain unchanged. The resulting file will be: ``` line 1 line 2 new line line 3 ``` REMOVAL: If you want to remove \"line 2\" and \"line 3\", you can set `new_content` to an empty string: edit_file_by_replace( 'path/to/example.txt', to_replace='line 2 line 3', new_content='', )",
                                    "signature": "(self, file_name: str, to_replace: str, new_content: str) -> str",
                                    "parameters": ""
                                },
                                "append_file": {
                                    "type": "async_function",
                                    "description": "Append content to the given file. It appends text `content` to the end of the specified file. ",
                                    "signature": "(self, file_name: str, content: str) -> str",
                                    "parameters": "Args: file_name: str: The name of the file to edit. content: str: The content to insert."
                                },
                                "similarity_search": {
                                    "type": "async_function",
                                    "description": "Given a filename or a pathname, performs a similarity search for a given query across the specified file or path. This method searches the index repository for the provided query, classifying the specified files or paths. It performs a search on each cluster of files and handles non-indexed files separately, merging results from structured indices with any direct results from non-indexed files. This function call does not depend on other functions. ",
                                    "signature": "(query: str, path: Union[str, pathlib.Path]) -> List[str]",
                                    "parameters": "Args: query (str): The search query string to look for in the indexed files. path (Union[str, Path]): A pathname or filename to search within. Returns: List[str]: A list of results as strings, containing the text from the merged results and any direct results from non-indexed files. Example: >>> query = \"The problem to be analyzed from the document\" >>> file_or_path = \"The pathname or filename you want to search within\" >>> texts: List[str] = await Editor.similarity_search(query=query, path=file_or_path) >>> print(texts)"
                                }
                            },
                            "tool_path": "metagpt/tools/libs/editor.py"
                        },
                        "code": "@register_tool(\n    include_functions=[\n        \"write\",\n        \"read\",\n        \"open_file\",\n        \"goto_line\",\n        \"scroll_down\",\n        \"scroll_up\",\n        \"create_file\",\n        \"edit_file_by_replace\",\n        \"insert_content_at_line\",\n        \"append_file\",\n        \"search_dir\",\n        \"search_file\",\n        \"find_file\",\n        \"similarity_search\",\n    ]\n)\nclass Editor(BaseModel):\n    \"\"\"\n    A tool for reading, understanding, writing, and editing files.\n    Support local file including text-based files (txt, md, json, py, html, js, css, etc.), pdf, docx, excluding images, csv, excel, or online links\n    For agent: Do NOT initiate multiple Editor.insert_content_at_line calls at the same time, since the line number will change starting with the first execution, making line number of the subsequent calls incorrect. Split the calls into separate responses.\n               For the same reason, Editor.insert_content_at_line should NOT go behind Editor.edit_file_by_replace in the same response. Perform insert operation in a separate response.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    resource: EditorReporter = EditorReporter()\n    current_file: Optional[Path] = None\n    current_line: int = 1\n    window: int = 50\n    enable_auto_lint: bool = True\n    working_dir: Path = DEFAULT_WORKSPACE_ROOT\n\n    async def write(self, path: str, content: str):\n        \"\"\"Write the whole content to a file. When used, make sure content arg contains the full content of the file.\"\"\"\n\n        path = self._try_fix_path(path)\n        content = ensure_string(content)  # Undo possible MCP conversion\n\n        if \"\\n\" not in content and \"\\\\n\" in content:\n            # A very raw rule to correct the content: If 'content' lacks actual newlines ('\\n') but includes '\\\\n', consider\n            # replacing them with '\\n' to potentially correct mistaken representations of newline characters.\n            content = content.replace(\"\\\\n\", \"\\n\")\n        directory = os.path.dirname(path)\n        if directory and not os.path.exists(directory):\n            os.makedirs(directory)\n        async with aiofiles.open(path, \"w\", encoding=\"utf-8\") as f:\n            await f.write(content)\n        # self.resource.report(path, \"path\")\n        return f\"File successfully written and saved to {path}.\"\n\n    def _read_structural_data(self, path: str) -> FileBlock:\n        \"\"\"Read the whole content of a csv file. Using an absolute path as the argument for specifying the file location.\"\"\"\n        if path.endswith(\".csv\"):\n            df = pd.read_csv(path)\n        elif path.endswith(\".xlsx\"):\n            df = pd.read_excel(path)\n        else:\n            raise ValueError(\"The file must be a csv or excel file.\")\n        return FileBlock(path=path, content=f\"Showing head of the data:\\n{df.head().to_string()}\")\n\n    async def read(self, path: str) -> FileBlock:\n        \"\"\"Read the whole content of a file. Using an absolute path as the argument for specifying the file location.\"\"\"\n\n        path = self._try_fix_path(path)\n\n        if path.suffix in STRUCTURAL_DATA_SUFFIX:\n            result = self._read_structural_data(str(path))\n            self.resource.report(str(path), \"path\")\n            return result\n\n        error = FileBlock(\n            path=str(path),\n            content=\"The file is too large to read. Use `Editor.similarity_search` to read the file instead.\",\n        )\n        path = Path(path)\n\n        # Limit the size only for text documents\n        # TODO: Formulate a complete mechanism to handle files of all types and of any size\n        if path.stat().st_size > 5 * DEFAULT_MIN_TOKEN_COUNT and path.suffix in TEXT_DOC_SUFFIX:\n            return error\n\n        content = await File.read_text_file(path)\n        if not content:\n            return FileBlock(path=str(path), content=\"\")\n        if self.is_large_file(content=content):\n            return error\n        self.resource.report(str(path), \"path\")\n\n        result = FileBlock(\n            path=str(path),\n            content=content,\n        )\n        return result\n\n    @staticmethod\n    def _is_valid_filename(file_name: str) -> bool:\n        if not file_name or not file_name.strip():\n            return False\n        invalid_chars = '<>:\"/\\\\|?*'\n        if os.name == \"nt\":  # Windows\n            invalid_chars = '<>:\"/\\\\|?*'\n        elif os.name == \"posix\":  # Unix-like systems\n            invalid_chars = \"\\0\"\n\n        for char in invalid_chars:\n            if char in file_name:\n                return False\n        return True\n\n    @staticmethod\n    def _is_valid_path(path: Path) -> bool:\n        try:\n            return path.exists()\n        except PermissionError:\n            return False\n\n    @staticmethod\n    def _create_paths(file_path: Path) -> bool:\n        try:\n            if file_path.parent:\n                file_path.parent.mkdir(parents=True, exist_ok=True)\n            return True\n        except PermissionError:\n            return False\n\n    def _check_current_file(self, file_path: Optional[Path] = None) -> bool:\n        if file_path is None:\n            file_path = self.current_file\n        if not file_path or not file_path.is_file():\n            raise ValueError(\"No file open. Use the open_file function first.\")\n        return True\n\n    @staticmethod\n    def _clamp(value, min_value, max_value):\n        return max(min_value, min(value, max_value))\n\n    def _lint_file(self, file_path: Path) -> tuple[Optional[str], Optional[int]]:\n        \"\"\"Lint the file at the given path and return a tuple with a boolean indicating if there are errors,\n        and the line number of the first error, if any.\n\n        Returns:\n            tuple[str | None, int | None]: (lint_error, first_error_line_number)\n        \"\"\"\n\n        linter = Linter(root=self.working_dir)\n        lint_error = linter.lint(str(file_path))\n        if not lint_error:\n            # Linting successful. No issues found.\n            return None, None\n        return \"ERRORS:\\n\" + lint_error.text, lint_error.lines[0]\n\n    async def _print_window(self, file_path: Path, targeted_line: int, window: int):\n        self._check_current_file(file_path)\n        async with aiofiles.open(file_path, \"r\") as file:\n            content = await file.read()\n\n            # Ensure the content ends with a newline character\n            if not content.endswith(\"\\n\"):\n                content += \"\\n\"\n\n            lines = content.splitlines(True)  # Keep all line ending characters\n            total_lines = len(lines)\n\n            # cover edge cases\n            self.current_line = self._clamp(targeted_line, 1, total_lines)\n            half_window = max(1, window // 2)\n\n            # Ensure at least one line above and below the targeted line\n            start = max(1, self.current_line - half_window)\n            end = min(total_lines, self.current_line + half_window)\n\n            # Adjust start and end to ensure at least one line above and below\n            if start == 1:\n                end = min(total_lines, start + window - 1)\n            if end == total_lines:\n                start = max(1, end - window + 1)\n\n            output = \"\"\n\n            # only display this when there's at least one line above\n            if start > 1:\n                output += f\"({start - 1} more lines above)\\n\"\n            else:\n                output += \"(this is the beginning of the file)\\n\"\n            for i in range(start, end + 1):\n                _new_line = f\"{i:03d}|{lines[i - 1]}\"\n                if not _new_line.endswith(\"\\n\"):\n                    _new_line += \"\\n\"\n                output += _new_line\n            if end < total_lines:\n                output += f\"({total_lines - end} more lines below)\\n\"\n            else:\n                output += \"(this is the end of the file)\\n\"\n            output = output.rstrip()\n\n            return output\n\n    @staticmethod\n    def _cur_file_header(current_file: Path, total_lines: int) -> str:\n        if not current_file:\n            return \"\"\n        return f\"[File: {current_file.resolve()} ({total_lines} lines total)]\\n\"\n\n    async def set_workdir(self, path: str) -> None:\n        \"\"\"\n        Sets the working directory to the given path. eg: repo directory.\n        You MUST to set it up before open the file.\n\n        Args:\n            path: str: The path to set as the working directory.\n        \"\"\"\n        self.working_dir = Path(path)\n\n    async def open_file(\n        self, path: Union[Path, str], line_number: Optional[int] = 1, context_lines: Optional[int] = None\n    ) -> str:\n        \"\"\"Opens the file at the given path in the editor. If line_number is provided, the window will be moved to include that line.\n        It only shows the first 100 lines by default! Max `context_lines` supported is 2000, use `scroll up/down`\n        to view the file if you want to see more.\n\n        Args:\n            path: str: The path to the file to open, preferred absolute path.\n            line_number: int | None = 1: The line number to move to. Defaults to 1.\n            context_lines: int | None = 100: Only shows this number of lines in the context window (usually from line 1), with line_number as the center (if possible). Defaults to 100.\n        \"\"\"\n        if context_lines is None:\n            context_lines = self.window\n\n        path = self._try_fix_path(path)\n\n        if not path.is_file():\n            raise FileNotFoundError(f\"File {path} not found\")\n\n        self.current_file = path\n\n        total_lines = await self._count_file_lines(file_path=path)\n\n        if not isinstance(line_number, int) or line_number < 1 or line_number > total_lines:\n            raise ValueError(f\"Line number must be between 1 and {total_lines}\")\n        self.current_line = line_number\n\n        # Override WINDOW with context_lines\n        if context_lines is None or context_lines < 1:\n            context_lines = self.window\n\n        output = self._cur_file_header(path, total_lines)\n        output += await self._print_window(path, self.current_line, self._clamp(context_lines, 1, 2000))\n        self.resource.report(path, \"path\")\n        return output\n\n    async def goto_line(self, line_number: int) -> str:\n        \"\"\"Moves the window to show the specified line number.\n\n        Args:\n            line_number: int: The line number to move to.\n        \"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        if not isinstance(line_number, int) or line_number < 1 or line_number > total_lines:\n            raise ValueError(f\"Line number must be between 1 and {total_lines}\")\n\n        self.current_line = self._clamp(line_number, 1, total_lines)\n\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def scroll_down(self) -> str:\n        \"\"\"Moves the window down by 100 lines.\"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        self.current_line = self._clamp(self.current_line + self.window, 1, total_lines)\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def scroll_up(self) -> str:\n        \"\"\"Moves the window up by 100 lines.\"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        self.current_line = self._clamp(self.current_line - self.window, 1, total_lines)\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def create_file(self, filename: str) -> str:\n        \"\"\"Creates and opens a new file with the given name.\n\n        Args:\n            filename: str: The name of the file to create. If the parent directory does not exist, it will be created.\n        \"\"\"\n        filename = self._try_fix_path(filename)\n\n        if filename.exists():\n            raise FileExistsError(f\"File '{filename}' already exists.\")\n        await awrite(filename, \"\\n\")\n\n        self.open_file(filename)\n        return f\"[File {filename} created.]\"\n\n    @staticmethod\n    def _append_impl(lines, content):\n        \"\"\"Internal method to handle appending to a file.\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            content: str: The content to append to the file.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n        \"\"\"\n        content_lines = content.splitlines(keepends=True)\n        n_added_lines = len(content_lines)\n        if lines and not (len(lines) == 1 and lines[0].strip() == \"\"):\n            # file is not empty\n            if not lines[-1].endswith(\"\\n\"):\n                lines[-1] += \"\\n\"\n            new_lines = lines + content_lines\n            content = \"\".join(new_lines)\n        else:\n            # file is empty\n            content = \"\".join(content_lines)\n\n        return content, n_added_lines\n\n    @staticmethod\n    def _insert_impl(lines, start, content):\n        \"\"\"Internal method to handle inserting to a file.\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            start: int: The start line number for inserting.\n            content: str: The content to insert to the file.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n\n        Raises:\n            LineNumberError: If the start line number is invalid.\n        \"\"\"\n        inserted_lines = [content + \"\\n\" if not content.endswith(\"\\n\") else content]\n        if len(lines) == 0:\n            new_lines = inserted_lines\n        elif start is not None:\n            if len(lines) == 1 and lines[0].strip() == \"\":\n                # if the file with only 1 line and that line is empty\n                lines = []\n\n            if len(lines) == 0:\n                new_lines = inserted_lines\n            else:\n                new_lines = lines[: start - 1] + inserted_lines + lines[start - 1 :]\n        else:\n            raise LineNumberError(\n                f\"Invalid line number: {start}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n\n        content = \"\".join(new_lines)\n        n_added_lines = len(inserted_lines)\n        return content, n_added_lines\n\n    @staticmethod\n    def _edit_impl(lines, start, end, content):\n        \"\"\"Internal method to handle editing a file.\n\n        REQUIRES (should be checked by caller):\n            start <= end\n            start and end are between 1 and len(lines) (inclusive)\n            content ends with a newline\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            start: int: The start line number for editing.\n            end: int: The end line number for editing.\n            content: str: The content to replace the lines with.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n        \"\"\"\n        # Handle cases where start or end are None\n        if start is None:\n            start = 1  # Default to the beginning\n        if end is None:\n            end = len(lines)  # Default to the end\n        # Check arguments\n        if not (1 <= start <= len(lines)):\n            raise LineNumberError(\n                f\"Invalid start line number: {start}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n        if not (1 <= end <= len(lines)):\n            raise LineNumberError(\n                f\"Invalid end line number: {end}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n        if start > end:\n            raise LineNumberError(f\"Invalid line range: {start}-{end}. Start must be less than or equal to end.\")\n\n        # Split content into lines and ensure it ends with a newline\n        if not content.endswith(\"\\n\"):\n            content += \"\\n\"\n        content_lines = content.splitlines(True)\n\n        # Calculate the number of lines to be added\n        n_added_lines = len(content_lines)\n\n        # Remove the specified range of lines and insert the new content\n        new_lines = lines[: start - 1] + content_lines + lines[end:]\n\n        # Handle the case where the original lines are empty\n        if len(lines) == 0:\n            new_lines = content_lines\n\n        # Join the lines to create the new content\n        content = \"\".join(new_lines)\n        return content, n_added_lines\n\n    def _get_indentation_info(self, content, first_line):\n        \"\"\"\n        The indentation of the first insert line and the previous line, along with guidance for the next attempt.\n        \"\"\"\n        content_lines = content.split(\"\\n\")\n        pre_line = content_lines[first_line - 2] if first_line - 2 >= 0 else \"\"\n        pre_line_indent = len(pre_line) - len(pre_line.lstrip())\n        insert_line = content_lines[first_line - 1]\n        insert_line_indent = len(insert_line) - len(insert_line.lstrip())\n        ret_str = INDENTATION_INFO.format(\n            pre_line=pre_line,\n            pre_line_indent=pre_line_indent,\n            insert_line=insert_line,\n            insert_line_indent=insert_line_indent,\n            sub_4_space=max(insert_line_indent - 4, 0),\n            add_4_space=insert_line_indent + 4,\n        )\n        return ret_str\n\n    async def _edit_file_impl(\n        self,\n        file_name: Path,\n        start: Optional[int] = None,\n        end: Optional[int] = None,\n        content: str = \"\",\n        is_insert: bool = False,\n        is_append: bool = False,\n    ) -> str:\n        \"\"\"Internal method to handle common logic for edit_/append_file methods.\n\n        Args:\n            file_name: Path: The name of the file to edit or append to.\n            start: int | None = None: The start line number for editing. Ignored if is_append is True.\n            end: int | None = None: The end line number for editing. Ignored if is_append is True.\n            content: str: The content to replace the lines with or to append.\n            is_insert: bool = False: Whether to insert content at the given line number instead of editing.\n            is_append: bool = False: Whether to append content to the file instead of editing.\n        \"\"\"\n\n        ERROR_MSG = f\"[Error editing file {file_name}. Please confirm the file is correct.]\"\n        ERROR_MSG_SUFFIX = (\n            \"Your changes have NOT been applied. Please fix your edit command and try again.\\n\"\n            \"You either need to 1) Open the correct file and try again or 2) Specify the correct line number arguments.\\n\"\n            \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n        )\n\n        if not self._is_valid_filename(file_name.name):\n            raise FileNotFoundError(\"Invalid file name.\")\n\n        if not self._is_valid_path(file_name):\n            raise FileNotFoundError(\"Invalid path or file name.\")\n\n        if not self._create_paths(file_name):\n            raise PermissionError(\"Could not access or create directories.\")\n\n        if not file_name.is_file():\n            raise FileNotFoundError(f\"File {file_name} not found.\")\n\n        if is_insert and is_append:\n            raise ValueError(\"Cannot insert and append at the same time.\")\n\n        # Use a temporary file to write changes\n        content = str(content or \"\")\n        temp_file_path = \"\"\n        src_abs_path = file_name.resolve()\n        first_error_line = None\n        # The file to store previous content and will be removed automatically.\n        temp_backup_file = tempfile.NamedTemporaryFile(\"w\", delete=True)\n\n        try:\n            # lint the original file\n            if self.enable_auto_lint:\n                original_lint_error, _ = self._lint_file(file_name)\n\n            # Create a temporary file\n            with tempfile.NamedTemporaryFile(\"w\", delete=False) as temp_file:\n                temp_file_path = temp_file.name\n\n                # Read the original file and check if empty and for a trailing newline\n                async with aiofiles.open(file_name, \"r\") as original_file:\n                    lines = await original_file.readlines()\n\n                if is_append:\n                    content, n_added_lines = self._append_impl(lines, content)\n                elif is_insert:\n                    try:\n                        content, n_added_lines = self._insert_impl(lines, start, content)\n                    except LineNumberError as e:\n                        return (f\"{ERROR_MSG}\\n\" f\"{e}\\n\" f\"{ERROR_MSG_SUFFIX}\") + \"\\n\"\n                else:\n                    try:\n                        content, n_added_lines = self._edit_impl(lines, start, end, content)\n                    except LineNumberError as e:\n                        return (f\"{ERROR_MSG}\\n\" f\"{e}\\n\" f\"{ERROR_MSG_SUFFIX}\") + \"\\n\"\n\n                if not content.endswith(\"\\n\"):\n                    content += \"\\n\"\n\n                # Write the new content to the temporary file\n                temp_file.write(content)\n\n            # Replace the original file with the temporary file atomically\n            shutil.move(temp_file_path, src_abs_path)\n\n            # Handle linting\n            # NOTE: we need to get env var inside this function\n            # because the env var will be set AFTER the agentskills is imported\n            if self.enable_auto_lint:\n                # BACKUP the original file\n                temp_backup_file.writelines(lines)\n                temp_backup_file.flush()\n                lint_error, first_error_line = self._lint_file(file_name)\n\n                # Select the errors caused by the modification\n                def extract_last_part(line):\n                    parts = line.split(\":\")\n                    if len(parts) > 1:\n                        return parts[-1].strip()\n                    return line.strip()\n\n                def subtract_strings(str1, str2) -> str:\n                    lines1 = str1.splitlines()\n                    lines2 = str2.splitlines()\n\n                    last_parts1 = [extract_last_part(line) for line in lines1]\n\n                    remaining_lines = [line for line in lines2 if extract_last_part(line) not in last_parts1]\n\n                    result = \"\\n\".join(remaining_lines)\n                    return result\n\n                if original_lint_error and lint_error:\n                    lint_error = subtract_strings(original_lint_error, lint_error)\n                    if lint_error == \"\":\n                        lint_error = None\n                        first_error_line = None\n\n                if lint_error is not None:\n                    # show the first insert line.\n                    if is_append:\n                        # original end-of-file\n                        show_line = len(lines)\n                    # insert OR edit WILL provide meaningful line numbers\n                    elif start is not None and end is not None:\n                        show_line = int((start + end) / 2)\n                    else:\n                        raise ValueError(\"Invalid state. This should never happen.\")\n\n                    guidance_message = self._get_indentation_info(content, start or len(lines))\n                    guidance_message += (\n                        \"You either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\\n\"\n                        \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n                    )\n                    lint_error_info = ERROR_GUIDANCE.format(\n                        linter_error_msg=LINTER_ERROR_MSG + lint_error,\n                        window_after_applied=await self._print_window(file_name, show_line, n_added_lines + 20),\n                        window_before_applied=await self._print_window(\n                            Path(temp_backup_file.name), show_line, n_added_lines + 20\n                        ),\n                        guidance_message=guidance_message,\n                    ).strip()\n\n                    # recover the original file\n                    shutil.move(temp_backup_file.name, src_abs_path)\n                    return lint_error_info\n\n        except FileNotFoundError as e:\n            return f\"File not found: {e}\\n\"\n        except IOError as e:\n            return f\"An error occurred while handling the file: {e}\\n\"\n        except ValueError as e:\n            return f\"Invalid input: {e}\\n\"\n        except Exception as e:\n            guidance_message = self._get_indentation_info(content, start or len(lines))\n            guidance_message += (\n                \"You either need to 1) Specify the correct start/end line arguments or 2) Enlarge the range of original code.\\n\"\n                \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n            )\n            error_info = ERROR_GUIDANCE.format(\n                linter_error_msg=LINTER_ERROR_MSG + str(e),\n                window_after_applied=await self._print_window(file_name, start or len(lines), 100),\n                window_before_applied=await self._print_window(Path(temp_backup_file.name), start or len(lines), 100),\n                guidance_message=guidance_message,\n            ).strip()\n            # Clean up the temporary file if an error occurs\n            shutil.move(temp_backup_file.name, src_abs_path)\n            if temp_file_path and Path(temp_file_path).exists():\n                Path(temp_file_path).unlink()\n\n            raise Exception(f\"{error_info}\") from e\n\n        # Update the file information and print the updated content\n        n_total_lines = await self._count_file_lines(file_path=file_name, encoding=\"utf-8\")\n\n        if first_error_line is not None and int(first_error_line) > 0:\n            self.current_line = first_error_line\n        else:\n            if is_append:\n                self.current_line = max(1, len(lines))  # end of original file\n            else:\n                self.current_line = start or n_total_lines or 1\n\n        success_edit_info = SUCCESS_EDIT_INFO.format(\n            file_name=file_name.resolve(),\n            n_total_lines=n_total_lines,\n            window_after_applied=await self._print_window(file_name, self.current_line, self.window),\n            line_number=self.current_line,\n        ).strip()\n\n        return success_edit_info\n\n    async def edit_file_by_replace(self, file_name: str, to_replace: str, new_content: str) -> str:\n        \"\"\"Edit a file. This will search for `to_replace` in the given file and replace it with `new_content`.\n\n        Every *to_replace* must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n        * The `to_replace` parameter should match EXACTLY one or more consecutive lines from the original file. Be mindful of whitespaces!\n        * If the `to_replace` parameter is not unique in the file, the replacement will not be performed. Make sure to include enough context in `to_replace` to make it unique\n        * The `new_content` parameter should contain the edited lines that should replace the `to_replace`\n\n        For example, given a file \"path/to/example.txt\" with the following content:\n        ```\n        line 1\n        line 2\n        line 2\n        line 3\n        ```\n\n        EDITING: If you want to replace the second occurrence of \"line 2\", you can make `to_replace` unique:\n\n        edit_file_by_replace(\n            'path/to/example.txt',\n            to_replace='line 2\\nline 3',\n            new_content='new line\\nline 3',\n        )\n\n        This will replace only the second \"line 2\" with \"new line\". The first \"line 2\" will remain unchanged.\n\n        The resulting file will be:\n        ```\n        line 1\n        line 2\n        new line\n        line 3\n        ```\n\n        REMOVAL: If you want to remove \"line 2\" and \"line 3\", you can set `new_content` to an empty string:\n\n        edit_file_by_replace(\n            'path/to/example.txt',\n            to_replace='line 2\\nline 3',\n            new_content='',\n        )\n        \"\"\"\n        # search for `to_replace` in the file\n        # if found, replace it with `new_content`\n        # if not found, perform a fuzzy search to find the closest match and replace it with `new_content`\n        file_name = self._try_fix_path(file_name)\n\n        async with aiofiles.open(file_name, \"r\") as file:\n            file_content = await file.read()\n\n        if to_replace.strip() == \"\":\n            if file_content.strip() == \"\":\n                raise ValueError(f\"The file '{file_name}' is empty. Please use the append method to add content.\")\n            raise ValueError(\"`to_replace` must not be empty.\")\n\n        to_replace = self._rm_line_no(to_replace)\n        new_content = self._rm_line_no(new_content)\n\n        if file_content.count(to_replace) > 1:\n            raise ValueError(\n                f\"`to_replace` ```{to_replace}``` appears more than once, please include enough lines to make code in `to_replace` unique.\"\n            )\n        elif file_content.count(to_replace) == 0:\n            # If no exact match, try to find close matches using LCS\n            if len(to_replace) > 20:\n                # white spaces are usually missing in multi-line content, thus hard coding a threshold of 20\n                _, similarity, file_content_match = self.lcs(to_replace, file_content)\n                if similarity > 0.9:  # High similarity suggests missing whitespace\n                    raise ValueError(\n                        f\"`to_replace` ```{to_replace}``` not found in {file_name}. Pay attention to spaces and line breaks! Did you actually mean ```{file_content_match}```\"\n                    )\n            raise ValueError(\n                f\"`to_replace` ```{to_replace}``` not found in {file_name}. Read the file carefully with Editor.read and make sure you give the right content to replace. If you want to insert new content, use Editor.insert_content_at_line instead.\"\n            )\n\n        ### Take a easy way to replace the content with direct string operation. Also disable linting for now ###\n        ### TODO: unittest to cover this part ###\n        new_content = file_content.replace(to_replace, new_content)\n        async with aiofiles.open(file_name, \"w\") as file:\n            await file.write(new_content)\n\n        start = file_content.find(to_replace) + 1  # index of starting char\n        start_line_number = file_content[:start].count(\"\\n\") + 1\n        window_after_applied = await self._print_window(file_name, start_line_number, 30)\n        success_edit_info = SUCCESS_EDIT_INFO.format(\n            file_name=file_name.resolve(),\n            n_total_lines=\"\",\n            window_after_applied=window_after_applied,\n            line_number=start_line_number,\n        ).strip()\n        self.resource.report(file_name, \"path\")\n        return success_edit_info\n\n    async def insert_content_at_line(self, file_name: str, line_number: int, insert_content: str) -> str:\n        \"\"\"Insert a complete block of code before the given line number in a file. That is, the new content will start at the beginning of the specified line, and the existing content of that line will be moved down.\n        This operation will NOT modify the content of the lines before or after the given line number.\n        This function can not insert content the end of the file. Please use append_file instead,\n        For example, if the file has the following content:\n        ```\n        001|contain g\n        002|contain h\n        003|contain i\n        004|contain j\n        ```\n        and you call\n        insert_content_at_line(\n            file_name='file.txt',\n            line_number=2,\n            insert_content='new line'\n        )\n        the file will be updated to:\n        ```\n        001|contain g\n        002|new line\n        003|contain h\n        004|contain i\n        005|contain j\n        ```\n\n        Args:\n            file_name: (str): The name of the file to edit.\n            line_number (int): The line number (starting from 1) to insert the content after. The insert content will be add between the line of line_number-1 and line_number\n            insert_content (str): The content to insert betweed the previous_line_content and current_line_content.The insert_content must be a complete block of code at.\n        \"\"\"\n        file_name = self._try_fix_path(file_name)\n        ret_str = await self._edit_file_impl(\n            file_name,\n            start=line_number,\n            end=line_number,\n            content=insert_content,\n            is_insert=True,\n            is_append=False,\n        )\n        self.resource.report(file_name, \"path\")\n        return ret_str\n\n    async def append_file(self, file_name: str, content: str) -> str:\n        \"\"\"Append content to the given file.\n        It appends text `content` to the end of the specified file.\n\n        Args:\n            file_name: str: The name of the file to edit.\n            content: str: The content to insert.\n        \"\"\"\n        file_name = self._try_fix_path(file_name)\n        ret_str = await self._edit_file_impl(\n            file_name,\n            start=None,\n            end=None,\n            content=content,\n            is_insert=False,\n            is_append=True,\n        )\n        self.resource.report(file_name, \"path\")\n        return ret_str\n\n    async def search_dir(self, search_term: str, dir_path: str = \"./\") -> str:\n        \"\"\"Searches for search_term in all files in dir. If dir is not provided, searches in the current directory.\n\n        Args:\n            search_term: str: The term to search for.\n            dir_path: str: The path to the directory to search.\n        \"\"\"\n        dir_path = self._try_fix_path(dir_path)\n        if not dir_path.is_dir():\n            raise FileNotFoundError(f\"Directory {dir_path} not found\")\n\n        matches = []\n        for root, _, files in os.walk(dir_path):\n            for file in files:\n                if file.startswith(\".\"):\n                    continue\n                file_path = Path(root) / file\n                try:\n                    async with aiofiles.open(file_path, \"r\", errors=\"ignore\") as f:\n                        line_num = 0\n                        async for line in f:\n                            line_num += 1\n                            if search_term in line:\n                                matches.append((file_path, line_num, line.strip()))\n                except Exception:\n                    # Skip files that can't be read\n                    continue\n\n        if not matches:\n            return f'No matches found for \"{search_term}\" in {dir_path}'\n\n        num_matches = len(matches)\n        num_files = len(set(match[0] for match in matches))\n\n        if num_files > 100:\n            return f'More than {num_files} files matched for \"{search_term}\" in {dir_path}. Please narrow your search.'\n\n        res_list = [f'[Found {num_matches} matches for \"{search_term}\" in {dir_path}]']\n        for file_path, line_num, line in matches:\n            res_list.append(f\"{file_path} (Line {line_num}): {line}\")\n        res_list.append(f'[End of matches for \"{search_term}\" in {dir_path}]')\n        return \"\\n\".join(res_list)\n\n    async def search_file(self, search_term: str, file_path: Optional[str] = None) -> str:\n        \"\"\"Searches for search_term in file. If file is not provided, searches in the current open file.\n\n        Args:\n            search_term: str: The term to search for.\n            file_path: str | None: The path to the file to search.\n        \"\"\"\n        if file_path is None:\n            file_path = self.current_file\n        else:\n            file_path = self._try_fix_path(file_path)\n        if file_path is None:\n            raise FileNotFoundError(\"No file specified or open. Use the open_file function first.\")\n        if not file_path.is_file():\n            raise FileNotFoundError(f\"File {file_path} not found\")\n\n        matches = []\n        async with aiofiles.open(file_path, \"r\") as file:\n            i = 0\n            async for line in file:\n                i += 1\n                if search_term in line:\n                    matches.append((i, line.strip()))\n\n        res_list = []\n        if matches:\n            res_list.append(f'[Found {len(matches)} matches for \"{search_term}\" in {file_path}]')\n            for match in matches:\n                res_list.append(f\"Line {match[0]}: {match[1]}\")\n            res_list.append(f'[End of matches for \"{search_term}\" in {file_path}]')\n        else:\n            res_list.append(f'[No matches found for \"{search_term}\" in {file_path}]')\n\n        extra = {\"type\": \"search\", \"symbol\": search_term, \"lines\": [i[0] - 1 for i in matches]} if matches else None\n        self.resource.report(file_path, \"path\", extra=extra)\n        return \"\\n\".join(res_list)\n\n    async def find_file(self, file_name: str, dir_path: str = \"./\") -> str:\n        \"\"\"Finds all files with the given name or path in the specified directory.\n\n        Args:\n            file_name: str: The name or path of the file to find.\n                           If it's an absolute path, will match the exact path.\n                           If it's a relative path, will match the path suffix.\n                           If it's just a name, will match file names.\n            dir_path: str: The path to the directory to search.\n        \"\"\"\n        dir_path = self._try_fix_path(dir_path)\n        if not dir_path.is_dir():\n            raise FileNotFoundError(f\"Directory {dir_path} not found\")\n\n        file_path = Path(file_name)\n\n        # TODO: use asyncio.gather to concurrently check multiple paths\n        matches = []\n        for root, _, files in os.walk(dir_path):\n            for file in files:\n                current_path = Path(root) / file\n\n                if file_path.is_absolute():\n                    if current_path == file_path:\n                        matches.append(current_path)\n                elif \"/\" in str(file_path) or \"\\\\\" in str(file_path):\n                    if str(current_path).endswith(str(file_path)):\n                        matches.append(current_path)\n                else:\n                    if current_path.name == file_name:\n                        matches.append(current_path)\n\n        res_list = []\n        if matches:\n            res_list.append(f'[Found {len(matches)} matches for \"{file_name}\" in {dir_path}]')\n            for match in matches:\n                res_list.append(f\"{match}\")\n            res_list.append(f'[End of matches for \"{file_name}\" in {dir_path}]')\n        else:\n            res_list.append(f'[No matches found for \"{file_name}\" in {dir_path}]')\n        return \"\\n\".join(res_list)\n\n    def _try_fix_path(self, path: Union[Path, str]) -> Path:\n        \"\"\"Tries to fix the path if it is not absolute.\"\"\"\n        if not isinstance(path, Path):\n            path = Path(path)\n        if not path.is_absolute():\n            path = self.working_dir / path\n        return path\n\n    @staticmethod\n    def _rm_line_no(content: str) -> str:\n        line_no_pattern = \"\\n[0-9]{3}\\|\"\n        if re.search(line_no_pattern, content):\n            logger.warning(\"Line numbers detected in content and will be removed\")\n            content = re.sub(line_no_pattern, \"\\n\", content)\n        return content\n\n    @staticmethod\n    async def similarity_search(query: str, path: Union[str, Path]) -> List[str]:\n        \"\"\"Given a filename or a pathname, performs a similarity search for a given query across the specified file or path.\n\n        This method searches the index repository for the provided query, classifying the specified\n        files or paths. It performs a search on each cluster of files and handles non-indexed files\n        separately, merging results from structured indices with any direct results from non-indexed files.\n        This function call does not depend on other functions.\n\n        Args:\n            query (str): The search query string to look for in the indexed files.\n            path (Union[str, Path]): A pathname or filename to search within.\n\n        Returns:\n            List[str]: A list of results as strings, containing the text from the merged results\n                        and any direct results from non-indexed files.\n\n        Example:\n            >>> query = \"The problem to be analyzed from the document\"\n            >>> file_or_path = \"The pathname or filename you want to search within\"\n            >>> texts: List[str] = await Editor.similarity_search(query=query, path=file_or_path)\n            >>> print(texts)\n        \"\"\"\n        return await IndexRepo.cross_repo_search(query=query, file_or_path=path)\n\n    @staticmethod\n    def is_large_file(content: str, mix_token_count: int = 0) -> bool:\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n        token_count = len(encoding.encode(content))\n        mix_token_count = mix_token_count or DEFAULT_MIN_TOKEN_COUNT\n        return token_count >= mix_token_count\n\n    @staticmethod\n    def lcs(s1: str, s2: str) -> tuple[str, float, str]:\n        \"\"\"Find the longest common subsequence between two strings and the matching section from s2. Assuming s1 is always a subsequence of s2.\n\n        Args:\n            s1: First string (typically the to_replace content)\n            s2: Second string (typically the file content)\n\n        Returns:\n            tuple[str, float, str]: The LCS string, its similarity score (0-1), and the matching section from s2\n        \"\"\"\n        # Create DP table\n        m, n = len(s1), len(s2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n\n        # Fill DP table\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if s1[i - 1] == s2[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n\n        # Reconstruct LCS and track positions in s2\n        lcs = []\n        i, j = m, n\n        positions = []  # Store positions of matches in s2\n        while i > 0 and j > 0:\n            if s1[i - 1] == s2[j - 1]:\n                lcs.append(s1[i - 1])\n                positions.append(j - 1)\n                i -= 1\n                j -= 1\n            elif dp[i - 1][j] > dp[i][j - 1]:\n                i -= 1\n            else:\n                j -= 1\n\n        lcs = \"\".join(reversed(lcs))\n        positions = list(reversed(positions))\n\n        # Calculate similarity score\n        score = len(lcs) / len(s1)\n\n        # Extract the matching section from s2\n        if positions:\n            start = min(positions)\n            end = max(positions) + 1\n            matching_section = s2[start:end]\n        else:\n            matching_section = \"\"\n        matching_section = matching_section[\n            : int(1.2 * len(s1))\n        ]  # hard truncate to avoid greedily match too long string\n\n        return lcs, score, matching_section\n\n    async def _count_file_lines(self, file_path: Union[Path, str], encoding: Optional[str] = None) -> int:\n        \"\"\"Asynchronously count the total number of lines in a file.\n\n        Args:\n            file_path: Path to the file.\n            encoding: The encoding used to open the file. Defaults to None (system default encoding).\n\n        Returns:\n            int: Total number of lines in the file, minimum 1.\n        \"\"\"\n        total_lines = 0\n        async with aiofiles.open(file_path, \"r\", encoding=encoding) as file:\n            async for _ in file:\n                total_lines += 1\n        return max(1, total_lines)\n\n    async def remove_file(self, file_path: Union[Path, str], missing_ok: bool = True) -> str:\n        \"\"\"Remove a file from the file system.\n\n        Args:\n            file_path: Path to the file to remove.\n            missing_ok: If True, don't raise error if file doesn't exist.\n        \"\"\"\n        path = self._try_fix_path(file_path)\n        path.unlink(missing_ok=missing_ok)\n        await self.resource.async_report(file_path, \"delete\")\n        return f\"File {path} removed successfully\"\n",
                        "tags": []
                    },
                    "RoleZero": {
                        "name": "RoleZero",
                        "path": "metagpt/roles/di/role_zero.py",
                        "schemas": {
                            "type": "class",
                            "description": "A role who can think and act dynamically",
                            "methods": {
                                "ask_human": {
                                    "type": "async_function",
                                    "description": "Use this when you fail the current task or if you are unsure of the situation encountered. Your response should contain a brief summary of your situation, ended with a clear and concise question.",
                                    "signature": "(self, question: 'str') -> 'str'",
                                    "parameters": ""
                                },
                                "reply_to_human": {
                                    "type": "async_function",
                                    "description": "Reply to human user with the content provided. Use this when you have a clear answer or solution to the user's question.",
                                    "signature": "(self, content: 'str') -> 'str'",
                                    "parameters": ""
                                }
                            },
                            "tool_path": "metagpt/roles/di/role_zero.py"
                        },
                        "code": "@register_tool(include_functions=[\"ask_human\", \"reply_to_human\"])\nclass RoleZero(Role):\n    \"\"\"A role who can think and act dynamically\"\"\"\n\n    # Basic Info\n    name: str = \"Zero\"\n    profile: str = \"RoleZero\"\n    goal: str = \"\"\n    system_msg: Optional[list[str]] = None  # Use None to conform to the default value at llm.aask\n    system_prompt: str = SYSTEM_PROMPT  # Use None to conform to the default value at llm.aask\n    cmd_prompt: str = CMD_PROMPT\n    instruction: str = ROLE_INSTRUCTION\n    task_type_desc: Optional[str] = None\n    example: str = \"\"  # global think & act demonstration for the role\n\n    # React Mode\n    react_mode: Literal[\"react\"] = \"react\"\n    max_react_loop: int = 50\n    # consecutive react count after receiving a message, used for triggering human intervention\n    consecutive_react_cnt: int = 0\n    max_consecutive_react_limit: int = 10\n\n    # Tools\n    tools: list[str] = []  # Use special symbol [\"<all>\"] to indicate use of all registered tools\n    mcp_tools: list[str] = []\n    tool_recommender: Optional[ToolRecommender] = None\n    tool_execution_map: Annotated[dict[str, Callable], Field(exclude=True)] = {}\n    special_tool_commands: list[str] = [\"Plan.finish_current_task\", \"end\", \"Terminal.run\", \"RoleZero.ask_human\"]\n\n    # Equipped with three basic tools by default for optional use\n    editor: Annotated[Union[Editor, RemoteProxy], BeforeValidator(ensure_editor_instance)] = Field(\n        default_factory=resolve_editor\n    )\n    browser: Annotated[Union[Browser, RemoteProxy], BeforeValidator(ensure_browser_instance)] = Field(\n        default_factory=resolve_browser\n    )\n    terminal: Annotated[Union[Terminal, RemoteProxy], BeforeValidator(ensure_terminal_instance)] = Field(\n        default_factory=resolve_terminal\n    )\n    working_dir: str = Field(default_factory=lambda: str(DEFAULT_WORKSPACE_ROOT.resolve()))\n\n    # Others\n    observe_all_msg_from_buffer: bool = True\n    command_rsp: str = \"\"  # the raw string containing the commands\n    commands: list[dict] = []  # commands to be executed\n    memory_k: int = 30  # number of memories (messages) to use as historical context\n    use_fixed_sop: bool = False\n    use_summary: bool = True  # whether to summarize at the end\n    browser_is_active: bool = False  # Track browser state to check if it is open\n    think_task: Optional[asyncio.Task] = Field(default=None, exclude=True)  # Track the background thinking task\n\n    # @model_validator(mode=\"after\")\n    # def validate_and_init_mcp_tools(self) -> \"RoleZero\":\n    #     \"\"\"Initialize mcp_tools if empty and MCP servers are enabled\"\"\"\n    #     if not self.mcp_tools:\n    #         if self.config.mcp.has_enabled_servers():\n    #             self.mcp_tools = mcp_manager.list_tool_names()\n    #     return self\n\n    @model_validator(mode=\"after\")\n    def set_plan_and_tool(self) -> \"RoleZero\":\n        # We force using this parameter for DataAnalyst\n        assert self.react_mode == \"react\"\n\n        # Roughly the same part as DataInterpreter.set_plan_and_tool\n        self._set_react_mode(react_mode=self.react_mode, max_react_loop=self.max_react_loop)\n\n        tools = self.tools  # + self.mcp_tools\n        if tools and not self.tool_recommender:\n            self.tool_recommender = BM25ToolRecommender(tools=tools, force=True)\n        self.set_actions([RunCommand])\n\n        # HACK: Init Planner, control it through dynamic thinking; Consider formalizing as a react mode\n        self.planner = Planner(goal=\"\", working_memory=self.rc.working_memory, auto_run=True)\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_tool_execution(self) -> \"RoleZero\":\n        # default map, includes all special tool commands\n        self.tool_execution_map = {\n            \"Plan.append_task\": self.planner.plan.append_task,\n            \"Plan.reset_task\": self.planner.plan.reset_task,\n            \"Plan.replace_task\": self.planner.plan.replace_task,\n            \"Terminal.run\": self.terminal.run,\n            \"Terminal\": self.terminal.run,  # alias\n            \"Terminal.run_command\": self.terminal.run,  # alias\n            \"Terminal.preview\": self.terminal.preview,\n            \"RoleZero.ask_human\": self.ask_human,\n            \"RoleZero.reply_to_human\": self.reply_to_human,\n            \"SearchEnhancedQA.run\": SearchEnhancedQA().run,\n            \"Plan.finish_current_task\": self.planner.plan.finish_current_task,\n            \"end\": self._end,\n        }\n        self.tool_execution_map.update(\n            {\n                f\"Browser.{i}\": getattr(self.browser, i)\n                for i in [\n                    \"click\",\n                    \"close_tab\",\n                    \"go_back\",\n                    \"go_forward\",\n                    \"goto\",\n                    \"hover\",\n                    \"press\",\n                    \"scroll\",\n                    \"tab_focus\",\n                    \"type\",\n                    \"take_screenshot\",\n                ]\n            }\n        )\n        self.tool_execution_map.update(\n            {\n                f\"Editor.{i}\": getattr(self.editor, i)\n                for i in [\n                    \"append_file\",\n                    \"create_file\",\n                    \"edit_file_by_replace\",\n                    \"find_file\",\n                    \"goto_line\",\n                    \"insert_content_at_line\",\n                    \"open_file\",\n                    \"read\",\n                    \"scroll_down\",\n                    \"scroll_up\",\n                    \"search_dir\",\n                    \"search_file\",\n                    \"similarity_search\",\n                    # \"set_workdir\",\n                    \"write\",\n                ]\n            }\n        )\n\n        for tool_name in self.mcp_tools:\n            self.tool_execution_map[tool_name] = mcp_manager.get_tool_executor(tool_name)\n\n        # can be updated by subclass\n        self._update_tool_execution()\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_tool_working_dir(self) -> \"RoleZero\":\n        asyncio.create_task(self._update_workdir_for_tools(self.working_dir))\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_longterm_memory(self) -> \"RoleZero\":\n        \"\"\"Set up special memory for the role if enabled in the configuration.\"\"\"\n\n        # We set Role.rc.memory as SerializeAsAny to retain all fields in Memory subclasses, ensuring model_dump() includes subclass fields.\n        # However, for some of the fields, we want to use the latest values from the config.\n        # To avoid repeated args, we init the subclass first, then update the fields with the config\n        if self.config.role_zero.enable_compressable_memory:\n            kvs = self.rc.memory.model_dump()\n            kvs[\"compress_model_name\"] = self.config.role_zero.compress_model_name\n            self.rc.memory = CompressableMemory(**kvs)\n            self.rc.memory.memory_k = self.config.role_zero.memory_k\n            self.rc.memory.compress_k = self.config.role_zero.compress_k\n            self.rc.memory.compress_trigger_token_threshold = self.config.role_zero.compress_trigger_token_threshold\n            logger.info(f\"Compressable memory set for role '{self.name}'\")\n\n        elif self.config.role_zero.enable_longterm_memory:\n            # Use config.role_zero to initialize long-term memory\n            # The role name will be used as the collection name.\n            self.rc.memory = RoleZeroLongTermMemory(**self.rc.memory.model_dump())\n            memory = self.rc.memory\n            memory.persist_path = self.config.role_zero.longterm_memory_persist_path\n            memory.collection_name = self.name.replace(\" \", \"\")\n            memory.memory_k = self.config.role_zero.memory_k\n            memory.similarity_top_k = self.config.role_zero.similarity_top_k\n            memory.use_llm_ranker = self.config.role_zero.use_llm_ranker\n            logger.info(f\"Long-term memory set for role '{self.name}'\")\n\n        return self\n\n    def _update_tool_execution(self):\n        pass\n\n    def _retrieve_example(self) -> str:\n        \"\"\"Default implementation of example retrieval. Can be overwritten in subclasses.\"\"\"\n        return self.example\n\n    def _get_goal_from_memories(self):\n        \"\"\"Get goal from memories by finding the latest message from TL or UserRequirement\"\"\"\n        memories = self.get_memories()\n        for msg in reversed(memories):\n            if msg.sent_from == \"Mike\" or \"UserRequirement\" in msg.cause_by:\n                return msg.content\n        # Fallback to last message if no matching message found\n        return memories[-1].content if memories else \"\"\n\n    async def _think(self) -> bool:\n        \"\"\"Useful in 'react' mode. Use LLM to decide whether and what to do next.\"\"\"\n        # Compatibility\n        if self.use_fixed_sop:\n            return await super()._think()\n\n        ### 0. Preparation ###\n        if not self.rc.todo:\n            return False\n\n        if not self.planner.plan.goal:\n            self.planner.plan.goal = self._get_goal_from_memories()\n\n        ### 1. Example ###\n        example = self._retrieve_example()\n\n        ### 2. Plan Status ###\n        plan_status, current_task = get_plan_status(planner=self.planner)\n\n        ### 3. Tool/Command Info ###\n        tools = await self.tool_recommender.recommend_tools()\n        tool_info = json.dumps({tool.name: tool.schemas for tool in tools})\n\n        ### Role Instruction ###\n        instruction = self.instruction.strip()\n        system_prompt = self.system_prompt.format(\n            role_info=self._get_prefix(),\n            domain_info=self._get_domain_info(),\n            task_type_desc=self.task_type_desc,\n            available_commands=tool_info,\n            example=example,\n            instruction=instruction,\n        )\n\n        ### Make Decision Dynamically ###\n        prompt = self.cmd_prompt.format(\n            current_state=f\"current directory: {self.working_dir}\",\n            plan_status=plan_status,\n            current_task=current_task,\n        )\n\n        ### Recent Observation ###\n        if self.config.role_zero.enable_compressable_memory:\n            await self.rc.memory.compress(extra_context=super()._get_prefix())\n        memory = self.rc.memory.get(self.memory_k)\n        if self.browser_is_active:\n            memory = await parse_browser_actions(memory, browser=self.browser)\n        # memory = await parse_images(memory, llm=self.llm)\n\n        req = self.llm.format_msg(memory + [UserMessage(content=prompt)])\n\n        state_data = dict(\n            plan_status=plan_status,\n            current_task=current_task,\n            instruction=instruction,\n        )\n        logger.info(f\"{self.name}, ready to think\")\n        # background task to handle LLM thinking process\n        self.create_think_background_task(req, system_prompt, state_data)\n        return True\n\n    def create_think_background_task(self, req, system_prompt, state_data):\n        self.think_task = asyncio.create_task(self._think_background(req, system_prompt, state_data))\n\n    async def _think_background(self, req, system_prompt, state_data):\n        \"\"\"Background task to handle LLM thinking process.\"\"\"\n        async with ThoughtReporter(enable_llm_stream=True) as reporter:\n            await reporter.async_report({\"type\": \"react\"})\n            self.command_rsp = await self.llm_cached_aask(req=req, system_msgs=[system_prompt], state_data=state_data)\n        logger.info(f\"Command response:\\n{self.command_rsp}\")\n        rsp_hist = [mem.content for mem in self.rc.memory.get()]\n        self.command_rsp = await check_duplicates(\n            req=req, command_rsp=self.command_rsp, rsp_hist=rsp_hist, llm=self.llm\n        )\n\n    @exp_cache(context_builder=RoleZeroContextBuilder(), serializer=RoleZeroSerializer())\n    async def llm_cached_aask(self, *, req: list[dict], system_msgs: list[str], **kwargs) -> str:\n        \"\"\"Use `exp_cache` to automatically manage experiences.\n\n        The `RoleZeroContextBuilder` attempts to add experiences to `req`.\n        The `RoleZeroSerializer` extracts essential parts of `req` for the experience pool, trimming lengthy entries to retain only necessary parts.\n        \"\"\"\n        return await self.llm.aask(req, system_msgs=system_msgs)\n\n    def _get_domain_info(self) -> str:\n        return get_time_info() + MGX_INFO + get_enable_supabase_guidance()\n\n    async def _act(self) -> Message:\n        if self.use_fixed_sop:\n            return await super()._act()\n\n        command_outputs = []\n        async for cmd in parse_stream_commands2(role_instance=self, tool_execution_map=self.tool_execution_map):\n            is_success, output = await self._run_command(cmd)\n            command_outputs.append(output)\n            if not is_success:\n                break\n        if not command_outputs:\n            command_outputs = [\"No valid commands found for execution, pay attention to the output format.\"]\n        self.rc.memory.add(AIMessage(content=self.command_rsp))\n        outputs = \"\\n\\n\".join(command_outputs)\n        logger.info(f\"Commands outputs: \\n{outputs}\")\n        self.rc.memory.add(UserMessage(content=outputs, cause_by=RunCommand))\n\n        think_task = self.think_task\n        self.think_task = None\n        await think_task\n\n        return AIMessage(\n            content=f\"I have finished the task, please mark my task as finished. Outputs: {outputs}\",\n            sent_from=self.name,\n            cause_by=RunCommand,\n        )\n\n    async def _react(self) -> Message:\n        # NOTE: Diff 1: Each time landing here means news is observed, set todo to allow news processing in _think\n        self._set_state(0)\n\n        actions_taken = 0\n        self.consecutive_react_cnt = 0\n        rsp = AIMessage(content=\"No actions taken yet\", cause_by=Action)  # will be overwritten after Role _act\n        while actions_taken < self.rc.max_react_loop:\n            # NOTE: Diff 2: Keep observing within _react, news will go into memory, allowing adapting to new info\n            if await self._observe():\n                self.consecutive_react_cnt = 0\n                # Reset state to 0 to override potential `end` command (state = -1) and ensure the loop continues.\n                self._set_state(0)\n            # think\n            has_todo = await self._think()\n            if not has_todo:\n                break\n            # act\n            logger.debug(f\"{self._setting}: {self.rc.state=}, will do {self.rc.todo}\")\n            rsp = await self._act()\n            actions_taken += 1\n            self.consecutive_react_cnt += 1\n\n            # post-check\n            if self.rc.max_react_loop >= 10 and actions_taken >= self.rc.max_react_loop:\n                # If max_react_loop is a small value (e.g. < 10), it is intended to be reached and make the agent stop\n                logger.warning(f\"reached max_react_loop: {actions_taken}\")\n                human_rsp = await self.ask_human(\n                    \"I have reached my max action rounds, do you want me to continue? Yes or no\"\n                )\n                if \"yes\" in human_rsp.lower():\n                    actions_taken = 0\n            if self.consecutive_react_cnt >= self.max_consecutive_react_limit:\n                memory = self.get_memories(k=self.memory_k)\n                context = self.llm.format_msg(memory + [UserMessage(content=SUMMARIZE_STATUS_WHEN_CONSECUTIVE)])\n                question = await self.llm.aask(context)\n                human_rsp = await self.ask_human(question)\n                self.rc.memory.add(UserMessage(content=\"User's extra instruction: \" + human_rsp, cause_by=RunCommand))\n                self.consecutive_react_cnt = 0\n\n        return rsp  # return output from the last action\n\n    async def _run_command(self, cmd) -> tuple[bool, str]:\n        \"\"\"stream command execution\"\"\"\n        output = f\"Command {cmd['command_name']} executed\"\n        try:\n            # handle special command first\n            if self._is_special_command(cmd):\n                special_command_output = await self._run_special_command(cmd)\n                output += \":\" + special_command_output\n                cmd[\"status\"] = \"success\"  # mgx_backend need these fields\n                cmd[\"error_msg\"] = \"\"\n                return True, output\n\n            # run command as specified by tool_execute_map\n            if cmd[\"command_name\"] in self.tool_execution_map:\n                tool_obj = self.tool_execution_map[cmd[\"command_name\"]]\n                if inspect.iscoroutinefunction(tool_obj):\n                    tool_output = await tool_obj(**cmd[\"args\"])\n                else:\n                    tool_output = tool_obj(**cmd[\"args\"])\n                self._try_update_browser_state(cmd[\"command_name\"])\n                if tool_output:\n                    output += f\": {str(tool_output)}\"\n                cmd[\"status\"] = \"success\"\n                cmd[\"error_msg\"] = \"\"\n            else:\n                output = f\"Command {cmd['command_name']} not found.\"\n                cmd[\"status\"] = \"failed\"\n                cmd[\"error_msg\"] = f\"Command {cmd['command_name']} not found.\"\n                return False, output\n\n        except Exception:\n            tb = traceback.format_exc()\n            output += f\": {tb}\"\n            cmd[\"status\"] = \"failed\"\n            cmd[\"error_msg\"] = f\"Command {cmd['command_name']} failed: {tb}\"\n            return False, output\n\n        return True, output\n\n    def _is_special_command(self, cmd) -> bool:\n        return cmd[\"command_name\"] in self.special_tool_commands\n\n    async def _run_special_command(self, cmd) -> str:\n        \"\"\"command requiring special check or parsing\"\"\"\n        command_output = \"\"\n\n        if cmd[\"command_name\"] == \"Plan.finish_current_task\":\n            if not self.planner.plan.is_plan_finished():\n                self.planner.plan.finish_current_task()\n            command_output = (\n                \"Current task is finished. If you no longer need to take action, use the command 'end' to stop.\"\n            )\n\n        elif cmd[\"command_name\"] == \"end\":\n            command_output = await self._end()\n        elif cmd[\"command_name\"] == \"RoleZero.ask_human\":\n            args = cmd[\"args\"]\n            # fault tolerance for llm hallucination\n            question = args.get(\"question\") or args.get(\"content\") or args.get(\"message\")\n            assert question, \"Use kwarg 'question' to provide the question.\"\n            human_response = await self.ask_human(question=question)\n            self.consecutive_react_cnt = 0\n            if human_response.strip().lower().endswith((\"stop\", \"<stop>\")):\n                human_response += \"The user has asked me to stop because I have encountered a problem.\"\n                self.rc.memory.add(UserMessage(content=human_response, cause_by=RunCommand))\n                end_output = \"\\nCommand end executed:\"\n                end_output += await self._end()\n                return end_output\n            return human_response\n        # output from bash.run may be empty, add decorations to the output to ensure visibility.\n        elif \"Terminal\" in cmd[\"command_name\"]:  # Terminal.run and its alias\n            tool_obj = self.tool_execution_map[cmd[\"command_name\"]]\n            tool_output = await tool_obj(**cmd[\"args\"])\n            command_output = format_terminal_output(cmd=cmd, raw_output=tool_output)\n            self.working_dir = await self.terminal.get_cwd()\n            await self._update_workdir_for_tools(self.working_dir)\n            token_count = count_string_tokens(command_output, model_name=\"default\")\n            if token_count >= self.config.llm.max_token * 0.9:\n                command_output = \"OutputTooLongException: the content is too long to use. Please optimize the command parameters to reduce the amount of output.\"\n\n        return command_output\n\n    async def _update_workdir_for_tools(self, new_workdir: str) -> None:\n        \"\"\"Update working directory for all relevant tools.\n        This method can be overridden by subclasses to update additional tools.\n        \"\"\"\n        await self.editor.set_workdir(new_workdir)\n\n    async def ask_human(self, question: str) -> str:\n        \"\"\"Use this when you fail the current task or if you are unsure of the situation encountered. Your response should contain a brief summary of your situation, ended with a clear and concise question.\"\"\"\n        # NOTE: Can be overwritten in remote setting\n        from metagpt.environment.mgx.mgx_env import MGXEnv  # avoid circular import\n\n        if not isinstance(self.rc.env, MGXEnv):\n            return \"Not in MGXEnv, command will not be executed.\"\n        return await self.rc.env.ask_human(question, sent_from=self)\n\n    async def reply_to_human(self, content: str) -> str:\n        \"\"\"Reply to human user with the content provided. Use this when you have a clear answer or solution to the user's question.\"\"\"\n        # NOTE: Can be overwritten in remote setting\n        from metagpt.environment.mgx.mgx_env import MGXEnv  # avoid circular import\n\n        if not isinstance(self.rc.env, MGXEnv):\n            return \"Not in MGXEnv, command will not be executed.\"\n        return await self.rc.env.reply_to_human(content, sent_from=self)\n\n    async def _end(self, **kwarg):\n        self._set_state(-1)\n        memory = self.rc.memory.get(self.memory_k)\n        outputs = \"\"\n        # Summary of the Completed Task and Deliverables\n        if self.use_summary:\n            logger.info(\"end current run and summarize\")\n            async with ThoughtReporter() as reporter:\n                await reporter.async_report({\"type\": \"summary\"})\n                outputs = await self.llm.aask(self.llm.format_msg(memory + [UserMessage(SUMMARY_PROMPT)]))\n        return outputs\n\n    def _try_update_browser_state(self, command_name: str) -> None:\n        \"\"\"Update browser_is_active based on the command_name.\"\"\"\n\n        if command_name == \"Browser.goto\":\n            self.browser_is_active = True\n        elif command_name == \"Browser.close_tab\":\n            self.browser_is_active = False\n",
                        "tags": []
                    },
                    "SearchEnhancedQA": {
                        "name": "SearchEnhancedQA",
                        "path": "metagpt/actions/search_enhanced_qa.py",
                        "schemas": {
                            "type": "class",
                            "description": "Question answering and info searching through search engine.",
                            "methods": {
                                "run": {
                                    "type": "async_function",
                                    "description": "Answer a query by leveraging web search results. ",
                                    "signature": "(self, query: 'str', rewrite_query: 'bool' = True) -> 'str'",
                                    "parameters": "Args: query (str): The original user query. rewrite_query (bool): Whether to rewrite the query for better web search results. Defaults to True. Returns: str: A detailed answer based on web search results. Raises: ValueError: If the query is invalid."
                                }
                            },
                            "tool_path": "metagpt/actions/search_enhanced_qa.py"
                        },
                        "code": "@register_tool(include_functions=[\"run\"])\nclass SearchEnhancedQA(Action):\n    \"\"\"Question answering and info searching through search engine.\"\"\"\n\n    name: str = \"SearchEnhancedQA\"\n    desc: str = \"Integrating search engine results to anwser the question.\"\n\n    collect_links_action: CollectLinks = Field(\n        default_factory=CollectLinks, description=\"Action to collect relevant links from a search engine.\"\n    )\n    web_browse_and_summarize_action: WebBrowseAndSummarize = Field(\n        default=None,\n        description=\"Action to explore the web and provide summaries of articles and webpages.\",\n    )\n    per_page_timeout: float = Field(\n        default=15, description=\"The maximum time for fetching a single page is in seconds. Defaults to 15s.\"\n    )\n    java_script_enabled: bool = Field(\n        default=False, description=\"Whether or not to enable JavaScript in the web browser context. Defaults to False.\"\n    )\n    user_agent: str = Field(\n        default=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.81\",\n        description=\"Specific user agent to use in browser.\",\n    )\n    extra_http_headers: dict = Field(\n        default={\"sec-ch-ua\": 'Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24'},\n        description=\"An object containing additional HTTP headers to be sent with every request.\",\n    )\n    max_chars_per_webpage_summary: int = Field(\n        default=4000, description=\"Maximum summary length for each web page content.\"\n    )\n    max_search_results: int = Field(\n        default=15,\n        description=\"Maximum number of search results (links) to collect using the collect_links_action. This controls the number of potential sources for answering the question.\",\n    )\n\n    _reporter: ThoughtReporter = PrivateAttr(ThoughtReporter())\n\n    @model_validator(mode=\"after\")\n    def initialize(self):\n        if self.web_browse_and_summarize_action is None:\n            web_browser_engine = WebBrowserEngine.from_browser_config(\n                self.config.browser,\n                proxy=self.config.proxy,\n                java_script_enabled=self.java_script_enabled,\n                extra_http_headers=self.extra_http_headers,\n                user_agent=self.user_agent,\n            )\n\n            self.web_browse_and_summarize_action = WebBrowseAndSummarize(web_browser_engine=web_browser_engine)\n\n        return self\n\n    async def run(self, query: str, rewrite_query: bool = True) -> str:\n        \"\"\"Answer a query by leveraging web search results.\n\n        Args:\n            query (str): The original user query.\n            rewrite_query (bool): Whether to rewrite the query for better web search results. Defaults to True.\n\n        Returns:\n            str: A detailed answer based on web search results.\n\n        Raises:\n            ValueError: If the query is invalid.\n        \"\"\"\n        async with self._reporter:\n            await self._reporter.async_report({\"type\": \"search\", \"stage\": \"init\"})\n            self._validate_query(query)\n\n            processed_query = await self._process_query(query, rewrite_query)\n            context = await self._build_context(processed_query)\n\n            return await self._generate_answer(processed_query, context)\n\n    def _validate_query(self, query: str) -> None:\n        \"\"\"Validate the input query.\n\n        Args:\n            query (str): The query to validate.\n\n        Raises:\n            ValueError: If the query is invalid.\n        \"\"\"\n\n        if not query.strip():\n            raise ValueError(\"Query cannot be empty or contain only whitespace.\")\n\n    async def _process_query(self, query: str, should_rewrite: bool) -> str:\n        \"\"\"Process the query, optionally rewriting it.\"\"\"\n\n        if should_rewrite:\n            return await self._rewrite_query(query)\n\n        return query\n\n    async def _rewrite_query(self, query: str) -> str:\n        \"\"\"Write a better search query for web search engine.\n\n        If the rewrite process fails, the original query is returned.\n\n        Args:\n            query (str): The original search query.\n\n        Returns:\n            str: The rewritten query if successful, otherwise the original query.\n        \"\"\"\n\n        prompt = REWRITE_QUERY_PROMPT.format(q=query)\n\n        try:\n            resp = await self._aask(prompt)\n            rewritten_query = self._extract_rewritten_query(resp)\n\n            logger.info(f\"Query rewritten: '{query}' -> '{rewritten_query}'\")\n            return rewritten_query\n        except Exception as e:\n            logger.warning(f\"Query rewrite failed. Returning original query. Error: {e}\")\n            return query\n\n    def _extract_rewritten_query(self, response: str) -> str:\n        \"\"\"Extract the rewritten query from the LLM's JSON response.\"\"\"\n\n        resp_json = json.loads(CodeParser.parse_code(response, lang=\"json\"))\n        return resp_json[\"query\"]\n\n    async def _build_context(self, query: str) -> str:\n        \"\"\"Construct a context string from web search citations.\n\n        Args:\n            query (str): The search query.\n\n        Returns:\n            str: Formatted context with numbered citations.\n        \"\"\"\n\n        citations = await self._search_citations(query)\n        context = \"\\n\\n\".join([f\"[[citation:{i+1}]] {c}\" for i, c in enumerate(citations)])\n\n        return context\n\n    async def _search_citations(self, query: str) -> list[str]:\n        \"\"\"Perform web search and summarize relevant content.\n\n        Args:\n            query (str): The search query.\n\n        Returns:\n            list[str]: Summaries of relevant web content.\n        \"\"\"\n\n        relevant_urls = await self._collect_relevant_links(query)\n        await self._reporter.async_report({\"type\": \"search\", \"stage\": \"searching\", \"urls\": relevant_urls})\n        if not relevant_urls:\n            logger.warning(f\"No relevant URLs found for query: {query}\")\n            return []\n\n        logger.info(f\"The Relevant links are: {relevant_urls}\")\n\n        web_summaries = await self._summarize_web_content(relevant_urls)\n        if not web_summaries:\n            logger.warning(f\"No summaries generated for query: {query}\")\n            return []\n\n        citations = list(web_summaries.values())\n\n        return citations\n\n    async def _collect_relevant_links(self, query: str) -> list[str]:\n        \"\"\"Search and rank URLs relevant to the query.\n\n        Args:\n            query (str): The search query.\n\n        Returns:\n            list[str]: Ranked list of relevant URLs.\n        \"\"\"\n\n        return await self.collect_links_action._search_and_rank_urls(\n            topic=query, query=query, max_num_results=self.max_search_results\n        )\n\n    async def _summarize_web_content(self, urls: list[str]) -> dict[str, str]:\n        \"\"\"Fetch and summarize content from given URLs.\n\n        Args:\n            urls (list[str]): List of URLs to summarize.\n\n        Returns:\n            dict[str, str]: Mapping of URLs to their summaries.\n        \"\"\"\n\n        contents = await self._fetch_web_contents(urls)\n        logger.info(\"Fetching web contents done\")\n\n        summaries = {}\n        await self._reporter.async_report(\n            {\"type\": \"search\", \"stage\": \"browsing\", \"pages\": [i.model_dump() for i in contents]}\n        )\n        for content in contents:\n            url = content.url\n            inner_text = content.inner_text.replace(\"\\n\", \"\")\n            if self.web_browse_and_summarize_action._is_content_invalid(inner_text):\n                logger.warning(f\"Invalid content detected for URL {url}: {inner_text[:10]}...\")\n                continue\n\n            summary = inner_text[: self.max_chars_per_webpage_summary]\n            summaries[url] = summary\n\n        return summaries\n\n    async def _fetch_web_contents(self, urls: list[str]) -> list[WebPage]:\n        return await self.web_browse_and_summarize_action._fetch_web_contents(\n            *urls, per_page_timeout=self.per_page_timeout\n        )\n\n    async def _generate_answer(self, query: str, context: str) -> str:\n        \"\"\"Generate an answer using the query and context.\n\n        Args:\n            query (str): The user's question.\n            context (str): Relevant information from web search.\n\n        Returns:\n            str: Generated answer based on the context.\n        \"\"\"\n\n        system_prompt = SEARCH_ENHANCED_QA_SYSTEM_PROMPT.format(context=context)\n\n        async with ThoughtReporter(uuid=self._reporter.uuid, enable_llm_stream=True) as reporter:\n            await reporter.async_report({\"type\": \"search\", \"stage\": \"answer\"})\n            rsp = await self._aask(query, [system_prompt])\n        return rsp\n",
                        "tags": []
                    },
                    "Terminal": {
                        "name": "Terminal",
                        "path": "metagpt/tools/libs/terminal.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool for running terminal commands. Don't initialize a new instance of this class if one already exists.",
                            "methods": {
                                "run": {
                                    "type": "async_function",
                                    "description": "Executes a specified command in the terminal and streams the output back in real time. ",
                                    "signature": "(self, cmd: str) -> str",
                                    "parameters": "Args: cmd (str): The command to execute in the terminal. Returns: str: The command's output."
                                }
                            },
                            "tool_path": "metagpt/tools/libs/terminal.py"
                        },
                        "code": "@register_tool(include_functions=[\"run\"])\nclass Terminal(BaseModel):\n    \"\"\"A tool for running terminal commands. Don't initialize a new instance of this class if one already exists.\"\"\"\n\n    tabs: dict[str, Tab] = {}\n    current_tab_id: str = \"\"\n    current_tab: Optional[Tab] = Field(None, exclude=True)\n    #  The cmd in forbidden_terminal_commands will be replace by pass ana return the advise. example:{\"cmd\":\"forbidden_reason/advice\"}\n    forbidden_commands: dict[str, str] = {\n        # \"run dev\": \"Use Deployer.deploy_to_public instead.\",\n        \"run preview\": \"Use Deployer.deploy_to_public instead.\",\n        # serve cmd have a space behind it,\n        \"serve \": \"Use Deployer.deploy_to_public instead.\",\n        \"python -m http.server\": \"Use python -u -m http.server port_number instead.\",  # force buffer flush for http.server\n    }\n    timeout: float = 300.0  # timeout for reading output\n    working_dir: str = Field(default_factory=lambda: str(DEFAULT_WORKSPACE_ROOT.absolute()))\n\n    @model_validator(mode=\"after\")\n    def valid_current_tab(self):\n        if self.current_tab_id:\n            self.current_tab = self.tabs[self.current_tab_id]\n        return self\n\n    async def switch_tab(self, tab_id: str = \"\") -> str:\n        \"\"\"Switch tab based on tab_id. Useful for checking out new output from detached tabs or typing on desired tabs.\"\"\"\n        # NOTE: Hide from agent for now (not registered), need to solve the echo END_MARKER_VALUE problem\n        if tab_id in self.tabs:\n            self.current_tab = self.tabs[tab_id]\n            self.current_tab_id = tab_id\n            tab_new_output = self.current_tab.read_background_output()\n            return f\"Switched to tab {tab_id}, pwd is {self.current_tab.cwd}, the tab has new output: {tab_new_output}\"\n        return f\"Tab {tab_id} not found, created tabs are {list(self.tabs.keys())}\"\n\n    async def _create_new_tab(self) -> str:\n        \"\"\"create a new tab and switch to it\"\"\"\n        new_tab_id = f\"{len(self.tabs):02}\"\n        new_tab = Tab(tab_id=new_tab_id, cwd=self.cwd)\n        await new_tab.start()\n        self.tabs.update({new_tab_id: new_tab})\n        switch_tab_info = await self.switch_tab(new_tab_id)\n        return f\"Tab {new_tab_id} created. \" + switch_tab_info\n\n    @property\n    def cwd(self):\n        return self.current_tab.cwd if self.current_tab else self.working_dir\n\n    async def get_cwd(self):\n        return self.cwd\n\n    async def run(self, cmd: str) -> str:\n        \"\"\"\n        Executes a specified command in the terminal and streams the output back in real time.\n\n        Args:\n            cmd (str): The command to execute in the terminal.\n\n        Returns:\n            str: The command's output.\n        \"\"\"\n        if not self.current_tab:\n            await self._create_new_tab()\n\n        output = \"\"\n        # Precheck for forbidden commands\n        commands = re.split(r\"\\s*&&\\s*\", cmd)\n        for cmd_name, reason in self.forbidden_commands.items():\n            for command in commands:\n                if cmd_name in command:\n                    output += f\"{command} is forbidden. {reason}\\n\"\n        if output:\n            output = f\"Command not executed due to forbidden syntax:\\n{output}\\nPlease rewrite the whole command for execution.\"\n            return output\n\n        cmd = \" && \".join(commands)\n\n        # Send the command\n        current_tab = self.current_tab\n        if current_tab.is_running:\n            return (\n                \"Cannot execute the command because a previous one is still running in the current terminal tab\"\n                f\" (ID: {self.current_tab_id}).\"\n            )\n\n        # clear the output queue before execute command\n        output_queue = current_tab.output_queue\n        while not output_queue.empty():\n            output_queue.get_nowait()\n\n        await current_tab.execute(cmd)\n\n        tmp = []\n        is_service_flag = False\n        while True:\n            try:\n                # shorter timeout for service process by detecting output patterns such as localhost:port, ip:port\n                timeout = self.timeout if not is_service_flag else 3\n                line = await asyncio.wait_for(output_queue.get(), timeout=timeout)\n                if line is None:\n                    # ensure the read_and_process_output task is done\n                    await current_tab.task\n                    break\n                is_service_flag = is_service_flag or is_service_process(line)  # if True already, skip checking\n                tmp.append(line)\n            except asyncio.TimeoutError:\n                output_so_far = \"\".join(tmp)\n                if (returncode := current_tab.process.returncode) is not None:\n                    msg = f\"The terminal has exited with code {returncode}\"\n                    logger.warning(msg)\n                    return f\"{msg}, currently with output: {output_so_far}\"\n\n                current_tab.update_cwd()  # update cwd for the command still running\n                logger.info(f\"No more output after {timeout}s, detached from current tab and switched to a new tab\")\n\n                detached_tab_id = self.current_tab_id\n                new_tab_info = await self._create_new_tab()\n                instruction = DETACH_PROMPT.format(\n                    detached_tab_id=detached_tab_id,\n                    output_so_far=output_so_far,\n                    new_tab_info=new_tab_info,\n                )\n                # print(instruction)\n                return instruction\n        return \"\".join(tmp)\n\n    async def preview(self, tab_id: str, port: int, proj_name: str) -> str:\n        \"\"\"Preview a web project by forwarding a local port to public. Specify the id of the tab that runs the service, which is usually not the current tab but some detached tab.\"\"\"\n        if tab_id not in self.tabs:\n            return f\"Tab {tab_id} not found, created tabs are {list(self.tabs.keys())}, specify the correct tab_id that runs the service.\"\n        return await self.tabs[tab_id].preview(port, proj_name)\n\n    async def execute_in_conda_env(self, cmd: str, env) -> str:\n        \"\"\"\n        Executes a given command within a specified Conda environment automatically without\n        the need for manual activation. Users just need to provide the name of the Conda\n        environment and the command to execute.\n\n        Args:\n            cmd (str): The command to execute within the Conda environment.\n            env (str, optional): The name of the Conda environment to activate before executing the command.\n                                 If not specified, the command will run in the current active environment.\n\n        Returns:\n            str: The command's output, or an empty string if `daemon` is True, with output processed\n                 asynchronously in that case.\n\n        Note:\n            This function wraps `run`, prepending the necessary Conda activation commands\n            to ensure the specified environment is active for the command's execution.\n        \"\"\"\n        cmd = f\"conda run -n {env} {cmd}\"\n        return await self.run(cmd)\n\n    @handle_exception\n    async def close(self):\n        for tab in self.tabs.values():\n            await tab.close()\n        self.tabs = {}\n\n    @handle_exception\n    def __del__(self):\n        \"\"\"Close tabls when Terminal instance is destroyed\"\"\"\n        if self.tabs:\n            loop = asyncio.get_running_loop()\n            loop.create_task(self.close())\n",
                        "tags": []
                    }
                },
                "force": true
            },
            "special_tool_commands": [
                "Plan.finish_current_task",
                "end",
                "Terminal.run",
                "RoleZero.ask_human"
            ],
            "editor": {
                "resource": {
                    "block": "Editor",
                    "uuid": "97aad5e8-9fa3-46bd-b6f2-ccd0778a4e14",
                    "enable_llm_stream": false,
                    "callback_url": ""
                },
                "current_file": null,
                "current_line": 1,
                "window": 50,
                "enable_auto_lint": true,
                "working_dir": "/workspace"
            },
            "browser": {
                "rp_meta": {
                    "uuid": "2340ec784c444d8cb37aeadaaaf63784",
                    "storage_dir": "/data/chats/28286616f10743a6baaf2bd913b64f39/workspace/.MGXTools",
                    "user_id": 644050,
                    "chat_id": "28286616f10743a6baaf2bd913b64f39",
                    "role_name": "Emma"
                }
            },
            "terminal": {
                "rp_meta": {
                    "uuid": "81fb83ae4f34427f90768fc9149e418f",
                    "storage_dir": "/data/chats/28286616f10743a6baaf2bd913b64f39/workspace/.MGXTools",
                    "user_id": 644050,
                    "chat_id": "28286616f10743a6baaf2bd913b64f39",
                    "role_name": "Emma"
                }
            },
            "working_dir": "/workspace/",
            "command_rsp": "",
            "commands": [],
            "memory_k": 30,
            "use_fixed_sop": false,
            "use_summary": true,
            "browser_is_active": false,
            "todo_action": "WritePRD",
            "__module_class_name": "metagpt.roles.product_manager.ProductManager"
        },
        "Bob": {
            "name": "Bob",
            "profile": "Architect",
            "goal": "Design a concise, usable, complete software system. Output the system design.",
            "constraints": "Make sure the architecture is simple enough and use appropriate open source libraries. Use same language as user requirement",
            "desc": "",
            "is_human": false,
            "enable_memory": false,
            "role_id": "",
            "states": [
                "0. WriteDesign"
            ],
            "actions": [
                {
                    "name": "WriteDesign",
                    "i_context": null,
                    "prefix": "You are a Architect, named Bob, your goal is Design a concise, usable, complete software system. Output the system design.. the constraint is Make sure the architecture is simple enough and use appropriate open source libraries. Use same language as user requirement. ",
                    "desc": "Based on the PRD, think about the system design, and design the corresponding APIs, data structures, library tables, processes, and paths. Please provide your design, feedback clearly and in detail.",
                    "__module_class_name": "metagpt.actions.design_api.WriteDesign"
                }
            ],
            "rc": {
                "memory": {
                    "storage": [
                        {
                            "id": "35cca740059444188b1a9c2c91664ce9",
                            "timestamp": "20251126060539224",
                            "content": "# Project Summary\nThe \"Talk to My Lawyer\" project is a SaaS application designed to facilitate legal document generation and review. It supports three user roles: Subscribers who generate letters, Employees who assist with sales and earn commissions through coupon codes, and Admins who review and approve the generated letters. The platform leverages AI for drafting legal documents and integrates with payment processors for subscriptions.\n\n# Project Module Description\nThe application comprises several functional modules:\n- **User Authentication**: Manages user roles and sessions.\n- **Letter Generation**: Allows subscribers to create legal letters using AI.\n- **Subscription Management**: Handles user subscriptions and coupon codes.\n- **Admin Review**: Provides a dashboard for admins to review, edit, and approve letters.\n- **Email Notifications**: Sends emails to users regarding their letters and subscriptions.\n- **PDF Generation**: Converts approved letters into downloadable PDF format.\n\n# Directory Tree\n```\nttml_app/\n\u251c\u2500\u2500 main-main/\n\u2502   \u251c\u2500\u2500 app/\n\u2502   \u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 dashboard/\n\u2502   \u2502   \u251c\u2500\u2500 globals.css\n\u2502   \u2502   \u2514\u2500\u2500 layout.tsx\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u251c\u2500\u2500 supabase/\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 styles/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 package.json\n```\n\n# File Description Inventory\n- `app/api/`: Contains API routes for handling requests related to user authentication, letter generation, and subscriptions.\n- `app/dashboard/`: Contains components for the admin and user dashboards, including letter management and analytics.\n- `components/`: Reusable UI components used throughout the application.\n- `supabase/`: Contains database migrations and functions related to the Supabase backend.\n- `scripts/`: Includes scripts for database setup and administration tasks.\n- `styles/`: Contains global styles for the application.\n- `README.md`: Documentation for the project setup and usage.\n\n# Technology Stack\n- **Frontend**: Next.js 14, TypeScript, Tailwind CSS\n- **Backend**: Supabase, OpenAI GPT-4\n- **Payment Processing**: Stripe\n- **Email Service**: Resend\n\n# Usage\n1. **Install Dependencies**: Run the command to install required packages.\n2. **Build the Application**: Use the build command to compile the application.\n3. **Run the Application**: Start the application server for local development.\n\n\n# INSTRUCTION\n- You can search for the file path in the `/workspace/`\n- After modifying the project files, if this project can be previewed, then you need to reinstall dependencies, restart service and preview;\n\n\nChange the working directory to `/workspace/ttml_app/main-main`.",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "df14fbea5cb742d489435448bacf7ffe",
                            "timestamp": "20251126061240991",
                            "content": "[Message] from User to Mike: there are 0 tables ",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>",
                                "Mike"
                            ],
                            "metadata": {}
                        }
                    ],
                    "index": {
                        "metagpt.actions.add_requirement.UserRequirement": [
                            {
                                "id": "df14fbea5cb742d489435448bacf7ffe",
                                "timestamp": "20251126061240991",
                                "content": "[Message] from User to Mike: there are 0 tables ",
                                "instruct_content": null,
                                "role": "user",
                                "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                                "sent_from": "",
                                "send_to": [
                                    "<all>",
                                    "Mike"
                                ],
                                "metadata": {}
                            }
                        ]
                    },
                    "ignore_id": false,
                    "appendix": null,
                    "buffer": [
                        {
                            "id": "35cca740059444188b1a9c2c91664ce9",
                            "timestamp": "20251126060539224",
                            "content": "# Project Summary\nThe \"Talk to My Lawyer\" project is a SaaS application designed to facilitate legal document generation and review. It supports three user roles: Subscribers who generate letters, Employees who assist with sales and earn commissions through coupon codes, and Admins who review and approve the generated letters. The platform leverages AI for drafting legal documents and integrates with payment processors for subscriptions.\n\n# Project Module Description\nThe application comprises several functional modules:\n- **User Authentication**: Manages user roles and sessions.\n- **Letter Generation**: Allows subscribers to create legal letters using AI.\n- **Subscription Management**: Handles user subscriptions and coupon codes.\n- **Admin Review**: Provides a dashboard for admins to review, edit, and approve letters.\n- **Email Notifications**: Sends emails to users regarding their letters and subscriptions.\n- **PDF Generation**: Converts approved letters into downloadable PDF format.\n\n# Directory Tree\n```\nttml_app/\n\u251c\u2500\u2500 main-main/\n\u2502   \u251c\u2500\u2500 app/\n\u2502   \u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 dashboard/\n\u2502   \u2502   \u251c\u2500\u2500 globals.css\n\u2502   \u2502   \u2514\u2500\u2500 layout.tsx\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u251c\u2500\u2500 supabase/\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 styles/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 package.json\n```\n\n# File Description Inventory\n- `app/api/`: Contains API routes for handling requests related to user authentication, letter generation, and subscriptions.\n- `app/dashboard/`: Contains components for the admin and user dashboards, including letter management and analytics.\n- `components/`: Reusable UI components used throughout the application.\n- `supabase/`: Contains database migrations and functions related to the Supabase backend.\n- `scripts/`: Includes scripts for database setup and administration tasks.\n- `styles/`: Contains global styles for the application.\n- `README.md`: Documentation for the project setup and usage.\n\n# Technology Stack\n- **Frontend**: Next.js 14, TypeScript, Tailwind CSS\n- **Backend**: Supabase, OpenAI GPT-4\n- **Payment Processing**: Stripe\n- **Email Service**: Resend\n\n# Usage\n1. **Install Dependencies**: Run the command to install required packages.\n2. **Build the Application**: Use the build command to compile the application.\n3. **Run the Application**: Start the application server for local development.\n\n\n# INSTRUCTION\n- You can search for the file path in the `/workspace/`\n- After modifying the project files, if this project can be previewed, then you need to reinstall dependencies, restart service and preview;\n\n\nChange the working directory to `/workspace/ttml_app/main-main`.",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "df14fbea5cb742d489435448bacf7ffe",
                            "timestamp": "20251126061240991",
                            "content": "[Message] from User to Mike: there are 0 tables ",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>",
                                "Mike"
                            ],
                            "metadata": {}
                        }
                    ],
                    "memory_k": 30,
                    "compress_k": 15,
                    "prev_summary": null,
                    "compress_model_name": "gpt-4o-mini",
                    "is_xml_format": null,
                    "compress_trigger_token_threshold": 0
                },
                "working_memory": {
                    "storage": [],
                    "index": {},
                    "ignore_id": false,
                    "appendix": null
                },
                "state": -1,
                "watch": [
                    "metagpt.actions.write_prd.WritePRD"
                ],
                "react_mode": "react",
                "max_react_loop": 50
            },
            "addresses": [
                "Bob",
                "metagpt.roles.architect.Architect"
            ],
            "planner": {
                "plan": {
                    "goal": "",
                    "context": "",
                    "tasks": [],
                    "task_map": {},
                    "current_task_id": ""
                },
                "working_memory": {
                    "storage": [],
                    "index": {},
                    "ignore_id": false,
                    "appendix": null
                },
                "auto_run": true
            },
            "recovered": false,
            "latest_observed_msg": null,
            "observe_all_msg_from_buffer": true,
            "system_msg": null,
            "system_prompt": "\n<Basic Info>\n{role_info}\n</Basic Info>\n\n<Domain Info>\n{domain_info}\n</Domain Info>\n\n<Data Structure>\nclass Task(BaseModel):\n    task_id: str = \"\"\n    dependent_task_ids: list[str] = []\n    instruction: str = \"\"\n    task_type: str = \"\"\n    assignee: str = \"\"\n</Data Structure>\n\n<Available Task Types>\n{task_type_desc}\n</Available Task Types>\n\n<Available Commands>\n{available_commands}\nSpecial Command: Use <end></end> to indicate completion of all requirements and termination of the entire workflow.\n</Available Commands>\n\n<Example>\n{example}\n</Example>\n\n<Instruction>\n{instruction}\n\nYou may use any of the available commands. You may output multiple commands, they will be executed sequentially.\nIf the task requires writing multiple files, you should output multiple commands to write files instead of writing one by one.\nIf you finish current task, you will automatically take the next task in the existing plan, use Plan.finish_current_task, DON'T append a new task.\nReview the latest plan's outcome. If your completed task matches the current, consider it finished.\nIn your response, include at least one command. Only use <end></end> when all requirements are met, and please use RoleZero.reply_to_human immediately before <end></end> to report completion.\nDo NOT use <end></end> when waiting for user input or clarification.\n</Instruction>\n\n<Output>\nYou should output a list of commands, follow the format below, replace the <ClassName.method_name> and <args_name> with the actual name.\n\nSome thoughts...\n<ClassName.method_name>\n<args_name1>\nargs_value1\n</args_name1>\n<args_name2>\nargs_value2\n</args_name2>\n</ClassName.method_name>\n\nSome thoughts...\n<ClassName2.method_name2>\n<args_name1>\nargs_value1\n</args_name1>\n<args_name2>\nargs_value2\n</args_name2>\n</ClassName2.method_name2>\n</Output>\n",
            "cmd_prompt": "\n<Current State>\n{current_state}\n</Current State>\n\n<Current Plan>\n{plan_status}\n</Current Plan>\n\n<Current Task>\n{current_task}\n</Current Task>\n\nYour commands (output ONE and ONLY ONE command block, the block can contain one or more commands. Use <end></end> when all requirements are met):\n",
            "instruction": "\nYou are an experienced software architect tasked with designing a clear, detailed, and maintainable software system.\n\n### Workflow Steps\n\n1. **Analyze Requirements**\n\n   * If provided, first read the Product Requirement Document (PRD) fully using `Editor.read` without other actions. Treat this as the definitive requirement.\n\n2. **Determine Project Type**\n\n   * Use tool `Terminal.run` to execute `tree` and analyze the directory structure:\n\n     * **New Project**: If no existing directory or codebase is found.\n     * **Incremental Change**: If a codebase already exists.\n\n3. **New Project Design Deliverables**\n\n   **A. Implementation Approach**\n\n   * Clearly outline difficult or critical requirements.\n   * Choose suitable open-source frameworks or technologies.\n\n   **B. User & UI Interaction**\n\n   * Enumerate all primary user interactions with the UI.\n   * Ensure interactions are intuitive and user-friendly.\n\n   **C. System Architecture (PlantUML)**\n\n   * Clearly delineate Frontend, Backend, Databases, and Third-party system boundaries.\n   * File: `architect.plantuml`\n\n   **D. Data Structures & Interfaces (Class Diagram)**\n\n   * Use detailed PlantUML class diagram syntax.\n   * Include precise type annotations, methods (with explicit inputs and outputs), and relationships.\n   * Ensure compliance with PEP8.\n   * File: `class_diagram.plantuml`\n\n   **E. Program Call Flow (Sequence Diagram)**\n\n   * Use precise PlantUML sequence diagram syntax.\n   * Explicitly cover CRUD operations and initialization for each critical object.\n   * Clearly define exact input/output parameters and interactions between components.\n   * File: `sequence_diagram.plantuml`\n\n   **F. Database ER Diagram** *(if applicable)*\n\n   * Clearly define entities, relationships, keys, and constraints using correct PlantUML ER syntax.\n   * File: `er_diagram.plantuml`\n   \n   **G. UI Navigation Paths**  \n     - Modeled using PlantUML's **finite-state machine** notation.  \n     - Keep navigation depth to **no more than 3\u20134 levels**.  \n     - Surface **high-frequency functions and critical information** first.  \n     - Provide a **clear, obvious way back** at every step.\n\n   **H. Clarifications**\n\n   * Explicitly state any ambiguous requirements or technical uncertainties.\n   * Propose assumptions or request clarification.\n\n4. **Incremental Change Deliverable**\n\n   * Analyze current directory structure and existing implementation.\n   * Propose precise and feasible incremental changes or improvements.\n\n5. **Final Documentation** *(New Projects Only)*\n\n   * Write a comprehensive and clearly formatted system design markdown document:\n\n     * File: `system_design.md`\n     * Structure:\n\n       1. Implementation approach\n       2. User & UI interaction behaviors\n       3. Data structures and interfaces overview\n       4. Program call flow overview\n       5. Database ER diagram overview *(if applicable)*\n       6. Unclear aspects or assumptions\n\n6. **File Structure Overview** *(New Projects Only)*\n\n   * Generate a logical project folder and file structure based on the architectural and class diagrams.\n   * File: `file_tree.md`\n\n### Technology Defaults\n\n* **Frontend:** Shadcn-ui, TypeScript, Tailwind CSS, Playwright MCP(UI\u6d4b\u8bd5\u7528) (unless otherwise specified)\n* **Backend:** Specify clearly based on requirements or use sensible defaults as contextually appropriate.\n  * FastAPI with SQLModel is always the best choice to new projects.\n\n### Best Practices\n\n* Always adhere strictly to MVP principles.\n* Provide detailed yet concise PlantUML diagrams.\n* Maintain a clean separation of concerns within the system architecture.\n* Ensure all syntax and conventions (PEP8, PlantUML) are strictly followed.\n* Ensure clarity, correctness, and comprehensiveness in all deliverables.\n\n### Reference System Design Example\n\n{system_design_example}\n\n### Project Template Information\n\n{template_info}\n\n### Other Notes\n1. **The `find` and `tree` commands must exclude dependency folders**, otherwise an LLM max-token error will occur:\n   - `find` example:  \n   `find <dirname> -type d \\( -name \"site-packages\" -o -name \"node_modules\" \\) -prune -o -print`\n   - `tree` example:  \n   `tree <dirname> -I 'site-packages|node_modules'`\n2. Bob, your job is **only** design\u2014writing code is Alex\u2019s responsibility. Stick to your own duties and don\u2019t poke your nose into other people\u2019s business!\n",
            "task_type_desc": null,
            "example": "\n<Example 1>\nnavigate to a path and read a file, then reply to human\n\n<Terminal.run>\n<cmd>\ncd PATH\n</cmd>\n</Terminal.run>\n\n<Editor.read>\n<path>\nPATH\n</path>\n</Editor.read>\n\n<RoleZero.reply_to_human>\n<content>\nsome reply text\n</content>\n</RoleZero.reply_to_human>\n\n<end></end>\n</Example 1>\n\n<Example 2>\nTo write multiple files/code\nNow I'll create the all files needed for the \"development requirements\", including:\n- component1.tsx for xxx\n- component2.tsx for xxx\n- ...\n\nSome thoughts...\n<Editor.write>\n<path>\ncomponent1.tsx\n</path>\n<content>\nYour code\n</content>\n</Editor.write>\n\nSome thoughts...\n<Editor.write>\n<path>\ncomponent2.tsx\n</path>\n<content>\nYour code\n</content>\n</Editor.write>\n...\n</Example 2>\n\n<Example 3: Working directly with user - unclear requirement>\nUser Requirement: Create an e-commerce platform.\nExplanation: The requirement is too vague and could mean many different things. Clarify the requirement first, but only ask about simple features within our capabilities (simple product catalog, basic shopping cart, user accounts). After getting clarification, MUST call FrontendEngineer.draft_plan.\n<RoleZero.ask_human>\n<question>\nI can help you build an e-commerce website. To create the right plan, I need to clarify:\n\n1. What type of products will you display and sell? (physical products, digital downloads, or both)\n2. Do you need a shopping cart and checkout page, or just a product showcase with inquiry forms?\n3. How many products approximately? (This helps me design the product browsing experience)\n\nI'll focus on creating a clean product display, user-friendly interface, and basic user authentication if needed.\n</question>\n</RoleZero.ask_human>\n</Example 3>\n\n<Example 4: Working directly with user - creating draft plan>\nUser Requirement: Build a website for selling motorcycles.\nExplanation: The requirement is complex and involves multiple features. First, create a draft plan and confirm with the human before distributing tasks to team members.\n\nI will create a structured draft plan for the motorcycle sales website and ask the human to confirm.\n\n<FrontendEngineer.draft_plan>\n<content>\n1. Product showcase page - Display all available motorcycles with product images, detailed specifications (engine, weight, price, etc.), and filtering options by category/price\n2. Administrator portal - Secure login page and dashboard for administrators to add/edit/delete motorcycle listings, update prices, and manage inventory\n3. Customer inquiry system - Contact form where potential customers can leave messages, select motorcycles they're interested in, and provide their contact information\n4. Shopping cart and checkout - Allow customers to add motorcycles to cart, review their selections, and complete purchase with payment integration\n5. Company information page - About us section, physical location with map, contact details, and business hours\n</content>\n</FrontendEngineer.draft_plan>\n</Example 4>\n\n<Example 4.1: User requests features beyond capabilities during plan confirmation>\nContext: After presenting the initial draft plan for the motorcycle website, the human responds with feedback requesting features beyond our capabilities.\nUser's feedback on draft plan: \"I approve task 1, 3, 4, 5. For task 3, I also need live video chat with sales representatives. For task 4, I need a shopping cart and checkout system that supports hundreds of users checking out at the same time.\"\nExplanation: The user requests live video chat and high-concurrency checkout system, which are beyond our capabilities. Suggest feasible alternatives and call draft_plan again.\n\nI understand you want enhanced communication and a robust checkout system. Let me suggest alternatives that I can build well:\n\nFor real-time communication with sales reps, instead of video chat, I can create a live inquiry form where customers can instantly submit questions, and you'll receive email notifications. I can also add a click-to-call button that opens their phone dialer with your sales number.\n\nFor the checkout system, I can build a shopping cart and checkout page that works smoothly for small to medium traffic (5-10 concurrent users). This is perfect for a boutique motorcycle shop. If you expect hundreds of simultaneous checkouts, you'll need enterprise-level infrastructure which is outside my scope.\n\nLet me show you the revised plan:\n\n<FrontendEngineer.draft_plan>\n<content>\n1. Product showcase page with motorcycle images, specifications, and filtering\n2. Administrator portal for managing motorcycle listings and inventory\n3. Enhanced customer inquiry system with instant notification forms and click-to-call buttons\n4. Shopping cart with checkout page optimized for small to medium traffic\n5. Company information page with contact details and location map\n</content>\n</FrontendEngineer.draft_plan>\n</Example 4.1>\n",
            "react_mode": "react",
            "max_react_loop": 50,
            "consecutive_react_cnt": 0,
            "max_consecutive_react_limit": 10,
            "tools": [
                "Editor:write,read,similarity_search",
                "RoleZero",
                "Terminal:run"
            ],
            "mcp_tools": [],
            "tool_recommender": {
                "tools": {
                    "Editor": {
                        "name": "Editor",
                        "path": "metagpt/tools/libs/editor.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool for reading, understanding, writing, and editing files. Support local file including text-based files (txt, md, json, py, html, js, css, etc.), pdf, docx, excluding images, csv, excel, or online links For agent: Do NOT initiate multiple Editor.insert_content_at_line calls at the same time, since the line number will change starting with the first execution, making line number of the subsequent calls incorrect. Split the calls into separate responses. For the same reason, Editor.insert_content_at_line should NOT go behind Editor.edit_file_by_replace in the same response. Perform insert operation in a separate response.",
                            "methods": {
                                "write": {
                                    "type": "async_function",
                                    "description": "Write the whole content to a file. When used, make sure content arg contains the full content of the file.",
                                    "signature": "(self, path: str, content: str)",
                                    "parameters": ""
                                },
                                "read": {
                                    "type": "async_function",
                                    "description": "Read the whole content of a file. Using an absolute path as the argument for specifying the file location.",
                                    "signature": "(self, path: str) -> metagpt.tools.libs.editor.FileBlock",
                                    "parameters": ""
                                },
                                "similarity_search": {
                                    "type": "async_function",
                                    "description": "Given a filename or a pathname, performs a similarity search for a given query across the specified file or path. This method searches the index repository for the provided query, classifying the specified files or paths. It performs a search on each cluster of files and handles non-indexed files separately, merging results from structured indices with any direct results from non-indexed files. This function call does not depend on other functions. ",
                                    "signature": "(query: str, path: Union[str, pathlib.Path]) -> List[str]",
                                    "parameters": "Args: query (str): The search query string to look for in the indexed files. path (Union[str, Path]): A pathname or filename to search within. Returns: List[str]: A list of results as strings, containing the text from the merged results and any direct results from non-indexed files. Example: >>> query = \"The problem to be analyzed from the document\" >>> file_or_path = \"The pathname or filename you want to search within\" >>> texts: List[str] = await Editor.similarity_search(query=query, path=file_or_path) >>> print(texts)"
                                }
                            },
                            "tool_path": "metagpt/tools/libs/editor.py"
                        },
                        "code": "@register_tool(\n    include_functions=[\n        \"write\",\n        \"read\",\n        \"open_file\",\n        \"goto_line\",\n        \"scroll_down\",\n        \"scroll_up\",\n        \"create_file\",\n        \"edit_file_by_replace\",\n        \"insert_content_at_line\",\n        \"append_file\",\n        \"search_dir\",\n        \"search_file\",\n        \"find_file\",\n        \"similarity_search\",\n    ]\n)\nclass Editor(BaseModel):\n    \"\"\"\n    A tool for reading, understanding, writing, and editing files.\n    Support local file including text-based files (txt, md, json, py, html, js, css, etc.), pdf, docx, excluding images, csv, excel, or online links\n    For agent: Do NOT initiate multiple Editor.insert_content_at_line calls at the same time, since the line number will change starting with the first execution, making line number of the subsequent calls incorrect. Split the calls into separate responses.\n               For the same reason, Editor.insert_content_at_line should NOT go behind Editor.edit_file_by_replace in the same response. Perform insert operation in a separate response.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    resource: EditorReporter = EditorReporter()\n    current_file: Optional[Path] = None\n    current_line: int = 1\n    window: int = 50\n    enable_auto_lint: bool = True\n    working_dir: Path = DEFAULT_WORKSPACE_ROOT\n\n    async def write(self, path: str, content: str):\n        \"\"\"Write the whole content to a file. When used, make sure content arg contains the full content of the file.\"\"\"\n\n        path = self._try_fix_path(path)\n        content = ensure_string(content)  # Undo possible MCP conversion\n\n        if \"\\n\" not in content and \"\\\\n\" in content:\n            # A very raw rule to correct the content: If 'content' lacks actual newlines ('\\n') but includes '\\\\n', consider\n            # replacing them with '\\n' to potentially correct mistaken representations of newline characters.\n            content = content.replace(\"\\\\n\", \"\\n\")\n        directory = os.path.dirname(path)\n        if directory and not os.path.exists(directory):\n            os.makedirs(directory)\n        async with aiofiles.open(path, \"w\", encoding=\"utf-8\") as f:\n            await f.write(content)\n        # self.resource.report(path, \"path\")\n        return f\"File successfully written and saved to {path}.\"\n\n    def _read_structural_data(self, path: str) -> FileBlock:\n        \"\"\"Read the whole content of a csv file. Using an absolute path as the argument for specifying the file location.\"\"\"\n        if path.endswith(\".csv\"):\n            df = pd.read_csv(path)\n        elif path.endswith(\".xlsx\"):\n            df = pd.read_excel(path)\n        else:\n            raise ValueError(\"The file must be a csv or excel file.\")\n        return FileBlock(path=path, content=f\"Showing head of the data:\\n{df.head().to_string()}\")\n\n    async def read(self, path: str) -> FileBlock:\n        \"\"\"Read the whole content of a file. Using an absolute path as the argument for specifying the file location.\"\"\"\n\n        path = self._try_fix_path(path)\n\n        if path.suffix in STRUCTURAL_DATA_SUFFIX:\n            result = self._read_structural_data(str(path))\n            self.resource.report(str(path), \"path\")\n            return result\n\n        error = FileBlock(\n            path=str(path),\n            content=\"The file is too large to read. Use `Editor.similarity_search` to read the file instead.\",\n        )\n        path = Path(path)\n\n        # Limit the size only for text documents\n        # TODO: Formulate a complete mechanism to handle files of all types and of any size\n        if path.stat().st_size > 5 * DEFAULT_MIN_TOKEN_COUNT and path.suffix in TEXT_DOC_SUFFIX:\n            return error\n\n        content = await File.read_text_file(path)\n        if not content:\n            return FileBlock(path=str(path), content=\"\")\n        if self.is_large_file(content=content):\n            return error\n        self.resource.report(str(path), \"path\")\n\n        result = FileBlock(\n            path=str(path),\n            content=content,\n        )\n        return result\n\n    @staticmethod\n    def _is_valid_filename(file_name: str) -> bool:\n        if not file_name or not file_name.strip():\n            return False\n        invalid_chars = '<>:\"/\\\\|?*'\n        if os.name == \"nt\":  # Windows\n            invalid_chars = '<>:\"/\\\\|?*'\n        elif os.name == \"posix\":  # Unix-like systems\n            invalid_chars = \"\\0\"\n\n        for char in invalid_chars:\n            if char in file_name:\n                return False\n        return True\n\n    @staticmethod\n    def _is_valid_path(path: Path) -> bool:\n        try:\n            return path.exists()\n        except PermissionError:\n            return False\n\n    @staticmethod\n    def _create_paths(file_path: Path) -> bool:\n        try:\n            if file_path.parent:\n                file_path.parent.mkdir(parents=True, exist_ok=True)\n            return True\n        except PermissionError:\n            return False\n\n    def _check_current_file(self, file_path: Optional[Path] = None) -> bool:\n        if file_path is None:\n            file_path = self.current_file\n        if not file_path or not file_path.is_file():\n            raise ValueError(\"No file open. Use the open_file function first.\")\n        return True\n\n    @staticmethod\n    def _clamp(value, min_value, max_value):\n        return max(min_value, min(value, max_value))\n\n    def _lint_file(self, file_path: Path) -> tuple[Optional[str], Optional[int]]:\n        \"\"\"Lint the file at the given path and return a tuple with a boolean indicating if there are errors,\n        and the line number of the first error, if any.\n\n        Returns:\n            tuple[str | None, int | None]: (lint_error, first_error_line_number)\n        \"\"\"\n\n        linter = Linter(root=self.working_dir)\n        lint_error = linter.lint(str(file_path))\n        if not lint_error:\n            # Linting successful. No issues found.\n            return None, None\n        return \"ERRORS:\\n\" + lint_error.text, lint_error.lines[0]\n\n    async def _print_window(self, file_path: Path, targeted_line: int, window: int):\n        self._check_current_file(file_path)\n        async with aiofiles.open(file_path, \"r\") as file:\n            content = await file.read()\n\n            # Ensure the content ends with a newline character\n            if not content.endswith(\"\\n\"):\n                content += \"\\n\"\n\n            lines = content.splitlines(True)  # Keep all line ending characters\n            total_lines = len(lines)\n\n            # cover edge cases\n            self.current_line = self._clamp(targeted_line, 1, total_lines)\n            half_window = max(1, window // 2)\n\n            # Ensure at least one line above and below the targeted line\n            start = max(1, self.current_line - half_window)\n            end = min(total_lines, self.current_line + half_window)\n\n            # Adjust start and end to ensure at least one line above and below\n            if start == 1:\n                end = min(total_lines, start + window - 1)\n            if end == total_lines:\n                start = max(1, end - window + 1)\n\n            output = \"\"\n\n            # only display this when there's at least one line above\n            if start > 1:\n                output += f\"({start - 1} more lines above)\\n\"\n            else:\n                output += \"(this is the beginning of the file)\\n\"\n            for i in range(start, end + 1):\n                _new_line = f\"{i:03d}|{lines[i - 1]}\"\n                if not _new_line.endswith(\"\\n\"):\n                    _new_line += \"\\n\"\n                output += _new_line\n            if end < total_lines:\n                output += f\"({total_lines - end} more lines below)\\n\"\n            else:\n                output += \"(this is the end of the file)\\n\"\n            output = output.rstrip()\n\n            return output\n\n    @staticmethod\n    def _cur_file_header(current_file: Path, total_lines: int) -> str:\n        if not current_file:\n            return \"\"\n        return f\"[File: {current_file.resolve()} ({total_lines} lines total)]\\n\"\n\n    async def set_workdir(self, path: str) -> None:\n        \"\"\"\n        Sets the working directory to the given path. eg: repo directory.\n        You MUST to set it up before open the file.\n\n        Args:\n            path: str: The path to set as the working directory.\n        \"\"\"\n        self.working_dir = Path(path)\n\n    async def open_file(\n        self, path: Union[Path, str], line_number: Optional[int] = 1, context_lines: Optional[int] = None\n    ) -> str:\n        \"\"\"Opens the file at the given path in the editor. If line_number is provided, the window will be moved to include that line.\n        It only shows the first 100 lines by default! Max `context_lines` supported is 2000, use `scroll up/down`\n        to view the file if you want to see more.\n\n        Args:\n            path: str: The path to the file to open, preferred absolute path.\n            line_number: int | None = 1: The line number to move to. Defaults to 1.\n            context_lines: int | None = 100: Only shows this number of lines in the context window (usually from line 1), with line_number as the center (if possible). Defaults to 100.\n        \"\"\"\n        if context_lines is None:\n            context_lines = self.window\n\n        path = self._try_fix_path(path)\n\n        if not path.is_file():\n            raise FileNotFoundError(f\"File {path} not found\")\n\n        self.current_file = path\n\n        total_lines = await self._count_file_lines(file_path=path)\n\n        if not isinstance(line_number, int) or line_number < 1 or line_number > total_lines:\n            raise ValueError(f\"Line number must be between 1 and {total_lines}\")\n        self.current_line = line_number\n\n        # Override WINDOW with context_lines\n        if context_lines is None or context_lines < 1:\n            context_lines = self.window\n\n        output = self._cur_file_header(path, total_lines)\n        output += await self._print_window(path, self.current_line, self._clamp(context_lines, 1, 2000))\n        self.resource.report(path, \"path\")\n        return output\n\n    async def goto_line(self, line_number: int) -> str:\n        \"\"\"Moves the window to show the specified line number.\n\n        Args:\n            line_number: int: The line number to move to.\n        \"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        if not isinstance(line_number, int) or line_number < 1 or line_number > total_lines:\n            raise ValueError(f\"Line number must be between 1 and {total_lines}\")\n\n        self.current_line = self._clamp(line_number, 1, total_lines)\n\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def scroll_down(self) -> str:\n        \"\"\"Moves the window down by 100 lines.\"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        self.current_line = self._clamp(self.current_line + self.window, 1, total_lines)\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def scroll_up(self) -> str:\n        \"\"\"Moves the window up by 100 lines.\"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        self.current_line = self._clamp(self.current_line - self.window, 1, total_lines)\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def create_file(self, filename: str) -> str:\n        \"\"\"Creates and opens a new file with the given name.\n\n        Args:\n            filename: str: The name of the file to create. If the parent directory does not exist, it will be created.\n        \"\"\"\n        filename = self._try_fix_path(filename)\n\n        if filename.exists():\n            raise FileExistsError(f\"File '{filename}' already exists.\")\n        await awrite(filename, \"\\n\")\n\n        self.open_file(filename)\n        return f\"[File {filename} created.]\"\n\n    @staticmethod\n    def _append_impl(lines, content):\n        \"\"\"Internal method to handle appending to a file.\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            content: str: The content to append to the file.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n        \"\"\"\n        content_lines = content.splitlines(keepends=True)\n        n_added_lines = len(content_lines)\n        if lines and not (len(lines) == 1 and lines[0].strip() == \"\"):\n            # file is not empty\n            if not lines[-1].endswith(\"\\n\"):\n                lines[-1] += \"\\n\"\n            new_lines = lines + content_lines\n            content = \"\".join(new_lines)\n        else:\n            # file is empty\n            content = \"\".join(content_lines)\n\n        return content, n_added_lines\n\n    @staticmethod\n    def _insert_impl(lines, start, content):\n        \"\"\"Internal method to handle inserting to a file.\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            start: int: The start line number for inserting.\n            content: str: The content to insert to the file.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n\n        Raises:\n            LineNumberError: If the start line number is invalid.\n        \"\"\"\n        inserted_lines = [content + \"\\n\" if not content.endswith(\"\\n\") else content]\n        if len(lines) == 0:\n            new_lines = inserted_lines\n        elif start is not None:\n            if len(lines) == 1 and lines[0].strip() == \"\":\n                # if the file with only 1 line and that line is empty\n                lines = []\n\n            if len(lines) == 0:\n                new_lines = inserted_lines\n            else:\n                new_lines = lines[: start - 1] + inserted_lines + lines[start - 1 :]\n        else:\n            raise LineNumberError(\n                f\"Invalid line number: {start}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n\n        content = \"\".join(new_lines)\n        n_added_lines = len(inserted_lines)\n        return content, n_added_lines\n\n    @staticmethod\n    def _edit_impl(lines, start, end, content):\n        \"\"\"Internal method to handle editing a file.\n\n        REQUIRES (should be checked by caller):\n            start <= end\n            start and end are between 1 and len(lines) (inclusive)\n            content ends with a newline\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            start: int: The start line number for editing.\n            end: int: The end line number for editing.\n            content: str: The content to replace the lines with.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n        \"\"\"\n        # Handle cases where start or end are None\n        if start is None:\n            start = 1  # Default to the beginning\n        if end is None:\n            end = len(lines)  # Default to the end\n        # Check arguments\n        if not (1 <= start <= len(lines)):\n            raise LineNumberError(\n                f\"Invalid start line number: {start}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n        if not (1 <= end <= len(lines)):\n            raise LineNumberError(\n                f\"Invalid end line number: {end}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n        if start > end:\n            raise LineNumberError(f\"Invalid line range: {start}-{end}. Start must be less than or equal to end.\")\n\n        # Split content into lines and ensure it ends with a newline\n        if not content.endswith(\"\\n\"):\n            content += \"\\n\"\n        content_lines = content.splitlines(True)\n\n        # Calculate the number of lines to be added\n        n_added_lines = len(content_lines)\n\n        # Remove the specified range of lines and insert the new content\n        new_lines = lines[: start - 1] + content_lines + lines[end:]\n\n        # Handle the case where the original lines are empty\n        if len(lines) == 0:\n            new_lines = content_lines\n\n        # Join the lines to create the new content\n        content = \"\".join(new_lines)\n        return content, n_added_lines\n\n    def _get_indentation_info(self, content, first_line):\n        \"\"\"\n        The indentation of the first insert line and the previous line, along with guidance for the next attempt.\n        \"\"\"\n        content_lines = content.split(\"\\n\")\n        pre_line = content_lines[first_line - 2] if first_line - 2 >= 0 else \"\"\n        pre_line_indent = len(pre_line) - len(pre_line.lstrip())\n        insert_line = content_lines[first_line - 1]\n        insert_line_indent = len(insert_line) - len(insert_line.lstrip())\n        ret_str = INDENTATION_INFO.format(\n            pre_line=pre_line,\n            pre_line_indent=pre_line_indent,\n            insert_line=insert_line,\n            insert_line_indent=insert_line_indent,\n            sub_4_space=max(insert_line_indent - 4, 0),\n            add_4_space=insert_line_indent + 4,\n        )\n        return ret_str\n\n    async def _edit_file_impl(\n        self,\n        file_name: Path,\n        start: Optional[int] = None,\n        end: Optional[int] = None,\n        content: str = \"\",\n        is_insert: bool = False,\n        is_append: bool = False,\n    ) -> str:\n        \"\"\"Internal method to handle common logic for edit_/append_file methods.\n\n        Args:\n            file_name: Path: The name of the file to edit or append to.\n            start: int | None = None: The start line number for editing. Ignored if is_append is True.\n            end: int | None = None: The end line number for editing. Ignored if is_append is True.\n            content: str: The content to replace the lines with or to append.\n            is_insert: bool = False: Whether to insert content at the given line number instead of editing.\n            is_append: bool = False: Whether to append content to the file instead of editing.\n        \"\"\"\n\n        ERROR_MSG = f\"[Error editing file {file_name}. Please confirm the file is correct.]\"\n        ERROR_MSG_SUFFIX = (\n            \"Your changes have NOT been applied. Please fix your edit command and try again.\\n\"\n            \"You either need to 1) Open the correct file and try again or 2) Specify the correct line number arguments.\\n\"\n            \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n        )\n\n        if not self._is_valid_filename(file_name.name):\n            raise FileNotFoundError(\"Invalid file name.\")\n\n        if not self._is_valid_path(file_name):\n            raise FileNotFoundError(\"Invalid path or file name.\")\n\n        if not self._create_paths(file_name):\n            raise PermissionError(\"Could not access or create directories.\")\n\n        if not file_name.is_file():\n            raise FileNotFoundError(f\"File {file_name} not found.\")\n\n        if is_insert and is_append:\n            raise ValueError(\"Cannot insert and append at the same time.\")\n\n        # Use a temporary file to write changes\n        content = str(content or \"\")\n        temp_file_path = \"\"\n        src_abs_path = file_name.resolve()\n        first_error_line = None\n        # The file to store previous content and will be removed automatically.\n        temp_backup_file = tempfile.NamedTemporaryFile(\"w\", delete=True)\n\n        try:\n            # lint the original file\n            if self.enable_auto_lint:\n                original_lint_error, _ = self._lint_file(file_name)\n\n            # Create a temporary file\n            with tempfile.NamedTemporaryFile(\"w\", delete=False) as temp_file:\n                temp_file_path = temp_file.name\n\n                # Read the original file and check if empty and for a trailing newline\n                async with aiofiles.open(file_name, \"r\") as original_file:\n                    lines = await original_file.readlines()\n\n                if is_append:\n                    content, n_added_lines = self._append_impl(lines, content)\n                elif is_insert:\n                    try:\n                        content, n_added_lines = self._insert_impl(lines, start, content)\n                    except LineNumberError as e:\n                        return (f\"{ERROR_MSG}\\n\" f\"{e}\\n\" f\"{ERROR_MSG_SUFFIX}\") + \"\\n\"\n                else:\n                    try:\n                        content, n_added_lines = self._edit_impl(lines, start, end, content)\n                    except LineNumberError as e:\n                        return (f\"{ERROR_MSG}\\n\" f\"{e}\\n\" f\"{ERROR_MSG_SUFFIX}\") + \"\\n\"\n\n                if not content.endswith(\"\\n\"):\n                    content += \"\\n\"\n\n                # Write the new content to the temporary file\n                temp_file.write(content)\n\n            # Replace the original file with the temporary file atomically\n            shutil.move(temp_file_path, src_abs_path)\n\n            # Handle linting\n            # NOTE: we need to get env var inside this function\n            # because the env var will be set AFTER the agentskills is imported\n            if self.enable_auto_lint:\n                # BACKUP the original file\n                temp_backup_file.writelines(lines)\n                temp_backup_file.flush()\n                lint_error, first_error_line = self._lint_file(file_name)\n\n                # Select the errors caused by the modification\n                def extract_last_part(line):\n                    parts = line.split(\":\")\n                    if len(parts) > 1:\n                        return parts[-1].strip()\n                    return line.strip()\n\n                def subtract_strings(str1, str2) -> str:\n                    lines1 = str1.splitlines()\n                    lines2 = str2.splitlines()\n\n                    last_parts1 = [extract_last_part(line) for line in lines1]\n\n                    remaining_lines = [line for line in lines2 if extract_last_part(line) not in last_parts1]\n\n                    result = \"\\n\".join(remaining_lines)\n                    return result\n\n                if original_lint_error and lint_error:\n                    lint_error = subtract_strings(original_lint_error, lint_error)\n                    if lint_error == \"\":\n                        lint_error = None\n                        first_error_line = None\n\n                if lint_error is not None:\n                    # show the first insert line.\n                    if is_append:\n                        # original end-of-file\n                        show_line = len(lines)\n                    # insert OR edit WILL provide meaningful line numbers\n                    elif start is not None and end is not None:\n                        show_line = int((start + end) / 2)\n                    else:\n                        raise ValueError(\"Invalid state. This should never happen.\")\n\n                    guidance_message = self._get_indentation_info(content, start or len(lines))\n                    guidance_message += (\n                        \"You either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\\n\"\n                        \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n                    )\n                    lint_error_info = ERROR_GUIDANCE.format(\n                        linter_error_msg=LINTER_ERROR_MSG + lint_error,\n                        window_after_applied=await self._print_window(file_name, show_line, n_added_lines + 20),\n                        window_before_applied=await self._print_window(\n                            Path(temp_backup_file.name), show_line, n_added_lines + 20\n                        ),\n                        guidance_message=guidance_message,\n                    ).strip()\n\n                    # recover the original file\n                    shutil.move(temp_backup_file.name, src_abs_path)\n                    return lint_error_info\n\n        except FileNotFoundError as e:\n            return f\"File not found: {e}\\n\"\n        except IOError as e:\n            return f\"An error occurred while handling the file: {e}\\n\"\n        except ValueError as e:\n            return f\"Invalid input: {e}\\n\"\n        except Exception as e:\n            guidance_message = self._get_indentation_info(content, start or len(lines))\n            guidance_message += (\n                \"You either need to 1) Specify the correct start/end line arguments or 2) Enlarge the range of original code.\\n\"\n                \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n            )\n            error_info = ERROR_GUIDANCE.format(\n                linter_error_msg=LINTER_ERROR_MSG + str(e),\n                window_after_applied=await self._print_window(file_name, start or len(lines), 100),\n                window_before_applied=await self._print_window(Path(temp_backup_file.name), start or len(lines), 100),\n                guidance_message=guidance_message,\n            ).strip()\n            # Clean up the temporary file if an error occurs\n            shutil.move(temp_backup_file.name, src_abs_path)\n            if temp_file_path and Path(temp_file_path).exists():\n                Path(temp_file_path).unlink()\n\n            raise Exception(f\"{error_info}\") from e\n\n        # Update the file information and print the updated content\n        n_total_lines = await self._count_file_lines(file_path=file_name, encoding=\"utf-8\")\n\n        if first_error_line is not None and int(first_error_line) > 0:\n            self.current_line = first_error_line\n        else:\n            if is_append:\n                self.current_line = max(1, len(lines))  # end of original file\n            else:\n                self.current_line = start or n_total_lines or 1\n\n        success_edit_info = SUCCESS_EDIT_INFO.format(\n            file_name=file_name.resolve(),\n            n_total_lines=n_total_lines,\n            window_after_applied=await self._print_window(file_name, self.current_line, self.window),\n            line_number=self.current_line,\n        ).strip()\n\n        return success_edit_info\n\n    async def edit_file_by_replace(self, file_name: str, to_replace: str, new_content: str) -> str:\n        \"\"\"Edit a file. This will search for `to_replace` in the given file and replace it with `new_content`.\n\n        Every *to_replace* must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n        * The `to_replace` parameter should match EXACTLY one or more consecutive lines from the original file. Be mindful of whitespaces!\n        * If the `to_replace` parameter is not unique in the file, the replacement will not be performed. Make sure to include enough context in `to_replace` to make it unique\n        * The `new_content` parameter should contain the edited lines that should replace the `to_replace`\n\n        For example, given a file \"path/to/example.txt\" with the following content:\n        ```\n        line 1\n        line 2\n        line 2\n        line 3\n        ```\n\n        EDITING: If you want to replace the second occurrence of \"line 2\", you can make `to_replace` unique:\n\n        edit_file_by_replace(\n            'path/to/example.txt',\n            to_replace='line 2\\nline 3',\n            new_content='new line\\nline 3',\n        )\n\n        This will replace only the second \"line 2\" with \"new line\". The first \"line 2\" will remain unchanged.\n\n        The resulting file will be:\n        ```\n        line 1\n        line 2\n        new line\n        line 3\n        ```\n\n        REMOVAL: If you want to remove \"line 2\" and \"line 3\", you can set `new_content` to an empty string:\n\n        edit_file_by_replace(\n            'path/to/example.txt',\n            to_replace='line 2\\nline 3',\n            new_content='',\n        )\n        \"\"\"\n        # search for `to_replace` in the file\n        # if found, replace it with `new_content`\n        # if not found, perform a fuzzy search to find the closest match and replace it with `new_content`\n        file_name = self._try_fix_path(file_name)\n\n        async with aiofiles.open(file_name, \"r\") as file:\n            file_content = await file.read()\n\n        if to_replace.strip() == \"\":\n            if file_content.strip() == \"\":\n                raise ValueError(f\"The file '{file_name}' is empty. Please use the append method to add content.\")\n            raise ValueError(\"`to_replace` must not be empty.\")\n\n        to_replace = self._rm_line_no(to_replace)\n        new_content = self._rm_line_no(new_content)\n\n        if file_content.count(to_replace) > 1:\n            raise ValueError(\n                f\"`to_replace` ```{to_replace}``` appears more than once, please include enough lines to make code in `to_replace` unique.\"\n            )\n        elif file_content.count(to_replace) == 0:\n            # If no exact match, try to find close matches using LCS\n            if len(to_replace) > 20:\n                # white spaces are usually missing in multi-line content, thus hard coding a threshold of 20\n                _, similarity, file_content_match = self.lcs(to_replace, file_content)\n                if similarity > 0.9:  # High similarity suggests missing whitespace\n                    raise ValueError(\n                        f\"`to_replace` ```{to_replace}``` not found in {file_name}. Pay attention to spaces and line breaks! Did you actually mean ```{file_content_match}```\"\n                    )\n            raise ValueError(\n                f\"`to_replace` ```{to_replace}``` not found in {file_name}. Read the file carefully with Editor.read and make sure you give the right content to replace. If you want to insert new content, use Editor.insert_content_at_line instead.\"\n            )\n\n        ### Take a easy way to replace the content with direct string operation. Also disable linting for now ###\n        ### TODO: unittest to cover this part ###\n        new_content = file_content.replace(to_replace, new_content)\n        async with aiofiles.open(file_name, \"w\") as file:\n            await file.write(new_content)\n\n        start = file_content.find(to_replace) + 1  # index of starting char\n        start_line_number = file_content[:start].count(\"\\n\") + 1\n        window_after_applied = await self._print_window(file_name, start_line_number, 30)\n        success_edit_info = SUCCESS_EDIT_INFO.format(\n            file_name=file_name.resolve(),\n            n_total_lines=\"\",\n            window_after_applied=window_after_applied,\n            line_number=start_line_number,\n        ).strip()\n        self.resource.report(file_name, \"path\")\n        return success_edit_info\n\n    async def insert_content_at_line(self, file_name: str, line_number: int, insert_content: str) -> str:\n        \"\"\"Insert a complete block of code before the given line number in a file. That is, the new content will start at the beginning of the specified line, and the existing content of that line will be moved down.\n        This operation will NOT modify the content of the lines before or after the given line number.\n        This function can not insert content the end of the file. Please use append_file instead,\n        For example, if the file has the following content:\n        ```\n        001|contain g\n        002|contain h\n        003|contain i\n        004|contain j\n        ```\n        and you call\n        insert_content_at_line(\n            file_name='file.txt',\n            line_number=2,\n            insert_content='new line'\n        )\n        the file will be updated to:\n        ```\n        001|contain g\n        002|new line\n        003|contain h\n        004|contain i\n        005|contain j\n        ```\n\n        Args:\n            file_name: (str): The name of the file to edit.\n            line_number (int): The line number (starting from 1) to insert the content after. The insert content will be add between the line of line_number-1 and line_number\n            insert_content (str): The content to insert betweed the previous_line_content and current_line_content.The insert_content must be a complete block of code at.\n        \"\"\"\n        file_name = self._try_fix_path(file_name)\n        ret_str = await self._edit_file_impl(\n            file_name,\n            start=line_number,\n            end=line_number,\n            content=insert_content,\n            is_insert=True,\n            is_append=False,\n        )\n        self.resource.report(file_name, \"path\")\n        return ret_str\n\n    async def append_file(self, file_name: str, content: str) -> str:\n        \"\"\"Append content to the given file.\n        It appends text `content` to the end of the specified file.\n\n        Args:\n            file_name: str: The name of the file to edit.\n            content: str: The content to insert.\n        \"\"\"\n        file_name = self._try_fix_path(file_name)\n        ret_str = await self._edit_file_impl(\n            file_name,\n            start=None,\n            end=None,\n            content=content,\n            is_insert=False,\n            is_append=True,\n        )\n        self.resource.report(file_name, \"path\")\n        return ret_str\n\n    async def search_dir(self, search_term: str, dir_path: str = \"./\") -> str:\n        \"\"\"Searches for search_term in all files in dir. If dir is not provided, searches in the current directory.\n\n        Args:\n            search_term: str: The term to search for.\n            dir_path: str: The path to the directory to search.\n        \"\"\"\n        dir_path = self._try_fix_path(dir_path)\n        if not dir_path.is_dir():\n            raise FileNotFoundError(f\"Directory {dir_path} not found\")\n\n        matches = []\n        for root, _, files in os.walk(dir_path):\n            for file in files:\n                if file.startswith(\".\"):\n                    continue\n                file_path = Path(root) / file\n                try:\n                    async with aiofiles.open(file_path, \"r\", errors=\"ignore\") as f:\n                        line_num = 0\n                        async for line in f:\n                            line_num += 1\n                            if search_term in line:\n                                matches.append((file_path, line_num, line.strip()))\n                except Exception:\n                    # Skip files that can't be read\n                    continue\n\n        if not matches:\n            return f'No matches found for \"{search_term}\" in {dir_path}'\n\n        num_matches = len(matches)\n        num_files = len(set(match[0] for match in matches))\n\n        if num_files > 100:\n            return f'More than {num_files} files matched for \"{search_term}\" in {dir_path}. Please narrow your search.'\n\n        res_list = [f'[Found {num_matches} matches for \"{search_term}\" in {dir_path}]']\n        for file_path, line_num, line in matches:\n            res_list.append(f\"{file_path} (Line {line_num}): {line}\")\n        res_list.append(f'[End of matches for \"{search_term}\" in {dir_path}]')\n        return \"\\n\".join(res_list)\n\n    async def search_file(self, search_term: str, file_path: Optional[str] = None) -> str:\n        \"\"\"Searches for search_term in file. If file is not provided, searches in the current open file.\n\n        Args:\n            search_term: str: The term to search for.\n            file_path: str | None: The path to the file to search.\n        \"\"\"\n        if file_path is None:\n            file_path = self.current_file\n        else:\n            file_path = self._try_fix_path(file_path)\n        if file_path is None:\n            raise FileNotFoundError(\"No file specified or open. Use the open_file function first.\")\n        if not file_path.is_file():\n            raise FileNotFoundError(f\"File {file_path} not found\")\n\n        matches = []\n        async with aiofiles.open(file_path, \"r\") as file:\n            i = 0\n            async for line in file:\n                i += 1\n                if search_term in line:\n                    matches.append((i, line.strip()))\n\n        res_list = []\n        if matches:\n            res_list.append(f'[Found {len(matches)} matches for \"{search_term}\" in {file_path}]')\n            for match in matches:\n                res_list.append(f\"Line {match[0]}: {match[1]}\")\n            res_list.append(f'[End of matches for \"{search_term}\" in {file_path}]')\n        else:\n            res_list.append(f'[No matches found for \"{search_term}\" in {file_path}]')\n\n        extra = {\"type\": \"search\", \"symbol\": search_term, \"lines\": [i[0] - 1 for i in matches]} if matches else None\n        self.resource.report(file_path, \"path\", extra=extra)\n        return \"\\n\".join(res_list)\n\n    async def find_file(self, file_name: str, dir_path: str = \"./\") -> str:\n        \"\"\"Finds all files with the given name or path in the specified directory.\n\n        Args:\n            file_name: str: The name or path of the file to find.\n                           If it's an absolute path, will match the exact path.\n                           If it's a relative path, will match the path suffix.\n                           If it's just a name, will match file names.\n            dir_path: str: The path to the directory to search.\n        \"\"\"\n        dir_path = self._try_fix_path(dir_path)\n        if not dir_path.is_dir():\n            raise FileNotFoundError(f\"Directory {dir_path} not found\")\n\n        file_path = Path(file_name)\n\n        # TODO: use asyncio.gather to concurrently check multiple paths\n        matches = []\n        for root, _, files in os.walk(dir_path):\n            for file in files:\n                current_path = Path(root) / file\n\n                if file_path.is_absolute():\n                    if current_path == file_path:\n                        matches.append(current_path)\n                elif \"/\" in str(file_path) or \"\\\\\" in str(file_path):\n                    if str(current_path).endswith(str(file_path)):\n                        matches.append(current_path)\n                else:\n                    if current_path.name == file_name:\n                        matches.append(current_path)\n\n        res_list = []\n        if matches:\n            res_list.append(f'[Found {len(matches)} matches for \"{file_name}\" in {dir_path}]')\n            for match in matches:\n                res_list.append(f\"{match}\")\n            res_list.append(f'[End of matches for \"{file_name}\" in {dir_path}]')\n        else:\n            res_list.append(f'[No matches found for \"{file_name}\" in {dir_path}]')\n        return \"\\n\".join(res_list)\n\n    def _try_fix_path(self, path: Union[Path, str]) -> Path:\n        \"\"\"Tries to fix the path if it is not absolute.\"\"\"\n        if not isinstance(path, Path):\n            path = Path(path)\n        if not path.is_absolute():\n            path = self.working_dir / path\n        return path\n\n    @staticmethod\n    def _rm_line_no(content: str) -> str:\n        line_no_pattern = \"\\n[0-9]{3}\\|\"\n        if re.search(line_no_pattern, content):\n            logger.warning(\"Line numbers detected in content and will be removed\")\n            content = re.sub(line_no_pattern, \"\\n\", content)\n        return content\n\n    @staticmethod\n    async def similarity_search(query: str, path: Union[str, Path]) -> List[str]:\n        \"\"\"Given a filename or a pathname, performs a similarity search for a given query across the specified file or path.\n\n        This method searches the index repository for the provided query, classifying the specified\n        files or paths. It performs a search on each cluster of files and handles non-indexed files\n        separately, merging results from structured indices with any direct results from non-indexed files.\n        This function call does not depend on other functions.\n\n        Args:\n            query (str): The search query string to look for in the indexed files.\n            path (Union[str, Path]): A pathname or filename to search within.\n\n        Returns:\n            List[str]: A list of results as strings, containing the text from the merged results\n                        and any direct results from non-indexed files.\n\n        Example:\n            >>> query = \"The problem to be analyzed from the document\"\n            >>> file_or_path = \"The pathname or filename you want to search within\"\n            >>> texts: List[str] = await Editor.similarity_search(query=query, path=file_or_path)\n            >>> print(texts)\n        \"\"\"\n        return await IndexRepo.cross_repo_search(query=query, file_or_path=path)\n\n    @staticmethod\n    def is_large_file(content: str, mix_token_count: int = 0) -> bool:\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n        token_count = len(encoding.encode(content))\n        mix_token_count = mix_token_count or DEFAULT_MIN_TOKEN_COUNT\n        return token_count >= mix_token_count\n\n    @staticmethod\n    def lcs(s1: str, s2: str) -> tuple[str, float, str]:\n        \"\"\"Find the longest common subsequence between two strings and the matching section from s2. Assuming s1 is always a subsequence of s2.\n\n        Args:\n            s1: First string (typically the to_replace content)\n            s2: Second string (typically the file content)\n\n        Returns:\n            tuple[str, float, str]: The LCS string, its similarity score (0-1), and the matching section from s2\n        \"\"\"\n        # Create DP table\n        m, n = len(s1), len(s2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n\n        # Fill DP table\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if s1[i - 1] == s2[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n\n        # Reconstruct LCS and track positions in s2\n        lcs = []\n        i, j = m, n\n        positions = []  # Store positions of matches in s2\n        while i > 0 and j > 0:\n            if s1[i - 1] == s2[j - 1]:\n                lcs.append(s1[i - 1])\n                positions.append(j - 1)\n                i -= 1\n                j -= 1\n            elif dp[i - 1][j] > dp[i][j - 1]:\n                i -= 1\n            else:\n                j -= 1\n\n        lcs = \"\".join(reversed(lcs))\n        positions = list(reversed(positions))\n\n        # Calculate similarity score\n        score = len(lcs) / len(s1)\n\n        # Extract the matching section from s2\n        if positions:\n            start = min(positions)\n            end = max(positions) + 1\n            matching_section = s2[start:end]\n        else:\n            matching_section = \"\"\n        matching_section = matching_section[\n            : int(1.2 * len(s1))\n        ]  # hard truncate to avoid greedily match too long string\n\n        return lcs, score, matching_section\n\n    async def _count_file_lines(self, file_path: Union[Path, str], encoding: Optional[str] = None) -> int:\n        \"\"\"Asynchronously count the total number of lines in a file.\n\n        Args:\n            file_path: Path to the file.\n            encoding: The encoding used to open the file. Defaults to None (system default encoding).\n\n        Returns:\n            int: Total number of lines in the file, minimum 1.\n        \"\"\"\n        total_lines = 0\n        async with aiofiles.open(file_path, \"r\", encoding=encoding) as file:\n            async for _ in file:\n                total_lines += 1\n        return max(1, total_lines)\n\n    async def remove_file(self, file_path: Union[Path, str], missing_ok: bool = True) -> str:\n        \"\"\"Remove a file from the file system.\n\n        Args:\n            file_path: Path to the file to remove.\n            missing_ok: If True, don't raise error if file doesn't exist.\n        \"\"\"\n        path = self._try_fix_path(file_path)\n        path.unlink(missing_ok=missing_ok)\n        await self.resource.async_report(file_path, \"delete\")\n        return f\"File {path} removed successfully\"\n",
                        "tags": []
                    },
                    "RoleZero": {
                        "name": "RoleZero",
                        "path": "metagpt/roles/di/role_zero.py",
                        "schemas": {
                            "type": "class",
                            "description": "A role who can think and act dynamically",
                            "methods": {
                                "ask_human": {
                                    "type": "async_function",
                                    "description": "Use this when you fail the current task or if you are unsure of the situation encountered. Your response should contain a brief summary of your situation, ended with a clear and concise question.",
                                    "signature": "(self, question: 'str') -> 'str'",
                                    "parameters": ""
                                },
                                "reply_to_human": {
                                    "type": "async_function",
                                    "description": "Reply to human user with the content provided. Use this when you have a clear answer or solution to the user's question.",
                                    "signature": "(self, content: 'str') -> 'str'",
                                    "parameters": ""
                                }
                            },
                            "tool_path": "metagpt/roles/di/role_zero.py"
                        },
                        "code": "@register_tool(include_functions=[\"ask_human\", \"reply_to_human\"])\nclass RoleZero(Role):\n    \"\"\"A role who can think and act dynamically\"\"\"\n\n    # Basic Info\n    name: str = \"Zero\"\n    profile: str = \"RoleZero\"\n    goal: str = \"\"\n    system_msg: Optional[list[str]] = None  # Use None to conform to the default value at llm.aask\n    system_prompt: str = SYSTEM_PROMPT  # Use None to conform to the default value at llm.aask\n    cmd_prompt: str = CMD_PROMPT\n    instruction: str = ROLE_INSTRUCTION\n    task_type_desc: Optional[str] = None\n    example: str = \"\"  # global think & act demonstration for the role\n\n    # React Mode\n    react_mode: Literal[\"react\"] = \"react\"\n    max_react_loop: int = 50\n    # consecutive react count after receiving a message, used for triggering human intervention\n    consecutive_react_cnt: int = 0\n    max_consecutive_react_limit: int = 10\n\n    # Tools\n    tools: list[str] = []  # Use special symbol [\"<all>\"] to indicate use of all registered tools\n    mcp_tools: list[str] = []\n    tool_recommender: Optional[ToolRecommender] = None\n    tool_execution_map: Annotated[dict[str, Callable], Field(exclude=True)] = {}\n    special_tool_commands: list[str] = [\"Plan.finish_current_task\", \"end\", \"Terminal.run\", \"RoleZero.ask_human\"]\n\n    # Equipped with three basic tools by default for optional use\n    editor: Annotated[Union[Editor, RemoteProxy], BeforeValidator(ensure_editor_instance)] = Field(\n        default_factory=resolve_editor\n    )\n    browser: Annotated[Union[Browser, RemoteProxy], BeforeValidator(ensure_browser_instance)] = Field(\n        default_factory=resolve_browser\n    )\n    terminal: Annotated[Union[Terminal, RemoteProxy], BeforeValidator(ensure_terminal_instance)] = Field(\n        default_factory=resolve_terminal\n    )\n    working_dir: str = Field(default_factory=lambda: str(DEFAULT_WORKSPACE_ROOT.resolve()))\n\n    # Others\n    observe_all_msg_from_buffer: bool = True\n    command_rsp: str = \"\"  # the raw string containing the commands\n    commands: list[dict] = []  # commands to be executed\n    memory_k: int = 30  # number of memories (messages) to use as historical context\n    use_fixed_sop: bool = False\n    use_summary: bool = True  # whether to summarize at the end\n    browser_is_active: bool = False  # Track browser state to check if it is open\n    think_task: Optional[asyncio.Task] = Field(default=None, exclude=True)  # Track the background thinking task\n\n    # @model_validator(mode=\"after\")\n    # def validate_and_init_mcp_tools(self) -> \"RoleZero\":\n    #     \"\"\"Initialize mcp_tools if empty and MCP servers are enabled\"\"\"\n    #     if not self.mcp_tools:\n    #         if self.config.mcp.has_enabled_servers():\n    #             self.mcp_tools = mcp_manager.list_tool_names()\n    #     return self\n\n    @model_validator(mode=\"after\")\n    def set_plan_and_tool(self) -> \"RoleZero\":\n        # We force using this parameter for DataAnalyst\n        assert self.react_mode == \"react\"\n\n        # Roughly the same part as DataInterpreter.set_plan_and_tool\n        self._set_react_mode(react_mode=self.react_mode, max_react_loop=self.max_react_loop)\n\n        tools = self.tools  # + self.mcp_tools\n        if tools and not self.tool_recommender:\n            self.tool_recommender = BM25ToolRecommender(tools=tools, force=True)\n        self.set_actions([RunCommand])\n\n        # HACK: Init Planner, control it through dynamic thinking; Consider formalizing as a react mode\n        self.planner = Planner(goal=\"\", working_memory=self.rc.working_memory, auto_run=True)\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_tool_execution(self) -> \"RoleZero\":\n        # default map, includes all special tool commands\n        self.tool_execution_map = {\n            \"Plan.append_task\": self.planner.plan.append_task,\n            \"Plan.reset_task\": self.planner.plan.reset_task,\n            \"Plan.replace_task\": self.planner.plan.replace_task,\n            \"Terminal.run\": self.terminal.run,\n            \"Terminal\": self.terminal.run,  # alias\n            \"Terminal.run_command\": self.terminal.run,  # alias\n            \"Terminal.preview\": self.terminal.preview,\n            \"RoleZero.ask_human\": self.ask_human,\n            \"RoleZero.reply_to_human\": self.reply_to_human,\n            \"SearchEnhancedQA.run\": SearchEnhancedQA().run,\n            \"Plan.finish_current_task\": self.planner.plan.finish_current_task,\n            \"end\": self._end,\n        }\n        self.tool_execution_map.update(\n            {\n                f\"Browser.{i}\": getattr(self.browser, i)\n                for i in [\n                    \"click\",\n                    \"close_tab\",\n                    \"go_back\",\n                    \"go_forward\",\n                    \"goto\",\n                    \"hover\",\n                    \"press\",\n                    \"scroll\",\n                    \"tab_focus\",\n                    \"type\",\n                    \"take_screenshot\",\n                ]\n            }\n        )\n        self.tool_execution_map.update(\n            {\n                f\"Editor.{i}\": getattr(self.editor, i)\n                for i in [\n                    \"append_file\",\n                    \"create_file\",\n                    \"edit_file_by_replace\",\n                    \"find_file\",\n                    \"goto_line\",\n                    \"insert_content_at_line\",\n                    \"open_file\",\n                    \"read\",\n                    \"scroll_down\",\n                    \"scroll_up\",\n                    \"search_dir\",\n                    \"search_file\",\n                    \"similarity_search\",\n                    # \"set_workdir\",\n                    \"write\",\n                ]\n            }\n        )\n\n        for tool_name in self.mcp_tools:\n            self.tool_execution_map[tool_name] = mcp_manager.get_tool_executor(tool_name)\n\n        # can be updated by subclass\n        self._update_tool_execution()\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_tool_working_dir(self) -> \"RoleZero\":\n        asyncio.create_task(self._update_workdir_for_tools(self.working_dir))\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_longterm_memory(self) -> \"RoleZero\":\n        \"\"\"Set up special memory for the role if enabled in the configuration.\"\"\"\n\n        # We set Role.rc.memory as SerializeAsAny to retain all fields in Memory subclasses, ensuring model_dump() includes subclass fields.\n        # However, for some of the fields, we want to use the latest values from the config.\n        # To avoid repeated args, we init the subclass first, then update the fields with the config\n        if self.config.role_zero.enable_compressable_memory:\n            kvs = self.rc.memory.model_dump()\n            kvs[\"compress_model_name\"] = self.config.role_zero.compress_model_name\n            self.rc.memory = CompressableMemory(**kvs)\n            self.rc.memory.memory_k = self.config.role_zero.memory_k\n            self.rc.memory.compress_k = self.config.role_zero.compress_k\n            self.rc.memory.compress_trigger_token_threshold = self.config.role_zero.compress_trigger_token_threshold\n            logger.info(f\"Compressable memory set for role '{self.name}'\")\n\n        elif self.config.role_zero.enable_longterm_memory:\n            # Use config.role_zero to initialize long-term memory\n            # The role name will be used as the collection name.\n            self.rc.memory = RoleZeroLongTermMemory(**self.rc.memory.model_dump())\n            memory = self.rc.memory\n            memory.persist_path = self.config.role_zero.longterm_memory_persist_path\n            memory.collection_name = self.name.replace(\" \", \"\")\n            memory.memory_k = self.config.role_zero.memory_k\n            memory.similarity_top_k = self.config.role_zero.similarity_top_k\n            memory.use_llm_ranker = self.config.role_zero.use_llm_ranker\n            logger.info(f\"Long-term memory set for role '{self.name}'\")\n\n        return self\n\n    def _update_tool_execution(self):\n        pass\n\n    def _retrieve_example(self) -> str:\n        \"\"\"Default implementation of example retrieval. Can be overwritten in subclasses.\"\"\"\n        return self.example\n\n    def _get_goal_from_memories(self):\n        \"\"\"Get goal from memories by finding the latest message from TL or UserRequirement\"\"\"\n        memories = self.get_memories()\n        for msg in reversed(memories):\n            if msg.sent_from == \"Mike\" or \"UserRequirement\" in msg.cause_by:\n                return msg.content\n        # Fallback to last message if no matching message found\n        return memories[-1].content if memories else \"\"\n\n    async def _think(self) -> bool:\n        \"\"\"Useful in 'react' mode. Use LLM to decide whether and what to do next.\"\"\"\n        # Compatibility\n        if self.use_fixed_sop:\n            return await super()._think()\n\n        ### 0. Preparation ###\n        if not self.rc.todo:\n            return False\n\n        if not self.planner.plan.goal:\n            self.planner.plan.goal = self._get_goal_from_memories()\n\n        ### 1. Example ###\n        example = self._retrieve_example()\n\n        ### 2. Plan Status ###\n        plan_status, current_task = get_plan_status(planner=self.planner)\n\n        ### 3. Tool/Command Info ###\n        tools = await self.tool_recommender.recommend_tools()\n        tool_info = json.dumps({tool.name: tool.schemas for tool in tools})\n\n        ### Role Instruction ###\n        instruction = self.instruction.strip()\n        system_prompt = self.system_prompt.format(\n            role_info=self._get_prefix(),\n            domain_info=self._get_domain_info(),\n            task_type_desc=self.task_type_desc,\n            available_commands=tool_info,\n            example=example,\n            instruction=instruction,\n        )\n\n        ### Make Decision Dynamically ###\n        prompt = self.cmd_prompt.format(\n            current_state=f\"current directory: {self.working_dir}\",\n            plan_status=plan_status,\n            current_task=current_task,\n        )\n\n        ### Recent Observation ###\n        if self.config.role_zero.enable_compressable_memory:\n            await self.rc.memory.compress(extra_context=super()._get_prefix())\n        memory = self.rc.memory.get(self.memory_k)\n        if self.browser_is_active:\n            memory = await parse_browser_actions(memory, browser=self.browser)\n        # memory = await parse_images(memory, llm=self.llm)\n\n        req = self.llm.format_msg(memory + [UserMessage(content=prompt)])\n\n        state_data = dict(\n            plan_status=plan_status,\n            current_task=current_task,\n            instruction=instruction,\n        )\n        logger.info(f\"{self.name}, ready to think\")\n        # background task to handle LLM thinking process\n        self.create_think_background_task(req, system_prompt, state_data)\n        return True\n\n    def create_think_background_task(self, req, system_prompt, state_data):\n        self.think_task = asyncio.create_task(self._think_background(req, system_prompt, state_data))\n\n    async def _think_background(self, req, system_prompt, state_data):\n        \"\"\"Background task to handle LLM thinking process.\"\"\"\n        async with ThoughtReporter(enable_llm_stream=True) as reporter:\n            await reporter.async_report({\"type\": \"react\"})\n            self.command_rsp = await self.llm_cached_aask(req=req, system_msgs=[system_prompt], state_data=state_data)\n        logger.info(f\"Command response:\\n{self.command_rsp}\")\n        rsp_hist = [mem.content for mem in self.rc.memory.get()]\n        self.command_rsp = await check_duplicates(\n            req=req, command_rsp=self.command_rsp, rsp_hist=rsp_hist, llm=self.llm\n        )\n\n    @exp_cache(context_builder=RoleZeroContextBuilder(), serializer=RoleZeroSerializer())\n    async def llm_cached_aask(self, *, req: list[dict], system_msgs: list[str], **kwargs) -> str:\n        \"\"\"Use `exp_cache` to automatically manage experiences.\n\n        The `RoleZeroContextBuilder` attempts to add experiences to `req`.\n        The `RoleZeroSerializer` extracts essential parts of `req` for the experience pool, trimming lengthy entries to retain only necessary parts.\n        \"\"\"\n        return await self.llm.aask(req, system_msgs=system_msgs)\n\n    def _get_domain_info(self) -> str:\n        return get_time_info() + MGX_INFO + get_enable_supabase_guidance()\n\n    async def _act(self) -> Message:\n        if self.use_fixed_sop:\n            return await super()._act()\n\n        command_outputs = []\n        async for cmd in parse_stream_commands2(role_instance=self, tool_execution_map=self.tool_execution_map):\n            is_success, output = await self._run_command(cmd)\n            command_outputs.append(output)\n            if not is_success:\n                break\n        if not command_outputs:\n            command_outputs = [\"No valid commands found for execution, pay attention to the output format.\"]\n        self.rc.memory.add(AIMessage(content=self.command_rsp))\n        outputs = \"\\n\\n\".join(command_outputs)\n        logger.info(f\"Commands outputs: \\n{outputs}\")\n        self.rc.memory.add(UserMessage(content=outputs, cause_by=RunCommand))\n\n        think_task = self.think_task\n        self.think_task = None\n        await think_task\n\n        return AIMessage(\n            content=f\"I have finished the task, please mark my task as finished. Outputs: {outputs}\",\n            sent_from=self.name,\n            cause_by=RunCommand,\n        )\n\n    async def _react(self) -> Message:\n        # NOTE: Diff 1: Each time landing here means news is observed, set todo to allow news processing in _think\n        self._set_state(0)\n\n        actions_taken = 0\n        self.consecutive_react_cnt = 0\n        rsp = AIMessage(content=\"No actions taken yet\", cause_by=Action)  # will be overwritten after Role _act\n        while actions_taken < self.rc.max_react_loop:\n            # NOTE: Diff 2: Keep observing within _react, news will go into memory, allowing adapting to new info\n            if await self._observe():\n                self.consecutive_react_cnt = 0\n                # Reset state to 0 to override potential `end` command (state = -1) and ensure the loop continues.\n                self._set_state(0)\n            # think\n            has_todo = await self._think()\n            if not has_todo:\n                break\n            # act\n            logger.debug(f\"{self._setting}: {self.rc.state=}, will do {self.rc.todo}\")\n            rsp = await self._act()\n            actions_taken += 1\n            self.consecutive_react_cnt += 1\n\n            # post-check\n            if self.rc.max_react_loop >= 10 and actions_taken >= self.rc.max_react_loop:\n                # If max_react_loop is a small value (e.g. < 10), it is intended to be reached and make the agent stop\n                logger.warning(f\"reached max_react_loop: {actions_taken}\")\n                human_rsp = await self.ask_human(\n                    \"I have reached my max action rounds, do you want me to continue? Yes or no\"\n                )\n                if \"yes\" in human_rsp.lower():\n                    actions_taken = 0\n            if self.consecutive_react_cnt >= self.max_consecutive_react_limit:\n                memory = self.get_memories(k=self.memory_k)\n                context = self.llm.format_msg(memory + [UserMessage(content=SUMMARIZE_STATUS_WHEN_CONSECUTIVE)])\n                question = await self.llm.aask(context)\n                human_rsp = await self.ask_human(question)\n                self.rc.memory.add(UserMessage(content=\"User's extra instruction: \" + human_rsp, cause_by=RunCommand))\n                self.consecutive_react_cnt = 0\n\n        return rsp  # return output from the last action\n\n    async def _run_command(self, cmd) -> tuple[bool, str]:\n        \"\"\"stream command execution\"\"\"\n        output = f\"Command {cmd['command_name']} executed\"\n        try:\n            # handle special command first\n            if self._is_special_command(cmd):\n                special_command_output = await self._run_special_command(cmd)\n                output += \":\" + special_command_output\n                cmd[\"status\"] = \"success\"  # mgx_backend need these fields\n                cmd[\"error_msg\"] = \"\"\n                return True, output\n\n            # run command as specified by tool_execute_map\n            if cmd[\"command_name\"] in self.tool_execution_map:\n                tool_obj = self.tool_execution_map[cmd[\"command_name\"]]\n                if inspect.iscoroutinefunction(tool_obj):\n                    tool_output = await tool_obj(**cmd[\"args\"])\n                else:\n                    tool_output = tool_obj(**cmd[\"args\"])\n                self._try_update_browser_state(cmd[\"command_name\"])\n                if tool_output:\n                    output += f\": {str(tool_output)}\"\n                cmd[\"status\"] = \"success\"\n                cmd[\"error_msg\"] = \"\"\n            else:\n                output = f\"Command {cmd['command_name']} not found.\"\n                cmd[\"status\"] = \"failed\"\n                cmd[\"error_msg\"] = f\"Command {cmd['command_name']} not found.\"\n                return False, output\n\n        except Exception:\n            tb = traceback.format_exc()\n            output += f\": {tb}\"\n            cmd[\"status\"] = \"failed\"\n            cmd[\"error_msg\"] = f\"Command {cmd['command_name']} failed: {tb}\"\n            return False, output\n\n        return True, output\n\n    def _is_special_command(self, cmd) -> bool:\n        return cmd[\"command_name\"] in self.special_tool_commands\n\n    async def _run_special_command(self, cmd) -> str:\n        \"\"\"command requiring special check or parsing\"\"\"\n        command_output = \"\"\n\n        if cmd[\"command_name\"] == \"Plan.finish_current_task\":\n            if not self.planner.plan.is_plan_finished():\n                self.planner.plan.finish_current_task()\n            command_output = (\n                \"Current task is finished. If you no longer need to take action, use the command 'end' to stop.\"\n            )\n\n        elif cmd[\"command_name\"] == \"end\":\n            command_output = await self._end()\n        elif cmd[\"command_name\"] == \"RoleZero.ask_human\":\n            args = cmd[\"args\"]\n            # fault tolerance for llm hallucination\n            question = args.get(\"question\") or args.get(\"content\") or args.get(\"message\")\n            assert question, \"Use kwarg 'question' to provide the question.\"\n            human_response = await self.ask_human(question=question)\n            self.consecutive_react_cnt = 0\n            if human_response.strip().lower().endswith((\"stop\", \"<stop>\")):\n                human_response += \"The user has asked me to stop because I have encountered a problem.\"\n                self.rc.memory.add(UserMessage(content=human_response, cause_by=RunCommand))\n                end_output = \"\\nCommand end executed:\"\n                end_output += await self._end()\n                return end_output\n            return human_response\n        # output from bash.run may be empty, add decorations to the output to ensure visibility.\n        elif \"Terminal\" in cmd[\"command_name\"]:  # Terminal.run and its alias\n            tool_obj = self.tool_execution_map[cmd[\"command_name\"]]\n            tool_output = await tool_obj(**cmd[\"args\"])\n            command_output = format_terminal_output(cmd=cmd, raw_output=tool_output)\n            self.working_dir = await self.terminal.get_cwd()\n            await self._update_workdir_for_tools(self.working_dir)\n            token_count = count_string_tokens(command_output, model_name=\"default\")\n            if token_count >= self.config.llm.max_token * 0.9:\n                command_output = \"OutputTooLongException: the content is too long to use. Please optimize the command parameters to reduce the amount of output.\"\n\n        return command_output\n\n    async def _update_workdir_for_tools(self, new_workdir: str) -> None:\n        \"\"\"Update working directory for all relevant tools.\n        This method can be overridden by subclasses to update additional tools.\n        \"\"\"\n        await self.editor.set_workdir(new_workdir)\n\n    async def ask_human(self, question: str) -> str:\n        \"\"\"Use this when you fail the current task or if you are unsure of the situation encountered. Your response should contain a brief summary of your situation, ended with a clear and concise question.\"\"\"\n        # NOTE: Can be overwritten in remote setting\n        from metagpt.environment.mgx.mgx_env import MGXEnv  # avoid circular import\n\n        if not isinstance(self.rc.env, MGXEnv):\n            return \"Not in MGXEnv, command will not be executed.\"\n        return await self.rc.env.ask_human(question, sent_from=self)\n\n    async def reply_to_human(self, content: str) -> str:\n        \"\"\"Reply to human user with the content provided. Use this when you have a clear answer or solution to the user's question.\"\"\"\n        # NOTE: Can be overwritten in remote setting\n        from metagpt.environment.mgx.mgx_env import MGXEnv  # avoid circular import\n\n        if not isinstance(self.rc.env, MGXEnv):\n            return \"Not in MGXEnv, command will not be executed.\"\n        return await self.rc.env.reply_to_human(content, sent_from=self)\n\n    async def _end(self, **kwarg):\n        self._set_state(-1)\n        memory = self.rc.memory.get(self.memory_k)\n        outputs = \"\"\n        # Summary of the Completed Task and Deliverables\n        if self.use_summary:\n            logger.info(\"end current run and summarize\")\n            async with ThoughtReporter() as reporter:\n                await reporter.async_report({\"type\": \"summary\"})\n                outputs = await self.llm.aask(self.llm.format_msg(memory + [UserMessage(SUMMARY_PROMPT)]))\n        return outputs\n\n    def _try_update_browser_state(self, command_name: str) -> None:\n        \"\"\"Update browser_is_active based on the command_name.\"\"\"\n\n        if command_name == \"Browser.goto\":\n            self.browser_is_active = True\n        elif command_name == \"Browser.close_tab\":\n            self.browser_is_active = False\n",
                        "tags": []
                    },
                    "Terminal": {
                        "name": "Terminal",
                        "path": "metagpt/tools/libs/terminal.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool for running terminal commands. Don't initialize a new instance of this class if one already exists.",
                            "methods": {
                                "run": {
                                    "type": "async_function",
                                    "description": "Executes a specified command in the terminal and streams the output back in real time. ",
                                    "signature": "(self, cmd: str) -> str",
                                    "parameters": "Args: cmd (str): The command to execute in the terminal. Returns: str: The command's output."
                                }
                            },
                            "tool_path": "metagpt/tools/libs/terminal.py"
                        },
                        "code": "@register_tool(include_functions=[\"run\"])\nclass Terminal(BaseModel):\n    \"\"\"A tool for running terminal commands. Don't initialize a new instance of this class if one already exists.\"\"\"\n\n    tabs: dict[str, Tab] = {}\n    current_tab_id: str = \"\"\n    current_tab: Optional[Tab] = Field(None, exclude=True)\n    #  The cmd in forbidden_terminal_commands will be replace by pass ana return the advise. example:{\"cmd\":\"forbidden_reason/advice\"}\n    forbidden_commands: dict[str, str] = {\n        # \"run dev\": \"Use Deployer.deploy_to_public instead.\",\n        \"run preview\": \"Use Deployer.deploy_to_public instead.\",\n        # serve cmd have a space behind it,\n        \"serve \": \"Use Deployer.deploy_to_public instead.\",\n        \"python -m http.server\": \"Use python -u -m http.server port_number instead.\",  # force buffer flush for http.server\n    }\n    timeout: float = 300.0  # timeout for reading output\n    working_dir: str = Field(default_factory=lambda: str(DEFAULT_WORKSPACE_ROOT.absolute()))\n\n    @model_validator(mode=\"after\")\n    def valid_current_tab(self):\n        if self.current_tab_id:\n            self.current_tab = self.tabs[self.current_tab_id]\n        return self\n\n    async def switch_tab(self, tab_id: str = \"\") -> str:\n        \"\"\"Switch tab based on tab_id. Useful for checking out new output from detached tabs or typing on desired tabs.\"\"\"\n        # NOTE: Hide from agent for now (not registered), need to solve the echo END_MARKER_VALUE problem\n        if tab_id in self.tabs:\n            self.current_tab = self.tabs[tab_id]\n            self.current_tab_id = tab_id\n            tab_new_output = self.current_tab.read_background_output()\n            return f\"Switched to tab {tab_id}, pwd is {self.current_tab.cwd}, the tab has new output: {tab_new_output}\"\n        return f\"Tab {tab_id} not found, created tabs are {list(self.tabs.keys())}\"\n\n    async def _create_new_tab(self) -> str:\n        \"\"\"create a new tab and switch to it\"\"\"\n        new_tab_id = f\"{len(self.tabs):02}\"\n        new_tab = Tab(tab_id=new_tab_id, cwd=self.cwd)\n        await new_tab.start()\n        self.tabs.update({new_tab_id: new_tab})\n        switch_tab_info = await self.switch_tab(new_tab_id)\n        return f\"Tab {new_tab_id} created. \" + switch_tab_info\n\n    @property\n    def cwd(self):\n        return self.current_tab.cwd if self.current_tab else self.working_dir\n\n    async def get_cwd(self):\n        return self.cwd\n\n    async def run(self, cmd: str) -> str:\n        \"\"\"\n        Executes a specified command in the terminal and streams the output back in real time.\n\n        Args:\n            cmd (str): The command to execute in the terminal.\n\n        Returns:\n            str: The command's output.\n        \"\"\"\n        if not self.current_tab:\n            await self._create_new_tab()\n\n        output = \"\"\n        # Precheck for forbidden commands\n        commands = re.split(r\"\\s*&&\\s*\", cmd)\n        for cmd_name, reason in self.forbidden_commands.items():\n            for command in commands:\n                if cmd_name in command:\n                    output += f\"{command} is forbidden. {reason}\\n\"\n        if output:\n            output = f\"Command not executed due to forbidden syntax:\\n{output}\\nPlease rewrite the whole command for execution.\"\n            return output\n\n        cmd = \" && \".join(commands)\n\n        # Send the command\n        current_tab = self.current_tab\n        if current_tab.is_running:\n            return (\n                \"Cannot execute the command because a previous one is still running in the current terminal tab\"\n                f\" (ID: {self.current_tab_id}).\"\n            )\n\n        # clear the output queue before execute command\n        output_queue = current_tab.output_queue\n        while not output_queue.empty():\n            output_queue.get_nowait()\n\n        await current_tab.execute(cmd)\n\n        tmp = []\n        is_service_flag = False\n        while True:\n            try:\n                # shorter timeout for service process by detecting output patterns such as localhost:port, ip:port\n                timeout = self.timeout if not is_service_flag else 3\n                line = await asyncio.wait_for(output_queue.get(), timeout=timeout)\n                if line is None:\n                    # ensure the read_and_process_output task is done\n                    await current_tab.task\n                    break\n                is_service_flag = is_service_flag or is_service_process(line)  # if True already, skip checking\n                tmp.append(line)\n            except asyncio.TimeoutError:\n                output_so_far = \"\".join(tmp)\n                if (returncode := current_tab.process.returncode) is not None:\n                    msg = f\"The terminal has exited with code {returncode}\"\n                    logger.warning(msg)\n                    return f\"{msg}, currently with output: {output_so_far}\"\n\n                current_tab.update_cwd()  # update cwd for the command still running\n                logger.info(f\"No more output after {timeout}s, detached from current tab and switched to a new tab\")\n\n                detached_tab_id = self.current_tab_id\n                new_tab_info = await self._create_new_tab()\n                instruction = DETACH_PROMPT.format(\n                    detached_tab_id=detached_tab_id,\n                    output_so_far=output_so_far,\n                    new_tab_info=new_tab_info,\n                )\n                # print(instruction)\n                return instruction\n        return \"\".join(tmp)\n\n    async def preview(self, tab_id: str, port: int, proj_name: str) -> str:\n        \"\"\"Preview a web project by forwarding a local port to public. Specify the id of the tab that runs the service, which is usually not the current tab but some detached tab.\"\"\"\n        if tab_id not in self.tabs:\n            return f\"Tab {tab_id} not found, created tabs are {list(self.tabs.keys())}, specify the correct tab_id that runs the service.\"\n        return await self.tabs[tab_id].preview(port, proj_name)\n\n    async def execute_in_conda_env(self, cmd: str, env) -> str:\n        \"\"\"\n        Executes a given command within a specified Conda environment automatically without\n        the need for manual activation. Users just need to provide the name of the Conda\n        environment and the command to execute.\n\n        Args:\n            cmd (str): The command to execute within the Conda environment.\n            env (str, optional): The name of the Conda environment to activate before executing the command.\n                                 If not specified, the command will run in the current active environment.\n\n        Returns:\n            str: The command's output, or an empty string if `daemon` is True, with output processed\n                 asynchronously in that case.\n\n        Note:\n            This function wraps `run`, prepending the necessary Conda activation commands\n            to ensure the specified environment is active for the command's execution.\n        \"\"\"\n        cmd = f\"conda run -n {env} {cmd}\"\n        return await self.run(cmd)\n\n    @handle_exception\n    async def close(self):\n        for tab in self.tabs.values():\n            await tab.close()\n        self.tabs = {}\n\n    @handle_exception\n    def __del__(self):\n        \"\"\"Close tabls when Terminal instance is destroyed\"\"\"\n        if self.tabs:\n            loop = asyncio.get_running_loop()\n            loop.create_task(self.close())\n",
                        "tags": []
                    }
                },
                "force": true
            },
            "special_tool_commands": [
                "Plan.finish_current_task",
                "end",
                "Terminal.run",
                "RoleZero.ask_human"
            ],
            "editor": {
                "resource": {
                    "block": "Editor",
                    "uuid": "97aad5e8-9fa3-46bd-b6f2-ccd0778a4e14",
                    "enable_llm_stream": false,
                    "callback_url": ""
                },
                "current_file": null,
                "current_line": 1,
                "window": 50,
                "enable_auto_lint": true,
                "working_dir": "/workspace"
            },
            "browser": {
                "rp_meta": {
                    "uuid": "bf083a5c55664dbe99e25aca6cd44b16",
                    "storage_dir": "/data/chats/28286616f10743a6baaf2bd913b64f39/workspace/.MGXTools",
                    "user_id": 644050,
                    "chat_id": "28286616f10743a6baaf2bd913b64f39",
                    "role_name": "Bob"
                }
            },
            "terminal": {
                "rp_meta": {
                    "uuid": "639e6de381ca4e6f83c17f4ada5ae83f",
                    "storage_dir": "/data/chats/28286616f10743a6baaf2bd913b64f39/workspace/.MGXTools",
                    "user_id": 644050,
                    "chat_id": "28286616f10743a6baaf2bd913b64f39",
                    "role_name": "Bob"
                }
            },
            "working_dir": "/workspace/",
            "command_rsp": "",
            "commands": [],
            "memory_k": 30,
            "use_fixed_sop": false,
            "use_summary": true,
            "browser_is_active": false,
            "func_sea_mode": false,
            "func_sea_counter": {
                "design_count": 0,
                "max_design_count": 3,
                "plan_count": 0,
                "max_plan_count": 3
            },
            "design_docs_dir": null,
            "bob_todo_filename": null,
            "alex_todo_filename": null,
            "report_docs_dir": null,
            "src_dir": null,
            "warning_prompt": "",
            "__module_class_name": "metagpt.roles.architect.Architect"
        },
        "Alex": {
            "name": "Alex",
            "profile": "Engineer",
            "goal": "Take on game, app, web development and deployment.",
            "constraints": "",
            "desc": "",
            "is_human": false,
            "enable_memory": true,
            "role_id": "",
            "states": [
                "0. RunCommand"
            ],
            "actions": [
                {
                    "name": "RunCommand",
                    "i_context": "",
                    "prefix": "You are a Engineer, named Alex, your goal is Take on game, app, web development and deployment.. ",
                    "desc": "",
                    "__module_class_name": "metagpt.actions.di.run_command.RunCommand"
                }
            ],
            "rc": {
                "memory": {
                    "storage": [
                        {
                            "id": "35cca740059444188b1a9c2c91664ce9",
                            "timestamp": "20251126060539250",
                            "content": "# Project Summary\nThe \"Talk to My Lawyer\" project is a SaaS application designed to facilitate legal document generation and review. It supports three user roles: Subscribers who generate letters, Employees who assist with sales and earn commissions through coupon codes, and Admins who review and approve the generated letters. The platform leverages AI for drafting legal documents and integrates with payment processors for subscriptions.\n\n# Project Module Description\nThe application comprises several functional modules:\n- **User Authentication**: Manages user roles and sessions.\n- **Letter Generation**: Allows subscribers to create legal letters using AI.\n- **Subscription Management**: Handles user subscriptions and coupon codes.\n- **Admin Review**: Provides a dashboard for admins to review, edit, and approve letters.\n- **Email Notifications**: Sends emails to users regarding their letters and subscriptions.\n- **PDF Generation**: Converts approved letters into downloadable PDF format.\n\n# Directory Tree\n```\nttml_app/\n\u251c\u2500\u2500 main-main/\n\u2502   \u251c\u2500\u2500 app/\n\u2502   \u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 dashboard/\n\u2502   \u2502   \u251c\u2500\u2500 globals.css\n\u2502   \u2502   \u2514\u2500\u2500 layout.tsx\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u251c\u2500\u2500 supabase/\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 styles/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 package.json\n```\n\n# File Description Inventory\n- `app/api/`: Contains API routes for handling requests related to user authentication, letter generation, and subscriptions.\n- `app/dashboard/`: Contains components for the admin and user dashboards, including letter management and analytics.\n- `components/`: Reusable UI components used throughout the application.\n- `supabase/`: Contains database migrations and functions related to the Supabase backend.\n- `scripts/`: Includes scripts for database setup and administration tasks.\n- `styles/`: Contains global styles for the application.\n- `README.md`: Documentation for the project setup and usage.\n\n# Technology Stack\n- **Frontend**: Next.js 14, TypeScript, Tailwind CSS\n- **Backend**: Supabase, OpenAI GPT-4\n- **Payment Processing**: Stripe\n- **Email Service**: Resend\n\n# Usage\n1. **Install Dependencies**: Run the command to install required packages.\n2. **Build the Application**: Use the build command to compile the application.\n3. **Run the Application**: Start the application server for local development.\n\n\n# INSTRUCTION\n- You can search for the file path in the `/workspace/`\n- After modifying the project files, if this project can be previewed, then you need to reinstall dependencies, restart service and preview;\n\n\nChange the working directory to `/workspace/ttml_app/main-main`.",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "df14fbea5cb742d489435448bacf7ffe",
                            "timestamp": "20251126061240991",
                            "content": "[Message] from User to Mike: there are 0 tables ",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>",
                                "Mike"
                            ],
                            "metadata": {}
                        }
                    ],
                    "index": {
                        "metagpt.actions.add_requirement.UserRequirement": [
                            {
                                "id": "df14fbea5cb742d489435448bacf7ffe",
                                "timestamp": "20251126061240991",
                                "content": "[Message] from User to Mike: there are 0 tables ",
                                "instruct_content": null,
                                "role": "user",
                                "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                                "sent_from": "",
                                "send_to": [
                                    "<all>",
                                    "Mike"
                                ],
                                "metadata": {}
                            }
                        ]
                    },
                    "ignore_id": false,
                    "appendix": null,
                    "buffer": [
                        {
                            "id": "35cca740059444188b1a9c2c91664ce9",
                            "timestamp": "20251126060539250",
                            "content": "# Project Summary\nThe \"Talk to My Lawyer\" project is a SaaS application designed to facilitate legal document generation and review. It supports three user roles: Subscribers who generate letters, Employees who assist with sales and earn commissions through coupon codes, and Admins who review and approve the generated letters. The platform leverages AI for drafting legal documents and integrates with payment processors for subscriptions.\n\n# Project Module Description\nThe application comprises several functional modules:\n- **User Authentication**: Manages user roles and sessions.\n- **Letter Generation**: Allows subscribers to create legal letters using AI.\n- **Subscription Management**: Handles user subscriptions and coupon codes.\n- **Admin Review**: Provides a dashboard for admins to review, edit, and approve letters.\n- **Email Notifications**: Sends emails to users regarding their letters and subscriptions.\n- **PDF Generation**: Converts approved letters into downloadable PDF format.\n\n# Directory Tree\n```\nttml_app/\n\u251c\u2500\u2500 main-main/\n\u2502   \u251c\u2500\u2500 app/\n\u2502   \u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 dashboard/\n\u2502   \u2502   \u251c\u2500\u2500 globals.css\n\u2502   \u2502   \u2514\u2500\u2500 layout.tsx\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u251c\u2500\u2500 supabase/\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 styles/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 package.json\n```\n\n# File Description Inventory\n- `app/api/`: Contains API routes for handling requests related to user authentication, letter generation, and subscriptions.\n- `app/dashboard/`: Contains components for the admin and user dashboards, including letter management and analytics.\n- `components/`: Reusable UI components used throughout the application.\n- `supabase/`: Contains database migrations and functions related to the Supabase backend.\n- `scripts/`: Includes scripts for database setup and administration tasks.\n- `styles/`: Contains global styles for the application.\n- `README.md`: Documentation for the project setup and usage.\n\n# Technology Stack\n- **Frontend**: Next.js 14, TypeScript, Tailwind CSS\n- **Backend**: Supabase, OpenAI GPT-4\n- **Payment Processing**: Stripe\n- **Email Service**: Resend\n\n# Usage\n1. **Install Dependencies**: Run the command to install required packages.\n2. **Build the Application**: Use the build command to compile the application.\n3. **Run the Application**: Start the application server for local development.\n\n\n# INSTRUCTION\n- You can search for the file path in the `/workspace/`\n- After modifying the project files, if this project can be previewed, then you need to reinstall dependencies, restart service and preview;\n\n\nChange the working directory to `/workspace/ttml_app/main-main`.",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "df14fbea5cb742d489435448bacf7ffe",
                            "timestamp": "20251126061240991",
                            "content": "[Message] from User to Mike: there are 0 tables ",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>",
                                "Mike"
                            ],
                            "metadata": {}
                        }
                    ],
                    "memory_k": 30,
                    "compress_k": 15,
                    "prev_summary": null,
                    "compress_model_name": "gpt-4o-mini",
                    "is_xml_format": null,
                    "compress_trigger_token_threshold": 0
                },
                "working_memory": {
                    "storage": [],
                    "index": {},
                    "ignore_id": false,
                    "appendix": null
                },
                "state": -1,
                "watch": [],
                "react_mode": "react",
                "max_react_loop": 200
            },
            "addresses": [
                "Alex",
                "metagpt.roles.di.frontend_engineer.FrontendEngineer"
            ],
            "planner": {
                "plan": {
                    "goal": "",
                    "context": "",
                    "tasks": [],
                    "task_map": {},
                    "current_task_id": ""
                },
                "working_memory": {
                    "storage": [],
                    "index": {},
                    "ignore_id": false,
                    "appendix": null
                },
                "auto_run": true
            },
            "recovered": false,
            "latest_observed_msg": null,
            "observe_all_msg_from_buffer": true,
            "system_msg": null,
            "system_prompt": "\n<Basic Info>\n{role_info}\n</Basic Info>\n\n<Domain Info>\n{domain_info}\n</Domain Info>\n\n<Data Structure>\nclass Task(BaseModel):\n    task_id: str = \"\"\n    dependent_task_ids: list[str] = []\n    instruction: str = \"\"\n    task_type: str = \"\"\n    assignee: str = \"\"\n</Data Structure>\n\n<Available Task Types>\n{task_type_desc}\n</Available Task Types>\n\n<Available Commands>\n{available_commands}\nSpecial Command: Use <end></end> to indicate completion of all requirements and termination of the entire workflow.\n</Available Commands>\n\n<Example>\n{example}\n</Example>\n\n<Instruction>\n{instruction}\n\nYou may use any of the available commands. You may output multiple commands, they will be executed sequentially.\nIf the task requires writing multiple files, you should output multiple commands to write files instead of writing one by one.\nIf you finish current task, you will automatically take the next task in the existing plan, use Plan.finish_current_task, DON'T append a new task.\nReview the latest plan's outcome. If your completed task matches the current, consider it finished.\nIn your response, include at least one command. Only use <end></end> when all requirements are met, and please use RoleZero.reply_to_human immediately before <end></end> to report completion.\nDo NOT use <end></end> when waiting for user input or clarification.\n</Instruction>\n\n<Output>\nYou should output a list of commands, follow the format below, replace the <ClassName.method_name> and <args_name> with the actual name.\n\nSome thoughts...\n<ClassName.method_name>\n<args_name1>\nargs_value1\n</args_name1>\n<args_name2>\nargs_value2\n</args_name2>\n</ClassName.method_name>\n\nSome thoughts...\n<ClassName2.method_name2>\n<args_name1>\nargs_value1\n</args_name1>\n<args_name2>\nargs_value2\n</args_name2>\n</ClassName2.method_name2>\n</Output>\n",
            "cmd_prompt": "\n<Current State>\n{current_state}\n</Current State>\n\nYour commands (output ONE and ONLY ONE command block, the block can contain one or more commands.\nPlease refer to and follow the command formats in your system prompt enclosed in the <Example> </Example> as your reference, and make sure your commands are in the correct format.\nCommand <end></end> means terminating the whole process, use it when you finish all tasks, you don't need this command in any intermediate steps, and please use RoleZero.reply_to_human immediately before <end></end> to report completion.\n",
            "instruction": "\nBased on the context, write a plan or modify an existing plan to achieve the goal. A plan consists of one to 3 tasks.\nIf plan is created, you should track the progress and update the plan accordingly, such as Plan.finish_current_task, Plan.append_task, Plan.reset_task, Plan.replace_task, etc.\nWhen presented a current task, tackle the task using the available commands.\nPay close attention to new user message, review the conversation history, use RoleZero.reply_to_human to respond to new user requirement.\nNote:\n1. If you keeping encountering errors, unexpected situation, or you are not sure of proceeding, use RoleZero.ask_human to ask for help.\n2. Carefully review your progress at the current task, if your actions so far has not fulfilled the task instruction, you should continue with current task. Otherwise, finish current task by Plan.finish_current_task explicitly.\n3. Each time you finish a task, use RoleZero.reply_to_human to report your progress.\n4. Don't forget to append task first when all existing tasks are finished and new tasks are required.\n5. Avoid repeating tasks you have already completed. And end loop when all requirements are met.\nYou are an autonomous programmer\n\nThe special interface consists of a file editor that shows you 100 lines of a file at a time.\n\nYou can use terminal commands (e.g., cat, ls, cd) by calling Terminal.run.\n\nYou should carefully observe the behavior and results of the previous action, and avoid triggering repeated errors.\n\nIn addition to the terminal, I also provide additional tools. \n\nIf provided an issue link, you first action must be navigate to the issue page using Browser tool to understand the issue.\n\nYour must check if the repository exists at the current path. If it exists, navigate to the repository path. If the repository doesn't exist, please download it and then navigate to it.\nAll subsequent actions must be performed within this repository path. Do not leave this directory to execute any actions at any time.\n\nNote:\n\n1. If you open a file and need to get to an area around a specific line that is not in the first 100 lines, say line 583, don't just use the scroll_down command multiple times. Instead, use the Editor.goto_line command. It's much quicker. \n2. Always make sure to look at the currently open file and the current working directory (which appears right after the currently open file). The currently open file might be in a different directory than the working directory! Note that some commands, such as 'create', open files, so they might change the current open file.\n3. When using Editor.edit_file_by_replace, if there is no exact match, take the difference in indentation into consideration.\n4. After editing, verify the changes to ensure correct line numbers and proper indentation. Adhere to PEP8 standards for Python code.\n5. NOTE ABOUT THE EDIT COMMAND: Indentation really matters! When editing a file, make sure to insert appropriate indentation before each line! Ensuring the code adheres to PEP8 standards. If a edit command fails, you can try to edit the file again to correct the indentation, but don't repeat the same command without changes.\n6. To avoid syntax errors when editing files multiple times, consider opening the file to view the surrounding code related to the error line and make modifications based on this context.\n7. Ensure to observe the currently open file and the current working directory, which is displayed right after the open file. The open file might be in a different directory than the working directory. Remember, commands like 'create' open files and might alter the current open file.\n8. Effectively using Use search commands (`search_dir`, `search_file`, `find_file`) and navigation commands (`open_file`, `goto_line`) to locate and modify files efficiently. The Editor tool can fully satisfy the requirements. Follow these steps and considerations for optimal results:\n\n9. When the edit fails, try to enlarge the range of code.\n10. You must use the Editor.open_file command to open a file before using the Editor tool's edit command to modify it. When you open a file, any currently open file will be automatically closed.\n11. Remember, when you use Editor.insert_content_at_line or Editor.edit_file_by_replace, the line numbers will change after the operation. Therefore, if there are multiple operations, perform only the first operation in the current response, and defer the subsequent operations to the next turn.\n11.1 Do not use Editor.insert_content_at_line or Editor.edit_file_by_replace more than once per command list.\n12. If you choose Editor.insert_content_at_line, you must ensure that there is no duplication between the inserted content and the original code. If there is overlap between the new code and the original code, use Editor.edit_file_by_replace instead.\n13. If you choose Editor.edit_file_by_replace, the original code that needs to be replaced must start at the beginning of the line and end at the end of the line\n14. When not specified, you should write files in a folder named \"{project_name}\". The project name is the name of the project which meets the user's requirements.\n15. When provided system design or project schedule, you MUST read them first before making a plan, then adhere to them in your implementation, especially in the programming language, package, or framework. You MUST implement all code files prescribed in the system design or project schedule.\n16. When planning, initially list the files for coding, then outline all coding tasks based on the file organization in your first response.\n17. If you plan to read a file, do not include other plans in the same response.\n18. Write only one code file each time and provide its full implementation.\n19. When the requirement is simple, you don't need to create a plan, just do it right away.\n20. When using the editor, pay attention to current directory. When you use editor tools, the paths must be either absolute or relative to the editor's current directory.\n21. When planning, merge multiple tasks that operate on the same file into a single task. For example, create one task for writing unit tests for all functions in a class. Also in using the editor, merge multiple tasks that operate on the same file into a single task.\n22. When create unit tests for a code file, use Editor.read() to read the code file before planing. And create one plan to writing the unit test for the whole file.\n23. The priority to select technology stacks: Describe in Sytem Design and Project Schedule > Vite, React, JavaScript, MUI and Tailwind CSS > native HTML \n23.1. The React template is in the \"/mgx_template/templates/default_web_project/react_template\" and Vue template is in the \"/mgx_template/templates/default_web_project/vue_template\". \n24. If use Vite, Vue/React, JavaScript, MUI, and Tailwind CSS as the programming language or no programming language is specified in document or user requirement, follow these steps:\n24.1. Create the project folder if no exists. Use cmd \" mkdir -p {project_name} \"\n24.2. Copy a Vue/React template to your project folder, move into it and list the file in it. Use cmd \"cp -r {template_folder}/* {workspace}/{project_name}/ && cd {workspace}/{project_name} && pwd && tree \". This must be a single response without other commands.\n24.3. User Editor.read to read the content of files in the src and read the index.html in the project root before making a plan.\n24.4. List the files that you need to rewrite and create when making a plan. Indicate clearly what file to rewrite or create in each task. \"index.html\" and all files in the src folder always must be rewritten. Use Tailwind CSS for styling. Notice that you are in {project_name}.\n24.5. After finish the project. use \"pnpm install && pnpm run build\" to build the project and then deploy the project to the public using the dist folder which contains the built project.\n25. Engineer2.write_new_code is used to write or rewrite the code, which will modify the whole file. Editor.edit_file_by_replace is used to edit a small part of the file.\n26. Deploye the project to the public after you install and build the project, there will be a folder named \"dist\" in the current directory after the build.\n27. Use Engineer2.write_new_code to rewrite the whole file when you fail to use Editor.edit_file_by_replace more than three times.",
            "task_type_desc": null,
            "example": "\n<Example 1>\nnavigate to a path and read a file, then reply to human\n\n<Terminal.run>\n<cmd>\ncd PATH\n</cmd>\n</Terminal.run>\n\n<Editor.read>\n<path>\nPATH\n</path>\n</Editor.read>\n\n<RoleZero.reply_to_human>\n<content>\nsome reply text\n</content>\n</RoleZero.reply_to_human>\n\n<end></end>\n</Example 1>\n\n<Example 2>\nTo write multiple files/code\nNow I'll create the all files needed for the \"development requirements\", including:\n- component1.tsx for xxx\n- component2.tsx for xxx\n- ...\n\nSome thoughts...\n<Editor.write>\n<path>\ncomponent1.tsx\n</path>\n<content>\nYour code\n</content>\n</Editor.write>\n\nSome thoughts...\n<Editor.write>\n<path>\ncomponent2.tsx\n</path>\n<content>\nYour code\n</content>\n</Editor.write>\n...\n</Example 2>\n\n<Example 3: Working directly with user - unclear requirement>\nUser Requirement: Create an e-commerce platform.\nExplanation: The requirement is too vague and could mean many different things. Clarify the requirement first, but only ask about simple features within our capabilities (simple product catalog, basic shopping cart, user accounts). After getting clarification, MUST call FrontendEngineer.draft_plan.\n<RoleZero.ask_human>\n<question>\nI can help you build an e-commerce website. To create the right plan, I need to clarify:\n\n1. What type of products will you display and sell? (physical products, digital downloads, or both)\n2. Do you need a shopping cart and checkout page, or just a product showcase with inquiry forms?\n3. How many products approximately? (This helps me design the product browsing experience)\n\nI'll focus on creating a clean product display, user-friendly interface, and basic user authentication if needed.\n</question>\n</RoleZero.ask_human>\n</Example 3>\n\n<Example 4: Working directly with user - creating draft plan>\nUser Requirement: Build a website for selling motorcycles.\nExplanation: The requirement is complex and involves multiple features. First, create a draft plan and confirm with the human before distributing tasks to team members.\n\nI will create a structured draft plan for the motorcycle sales website and ask the human to confirm.\n\n<FrontendEngineer.draft_plan>\n<content>\n1. Product showcase page - Display all available motorcycles with product images, detailed specifications (engine, weight, price, etc.), and filtering options by category/price\n2. Administrator portal - Secure login page and dashboard for administrators to add/edit/delete motorcycle listings, update prices, and manage inventory\n3. Customer inquiry system - Contact form where potential customers can leave messages, select motorcycles they're interested in, and provide their contact information\n4. Shopping cart and checkout - Allow customers to add motorcycles to cart, review their selections, and complete purchase with payment integration\n5. Company information page - About us section, physical location with map, contact details, and business hours\n</content>\n</FrontendEngineer.draft_plan>\n</Example 4>\n\n<Example 4.1: User requests features beyond capabilities during plan confirmation>\nContext: After presenting the initial draft plan for the motorcycle website, the human responds with feedback requesting features beyond our capabilities.\nUser's feedback on draft plan: \"I approve task 1, 3, 4, 5. For task 3, I also need live video chat with sales representatives. For task 4, I need a shopping cart and checkout system that supports hundreds of users checking out at the same time.\"\nExplanation: The user requests live video chat and high-concurrency checkout system, which are beyond our capabilities. Suggest feasible alternatives and call draft_plan again.\n\nI understand you want enhanced communication and a robust checkout system. Let me suggest alternatives that I can build well:\n\nFor real-time communication with sales reps, instead of video chat, I can create a live inquiry form where customers can instantly submit questions, and you'll receive email notifications. I can also add a click-to-call button that opens their phone dialer with your sales number.\n\nFor the checkout system, I can build a shopping cart and checkout page that works smoothly for small to medium traffic (5-10 concurrent users). This is perfect for a boutique motorcycle shop. If you expect hundreds of simultaneous checkouts, you'll need enterprise-level infrastructure which is outside my scope.\n\nLet me show you the revised plan:\n\n<FrontendEngineer.draft_plan>\n<content>\n1. Product showcase page with motorcycle images, specifications, and filtering\n2. Administrator portal for managing motorcycle listings and inventory\n3. Enhanced customer inquiry system with instant notification forms and click-to-call buttons\n4. Shopping cart with checkout page optimized for small to medium traffic\n5. Company information page with contact details and location map\n</content>\n</FrontendEngineer.draft_plan>\n</Example 4.1>\n",
            "react_mode": "react",
            "max_react_loop": 200,
            "consecutive_react_cnt": 0,
            "max_consecutive_react_limit": 20,
            "tools": [
                "Editor:read,write,similarity_search",
                "RoleZero",
                "Terminal:run",
                "SearchEnhancedQA",
                "SupabaseManager",
                "Browser:click,goto,scroll",
                "Previewer",
                "FrontendEngineer",
                "ImageCreator",
                "CheckUI:run",
                "ImageGetter"
            ],
            "mcp_tools": [],
            "tool_recommender": {
                "tools": {
                    "Editor": {
                        "name": "Editor",
                        "path": "metagpt/tools/libs/editor.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool for reading, understanding, writing, and editing files. Support local file including text-based files (txt, md, json, py, html, js, css, etc.), pdf, docx, excluding images, csv, excel, or online links For agent: Do NOT initiate multiple Editor.insert_content_at_line calls at the same time, since the line number will change starting with the first execution, making line number of the subsequent calls incorrect. Split the calls into separate responses. For the same reason, Editor.insert_content_at_line should NOT go behind Editor.edit_file_by_replace in the same response. Perform insert operation in a separate response.",
                            "methods": {
                                "read": {
                                    "type": "async_function",
                                    "description": "Read the whole content of a file. Using an absolute path as the argument for specifying the file location.",
                                    "signature": "(self, path: str) -> metagpt.tools.libs.editor.FileBlock",
                                    "parameters": ""
                                },
                                "write": {
                                    "type": "async_function",
                                    "description": "Write the whole content to a file. When used, make sure content arg contains the full content of the file.",
                                    "signature": "(self, path: str, content: str)",
                                    "parameters": ""
                                },
                                "similarity_search": {
                                    "type": "async_function",
                                    "description": "Given a filename or a pathname, performs a similarity search for a given query across the specified file or path. This method searches the index repository for the provided query, classifying the specified files or paths. It performs a search on each cluster of files and handles non-indexed files separately, merging results from structured indices with any direct results from non-indexed files. This function call does not depend on other functions. ",
                                    "signature": "(query: str, path: Union[str, pathlib.Path]) -> List[str]",
                                    "parameters": "Args: query (str): The search query string to look for in the indexed files. path (Union[str, Path]): A pathname or filename to search within. Returns: List[str]: A list of results as strings, containing the text from the merged results and any direct results from non-indexed files. Example: >>> query = \"The problem to be analyzed from the document\" >>> file_or_path = \"The pathname or filename you want to search within\" >>> texts: List[str] = await Editor.similarity_search(query=query, path=file_or_path) >>> print(texts)"
                                }
                            },
                            "tool_path": "metagpt/tools/libs/editor.py"
                        },
                        "code": "@register_tool(\n    include_functions=[\n        \"write\",\n        \"read\",\n        \"open_file\",\n        \"goto_line\",\n        \"scroll_down\",\n        \"scroll_up\",\n        \"create_file\",\n        \"edit_file_by_replace\",\n        \"insert_content_at_line\",\n        \"append_file\",\n        \"search_dir\",\n        \"search_file\",\n        \"find_file\",\n        \"similarity_search\",\n    ]\n)\nclass Editor(BaseModel):\n    \"\"\"\n    A tool for reading, understanding, writing, and editing files.\n    Support local file including text-based files (txt, md, json, py, html, js, css, etc.), pdf, docx, excluding images, csv, excel, or online links\n    For agent: Do NOT initiate multiple Editor.insert_content_at_line calls at the same time, since the line number will change starting with the first execution, making line number of the subsequent calls incorrect. Split the calls into separate responses.\n               For the same reason, Editor.insert_content_at_line should NOT go behind Editor.edit_file_by_replace in the same response. Perform insert operation in a separate response.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    resource: EditorReporter = EditorReporter()\n    current_file: Optional[Path] = None\n    current_line: int = 1\n    window: int = 50\n    enable_auto_lint: bool = True\n    working_dir: Path = DEFAULT_WORKSPACE_ROOT\n\n    async def write(self, path: str, content: str):\n        \"\"\"Write the whole content to a file. When used, make sure content arg contains the full content of the file.\"\"\"\n\n        path = self._try_fix_path(path)\n        content = ensure_string(content)  # Undo possible MCP conversion\n\n        if \"\\n\" not in content and \"\\\\n\" in content:\n            # A very raw rule to correct the content: If 'content' lacks actual newlines ('\\n') but includes '\\\\n', consider\n            # replacing them with '\\n' to potentially correct mistaken representations of newline characters.\n            content = content.replace(\"\\\\n\", \"\\n\")\n        directory = os.path.dirname(path)\n        if directory and not os.path.exists(directory):\n            os.makedirs(directory)\n        async with aiofiles.open(path, \"w\", encoding=\"utf-8\") as f:\n            await f.write(content)\n        # self.resource.report(path, \"path\")\n        return f\"File successfully written and saved to {path}.\"\n\n    def _read_structural_data(self, path: str) -> FileBlock:\n        \"\"\"Read the whole content of a csv file. Using an absolute path as the argument for specifying the file location.\"\"\"\n        if path.endswith(\".csv\"):\n            df = pd.read_csv(path)\n        elif path.endswith(\".xlsx\"):\n            df = pd.read_excel(path)\n        else:\n            raise ValueError(\"The file must be a csv or excel file.\")\n        return FileBlock(path=path, content=f\"Showing head of the data:\\n{df.head().to_string()}\")\n\n    async def read(self, path: str) -> FileBlock:\n        \"\"\"Read the whole content of a file. Using an absolute path as the argument for specifying the file location.\"\"\"\n\n        path = self._try_fix_path(path)\n\n        if path.suffix in STRUCTURAL_DATA_SUFFIX:\n            result = self._read_structural_data(str(path))\n            self.resource.report(str(path), \"path\")\n            return result\n\n        error = FileBlock(\n            path=str(path),\n            content=\"The file is too large to read. Use `Editor.similarity_search` to read the file instead.\",\n        )\n        path = Path(path)\n\n        # Limit the size only for text documents\n        # TODO: Formulate a complete mechanism to handle files of all types and of any size\n        if path.stat().st_size > 5 * DEFAULT_MIN_TOKEN_COUNT and path.suffix in TEXT_DOC_SUFFIX:\n            return error\n\n        content = await File.read_text_file(path)\n        if not content:\n            return FileBlock(path=str(path), content=\"\")\n        if self.is_large_file(content=content):\n            return error\n        self.resource.report(str(path), \"path\")\n\n        result = FileBlock(\n            path=str(path),\n            content=content,\n        )\n        return result\n\n    @staticmethod\n    def _is_valid_filename(file_name: str) -> bool:\n        if not file_name or not file_name.strip():\n            return False\n        invalid_chars = '<>:\"/\\\\|?*'\n        if os.name == \"nt\":  # Windows\n            invalid_chars = '<>:\"/\\\\|?*'\n        elif os.name == \"posix\":  # Unix-like systems\n            invalid_chars = \"\\0\"\n\n        for char in invalid_chars:\n            if char in file_name:\n                return False\n        return True\n\n    @staticmethod\n    def _is_valid_path(path: Path) -> bool:\n        try:\n            return path.exists()\n        except PermissionError:\n            return False\n\n    @staticmethod\n    def _create_paths(file_path: Path) -> bool:\n        try:\n            if file_path.parent:\n                file_path.parent.mkdir(parents=True, exist_ok=True)\n            return True\n        except PermissionError:\n            return False\n\n    def _check_current_file(self, file_path: Optional[Path] = None) -> bool:\n        if file_path is None:\n            file_path = self.current_file\n        if not file_path or not file_path.is_file():\n            raise ValueError(\"No file open. Use the open_file function first.\")\n        return True\n\n    @staticmethod\n    def _clamp(value, min_value, max_value):\n        return max(min_value, min(value, max_value))\n\n    def _lint_file(self, file_path: Path) -> tuple[Optional[str], Optional[int]]:\n        \"\"\"Lint the file at the given path and return a tuple with a boolean indicating if there are errors,\n        and the line number of the first error, if any.\n\n        Returns:\n            tuple[str | None, int | None]: (lint_error, first_error_line_number)\n        \"\"\"\n\n        linter = Linter(root=self.working_dir)\n        lint_error = linter.lint(str(file_path))\n        if not lint_error:\n            # Linting successful. No issues found.\n            return None, None\n        return \"ERRORS:\\n\" + lint_error.text, lint_error.lines[0]\n\n    async def _print_window(self, file_path: Path, targeted_line: int, window: int):\n        self._check_current_file(file_path)\n        async with aiofiles.open(file_path, \"r\") as file:\n            content = await file.read()\n\n            # Ensure the content ends with a newline character\n            if not content.endswith(\"\\n\"):\n                content += \"\\n\"\n\n            lines = content.splitlines(True)  # Keep all line ending characters\n            total_lines = len(lines)\n\n            # cover edge cases\n            self.current_line = self._clamp(targeted_line, 1, total_lines)\n            half_window = max(1, window // 2)\n\n            # Ensure at least one line above and below the targeted line\n            start = max(1, self.current_line - half_window)\n            end = min(total_lines, self.current_line + half_window)\n\n            # Adjust start and end to ensure at least one line above and below\n            if start == 1:\n                end = min(total_lines, start + window - 1)\n            if end == total_lines:\n                start = max(1, end - window + 1)\n\n            output = \"\"\n\n            # only display this when there's at least one line above\n            if start > 1:\n                output += f\"({start - 1} more lines above)\\n\"\n            else:\n                output += \"(this is the beginning of the file)\\n\"\n            for i in range(start, end + 1):\n                _new_line = f\"{i:03d}|{lines[i - 1]}\"\n                if not _new_line.endswith(\"\\n\"):\n                    _new_line += \"\\n\"\n                output += _new_line\n            if end < total_lines:\n                output += f\"({total_lines - end} more lines below)\\n\"\n            else:\n                output += \"(this is the end of the file)\\n\"\n            output = output.rstrip()\n\n            return output\n\n    @staticmethod\n    def _cur_file_header(current_file: Path, total_lines: int) -> str:\n        if not current_file:\n            return \"\"\n        return f\"[File: {current_file.resolve()} ({total_lines} lines total)]\\n\"\n\n    async def set_workdir(self, path: str) -> None:\n        \"\"\"\n        Sets the working directory to the given path. eg: repo directory.\n        You MUST to set it up before open the file.\n\n        Args:\n            path: str: The path to set as the working directory.\n        \"\"\"\n        self.working_dir = Path(path)\n\n    async def open_file(\n        self, path: Union[Path, str], line_number: Optional[int] = 1, context_lines: Optional[int] = None\n    ) -> str:\n        \"\"\"Opens the file at the given path in the editor. If line_number is provided, the window will be moved to include that line.\n        It only shows the first 100 lines by default! Max `context_lines` supported is 2000, use `scroll up/down`\n        to view the file if you want to see more.\n\n        Args:\n            path: str: The path to the file to open, preferred absolute path.\n            line_number: int | None = 1: The line number to move to. Defaults to 1.\n            context_lines: int | None = 100: Only shows this number of lines in the context window (usually from line 1), with line_number as the center (if possible). Defaults to 100.\n        \"\"\"\n        if context_lines is None:\n            context_lines = self.window\n\n        path = self._try_fix_path(path)\n\n        if not path.is_file():\n            raise FileNotFoundError(f\"File {path} not found\")\n\n        self.current_file = path\n\n        total_lines = await self._count_file_lines(file_path=path)\n\n        if not isinstance(line_number, int) or line_number < 1 or line_number > total_lines:\n            raise ValueError(f\"Line number must be between 1 and {total_lines}\")\n        self.current_line = line_number\n\n        # Override WINDOW with context_lines\n        if context_lines is None or context_lines < 1:\n            context_lines = self.window\n\n        output = self._cur_file_header(path, total_lines)\n        output += await self._print_window(path, self.current_line, self._clamp(context_lines, 1, 2000))\n        self.resource.report(path, \"path\")\n        return output\n\n    async def goto_line(self, line_number: int) -> str:\n        \"\"\"Moves the window to show the specified line number.\n\n        Args:\n            line_number: int: The line number to move to.\n        \"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        if not isinstance(line_number, int) or line_number < 1 or line_number > total_lines:\n            raise ValueError(f\"Line number must be between 1 and {total_lines}\")\n\n        self.current_line = self._clamp(line_number, 1, total_lines)\n\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def scroll_down(self) -> str:\n        \"\"\"Moves the window down by 100 lines.\"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        self.current_line = self._clamp(self.current_line + self.window, 1, total_lines)\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def scroll_up(self) -> str:\n        \"\"\"Moves the window up by 100 lines.\"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        self.current_line = self._clamp(self.current_line - self.window, 1, total_lines)\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def create_file(self, filename: str) -> str:\n        \"\"\"Creates and opens a new file with the given name.\n\n        Args:\n            filename: str: The name of the file to create. If the parent directory does not exist, it will be created.\n        \"\"\"\n        filename = self._try_fix_path(filename)\n\n        if filename.exists():\n            raise FileExistsError(f\"File '{filename}' already exists.\")\n        await awrite(filename, \"\\n\")\n\n        self.open_file(filename)\n        return f\"[File {filename} created.]\"\n\n    @staticmethod\n    def _append_impl(lines, content):\n        \"\"\"Internal method to handle appending to a file.\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            content: str: The content to append to the file.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n        \"\"\"\n        content_lines = content.splitlines(keepends=True)\n        n_added_lines = len(content_lines)\n        if lines and not (len(lines) == 1 and lines[0].strip() == \"\"):\n            # file is not empty\n            if not lines[-1].endswith(\"\\n\"):\n                lines[-1] += \"\\n\"\n            new_lines = lines + content_lines\n            content = \"\".join(new_lines)\n        else:\n            # file is empty\n            content = \"\".join(content_lines)\n\n        return content, n_added_lines\n\n    @staticmethod\n    def _insert_impl(lines, start, content):\n        \"\"\"Internal method to handle inserting to a file.\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            start: int: The start line number for inserting.\n            content: str: The content to insert to the file.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n\n        Raises:\n            LineNumberError: If the start line number is invalid.\n        \"\"\"\n        inserted_lines = [content + \"\\n\" if not content.endswith(\"\\n\") else content]\n        if len(lines) == 0:\n            new_lines = inserted_lines\n        elif start is not None:\n            if len(lines) == 1 and lines[0].strip() == \"\":\n                # if the file with only 1 line and that line is empty\n                lines = []\n\n            if len(lines) == 0:\n                new_lines = inserted_lines\n            else:\n                new_lines = lines[: start - 1] + inserted_lines + lines[start - 1 :]\n        else:\n            raise LineNumberError(\n                f\"Invalid line number: {start}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n\n        content = \"\".join(new_lines)\n        n_added_lines = len(inserted_lines)\n        return content, n_added_lines\n\n    @staticmethod\n    def _edit_impl(lines, start, end, content):\n        \"\"\"Internal method to handle editing a file.\n\n        REQUIRES (should be checked by caller):\n            start <= end\n            start and end are between 1 and len(lines) (inclusive)\n            content ends with a newline\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            start: int: The start line number for editing.\n            end: int: The end line number for editing.\n            content: str: The content to replace the lines with.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n        \"\"\"\n        # Handle cases where start or end are None\n        if start is None:\n            start = 1  # Default to the beginning\n        if end is None:\n            end = len(lines)  # Default to the end\n        # Check arguments\n        if not (1 <= start <= len(lines)):\n            raise LineNumberError(\n                f\"Invalid start line number: {start}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n        if not (1 <= end <= len(lines)):\n            raise LineNumberError(\n                f\"Invalid end line number: {end}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n        if start > end:\n            raise LineNumberError(f\"Invalid line range: {start}-{end}. Start must be less than or equal to end.\")\n\n        # Split content into lines and ensure it ends with a newline\n        if not content.endswith(\"\\n\"):\n            content += \"\\n\"\n        content_lines = content.splitlines(True)\n\n        # Calculate the number of lines to be added\n        n_added_lines = len(content_lines)\n\n        # Remove the specified range of lines and insert the new content\n        new_lines = lines[: start - 1] + content_lines + lines[end:]\n\n        # Handle the case where the original lines are empty\n        if len(lines) == 0:\n            new_lines = content_lines\n\n        # Join the lines to create the new content\n        content = \"\".join(new_lines)\n        return content, n_added_lines\n\n    def _get_indentation_info(self, content, first_line):\n        \"\"\"\n        The indentation of the first insert line and the previous line, along with guidance for the next attempt.\n        \"\"\"\n        content_lines = content.split(\"\\n\")\n        pre_line = content_lines[first_line - 2] if first_line - 2 >= 0 else \"\"\n        pre_line_indent = len(pre_line) - len(pre_line.lstrip())\n        insert_line = content_lines[first_line - 1]\n        insert_line_indent = len(insert_line) - len(insert_line.lstrip())\n        ret_str = INDENTATION_INFO.format(\n            pre_line=pre_line,\n            pre_line_indent=pre_line_indent,\n            insert_line=insert_line,\n            insert_line_indent=insert_line_indent,\n            sub_4_space=max(insert_line_indent - 4, 0),\n            add_4_space=insert_line_indent + 4,\n        )\n        return ret_str\n\n    async def _edit_file_impl(\n        self,\n        file_name: Path,\n        start: Optional[int] = None,\n        end: Optional[int] = None,\n        content: str = \"\",\n        is_insert: bool = False,\n        is_append: bool = False,\n    ) -> str:\n        \"\"\"Internal method to handle common logic for edit_/append_file methods.\n\n        Args:\n            file_name: Path: The name of the file to edit or append to.\n            start: int | None = None: The start line number for editing. Ignored if is_append is True.\n            end: int | None = None: The end line number for editing. Ignored if is_append is True.\n            content: str: The content to replace the lines with or to append.\n            is_insert: bool = False: Whether to insert content at the given line number instead of editing.\n            is_append: bool = False: Whether to append content to the file instead of editing.\n        \"\"\"\n\n        ERROR_MSG = f\"[Error editing file {file_name}. Please confirm the file is correct.]\"\n        ERROR_MSG_SUFFIX = (\n            \"Your changes have NOT been applied. Please fix your edit command and try again.\\n\"\n            \"You either need to 1) Open the correct file and try again or 2) Specify the correct line number arguments.\\n\"\n            \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n        )\n\n        if not self._is_valid_filename(file_name.name):\n            raise FileNotFoundError(\"Invalid file name.\")\n\n        if not self._is_valid_path(file_name):\n            raise FileNotFoundError(\"Invalid path or file name.\")\n\n        if not self._create_paths(file_name):\n            raise PermissionError(\"Could not access or create directories.\")\n\n        if not file_name.is_file():\n            raise FileNotFoundError(f\"File {file_name} not found.\")\n\n        if is_insert and is_append:\n            raise ValueError(\"Cannot insert and append at the same time.\")\n\n        # Use a temporary file to write changes\n        content = str(content or \"\")\n        temp_file_path = \"\"\n        src_abs_path = file_name.resolve()\n        first_error_line = None\n        # The file to store previous content and will be removed automatically.\n        temp_backup_file = tempfile.NamedTemporaryFile(\"w\", delete=True)\n\n        try:\n            # lint the original file\n            if self.enable_auto_lint:\n                original_lint_error, _ = self._lint_file(file_name)\n\n            # Create a temporary file\n            with tempfile.NamedTemporaryFile(\"w\", delete=False) as temp_file:\n                temp_file_path = temp_file.name\n\n                # Read the original file and check if empty and for a trailing newline\n                async with aiofiles.open(file_name, \"r\") as original_file:\n                    lines = await original_file.readlines()\n\n                if is_append:\n                    content, n_added_lines = self._append_impl(lines, content)\n                elif is_insert:\n                    try:\n                        content, n_added_lines = self._insert_impl(lines, start, content)\n                    except LineNumberError as e:\n                        return (f\"{ERROR_MSG}\\n\" f\"{e}\\n\" f\"{ERROR_MSG_SUFFIX}\") + \"\\n\"\n                else:\n                    try:\n                        content, n_added_lines = self._edit_impl(lines, start, end, content)\n                    except LineNumberError as e:\n                        return (f\"{ERROR_MSG}\\n\" f\"{e}\\n\" f\"{ERROR_MSG_SUFFIX}\") + \"\\n\"\n\n                if not content.endswith(\"\\n\"):\n                    content += \"\\n\"\n\n                # Write the new content to the temporary file\n                temp_file.write(content)\n\n            # Replace the original file with the temporary file atomically\n            shutil.move(temp_file_path, src_abs_path)\n\n            # Handle linting\n            # NOTE: we need to get env var inside this function\n            # because the env var will be set AFTER the agentskills is imported\n            if self.enable_auto_lint:\n                # BACKUP the original file\n                temp_backup_file.writelines(lines)\n                temp_backup_file.flush()\n                lint_error, first_error_line = self._lint_file(file_name)\n\n                # Select the errors caused by the modification\n                def extract_last_part(line):\n                    parts = line.split(\":\")\n                    if len(parts) > 1:\n                        return parts[-1].strip()\n                    return line.strip()\n\n                def subtract_strings(str1, str2) -> str:\n                    lines1 = str1.splitlines()\n                    lines2 = str2.splitlines()\n\n                    last_parts1 = [extract_last_part(line) for line in lines1]\n\n                    remaining_lines = [line for line in lines2 if extract_last_part(line) not in last_parts1]\n\n                    result = \"\\n\".join(remaining_lines)\n                    return result\n\n                if original_lint_error and lint_error:\n                    lint_error = subtract_strings(original_lint_error, lint_error)\n                    if lint_error == \"\":\n                        lint_error = None\n                        first_error_line = None\n\n                if lint_error is not None:\n                    # show the first insert line.\n                    if is_append:\n                        # original end-of-file\n                        show_line = len(lines)\n                    # insert OR edit WILL provide meaningful line numbers\n                    elif start is not None and end is not None:\n                        show_line = int((start + end) / 2)\n                    else:\n                        raise ValueError(\"Invalid state. This should never happen.\")\n\n                    guidance_message = self._get_indentation_info(content, start or len(lines))\n                    guidance_message += (\n                        \"You either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\\n\"\n                        \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n                    )\n                    lint_error_info = ERROR_GUIDANCE.format(\n                        linter_error_msg=LINTER_ERROR_MSG + lint_error,\n                        window_after_applied=await self._print_window(file_name, show_line, n_added_lines + 20),\n                        window_before_applied=await self._print_window(\n                            Path(temp_backup_file.name), show_line, n_added_lines + 20\n                        ),\n                        guidance_message=guidance_message,\n                    ).strip()\n\n                    # recover the original file\n                    shutil.move(temp_backup_file.name, src_abs_path)\n                    return lint_error_info\n\n        except FileNotFoundError as e:\n            return f\"File not found: {e}\\n\"\n        except IOError as e:\n            return f\"An error occurred while handling the file: {e}\\n\"\n        except ValueError as e:\n            return f\"Invalid input: {e}\\n\"\n        except Exception as e:\n            guidance_message = self._get_indentation_info(content, start or len(lines))\n            guidance_message += (\n                \"You either need to 1) Specify the correct start/end line arguments or 2) Enlarge the range of original code.\\n\"\n                \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n            )\n            error_info = ERROR_GUIDANCE.format(\n                linter_error_msg=LINTER_ERROR_MSG + str(e),\n                window_after_applied=await self._print_window(file_name, start or len(lines), 100),\n                window_before_applied=await self._print_window(Path(temp_backup_file.name), start or len(lines), 100),\n                guidance_message=guidance_message,\n            ).strip()\n            # Clean up the temporary file if an error occurs\n            shutil.move(temp_backup_file.name, src_abs_path)\n            if temp_file_path and Path(temp_file_path).exists():\n                Path(temp_file_path).unlink()\n\n            raise Exception(f\"{error_info}\") from e\n\n        # Update the file information and print the updated content\n        n_total_lines = await self._count_file_lines(file_path=file_name, encoding=\"utf-8\")\n\n        if first_error_line is not None and int(first_error_line) > 0:\n            self.current_line = first_error_line\n        else:\n            if is_append:\n                self.current_line = max(1, len(lines))  # end of original file\n            else:\n                self.current_line = start or n_total_lines or 1\n\n        success_edit_info = SUCCESS_EDIT_INFO.format(\n            file_name=file_name.resolve(),\n            n_total_lines=n_total_lines,\n            window_after_applied=await self._print_window(file_name, self.current_line, self.window),\n            line_number=self.current_line,\n        ).strip()\n\n        return success_edit_info\n\n    async def edit_file_by_replace(self, file_name: str, to_replace: str, new_content: str) -> str:\n        \"\"\"Edit a file. This will search for `to_replace` in the given file and replace it with `new_content`.\n\n        Every *to_replace* must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n        * The `to_replace` parameter should match EXACTLY one or more consecutive lines from the original file. Be mindful of whitespaces!\n        * If the `to_replace` parameter is not unique in the file, the replacement will not be performed. Make sure to include enough context in `to_replace` to make it unique\n        * The `new_content` parameter should contain the edited lines that should replace the `to_replace`\n\n        For example, given a file \"path/to/example.txt\" with the following content:\n        ```\n        line 1\n        line 2\n        line 2\n        line 3\n        ```\n\n        EDITING: If you want to replace the second occurrence of \"line 2\", you can make `to_replace` unique:\n\n        edit_file_by_replace(\n            'path/to/example.txt',\n            to_replace='line 2\\nline 3',\n            new_content='new line\\nline 3',\n        )\n\n        This will replace only the second \"line 2\" with \"new line\". The first \"line 2\" will remain unchanged.\n\n        The resulting file will be:\n        ```\n        line 1\n        line 2\n        new line\n        line 3\n        ```\n\n        REMOVAL: If you want to remove \"line 2\" and \"line 3\", you can set `new_content` to an empty string:\n\n        edit_file_by_replace(\n            'path/to/example.txt',\n            to_replace='line 2\\nline 3',\n            new_content='',\n        )\n        \"\"\"\n        # search for `to_replace` in the file\n        # if found, replace it with `new_content`\n        # if not found, perform a fuzzy search to find the closest match and replace it with `new_content`\n        file_name = self._try_fix_path(file_name)\n\n        async with aiofiles.open(file_name, \"r\") as file:\n            file_content = await file.read()\n\n        if to_replace.strip() == \"\":\n            if file_content.strip() == \"\":\n                raise ValueError(f\"The file '{file_name}' is empty. Please use the append method to add content.\")\n            raise ValueError(\"`to_replace` must not be empty.\")\n\n        to_replace = self._rm_line_no(to_replace)\n        new_content = self._rm_line_no(new_content)\n\n        if file_content.count(to_replace) > 1:\n            raise ValueError(\n                f\"`to_replace` ```{to_replace}``` appears more than once, please include enough lines to make code in `to_replace` unique.\"\n            )\n        elif file_content.count(to_replace) == 0:\n            # If no exact match, try to find close matches using LCS\n            if len(to_replace) > 20:\n                # white spaces are usually missing in multi-line content, thus hard coding a threshold of 20\n                _, similarity, file_content_match = self.lcs(to_replace, file_content)\n                if similarity > 0.9:  # High similarity suggests missing whitespace\n                    raise ValueError(\n                        f\"`to_replace` ```{to_replace}``` not found in {file_name}. Pay attention to spaces and line breaks! Did you actually mean ```{file_content_match}```\"\n                    )\n            raise ValueError(\n                f\"`to_replace` ```{to_replace}``` not found in {file_name}. Read the file carefully with Editor.read and make sure you give the right content to replace. If you want to insert new content, use Editor.insert_content_at_line instead.\"\n            )\n\n        ### Take a easy way to replace the content with direct string operation. Also disable linting for now ###\n        ### TODO: unittest to cover this part ###\n        new_content = file_content.replace(to_replace, new_content)\n        async with aiofiles.open(file_name, \"w\") as file:\n            await file.write(new_content)\n\n        start = file_content.find(to_replace) + 1  # index of starting char\n        start_line_number = file_content[:start].count(\"\\n\") + 1\n        window_after_applied = await self._print_window(file_name, start_line_number, 30)\n        success_edit_info = SUCCESS_EDIT_INFO.format(\n            file_name=file_name.resolve(),\n            n_total_lines=\"\",\n            window_after_applied=window_after_applied,\n            line_number=start_line_number,\n        ).strip()\n        self.resource.report(file_name, \"path\")\n        return success_edit_info\n\n    async def insert_content_at_line(self, file_name: str, line_number: int, insert_content: str) -> str:\n        \"\"\"Insert a complete block of code before the given line number in a file. That is, the new content will start at the beginning of the specified line, and the existing content of that line will be moved down.\n        This operation will NOT modify the content of the lines before or after the given line number.\n        This function can not insert content the end of the file. Please use append_file instead,\n        For example, if the file has the following content:\n        ```\n        001|contain g\n        002|contain h\n        003|contain i\n        004|contain j\n        ```\n        and you call\n        insert_content_at_line(\n            file_name='file.txt',\n            line_number=2,\n            insert_content='new line'\n        )\n        the file will be updated to:\n        ```\n        001|contain g\n        002|new line\n        003|contain h\n        004|contain i\n        005|contain j\n        ```\n\n        Args:\n            file_name: (str): The name of the file to edit.\n            line_number (int): The line number (starting from 1) to insert the content after. The insert content will be add between the line of line_number-1 and line_number\n            insert_content (str): The content to insert betweed the previous_line_content and current_line_content.The insert_content must be a complete block of code at.\n        \"\"\"\n        file_name = self._try_fix_path(file_name)\n        ret_str = await self._edit_file_impl(\n            file_name,\n            start=line_number,\n            end=line_number,\n            content=insert_content,\n            is_insert=True,\n            is_append=False,\n        )\n        self.resource.report(file_name, \"path\")\n        return ret_str\n\n    async def append_file(self, file_name: str, content: str) -> str:\n        \"\"\"Append content to the given file.\n        It appends text `content` to the end of the specified file.\n\n        Args:\n            file_name: str: The name of the file to edit.\n            content: str: The content to insert.\n        \"\"\"\n        file_name = self._try_fix_path(file_name)\n        ret_str = await self._edit_file_impl(\n            file_name,\n            start=None,\n            end=None,\n            content=content,\n            is_insert=False,\n            is_append=True,\n        )\n        self.resource.report(file_name, \"path\")\n        return ret_str\n\n    async def search_dir(self, search_term: str, dir_path: str = \"./\") -> str:\n        \"\"\"Searches for search_term in all files in dir. If dir is not provided, searches in the current directory.\n\n        Args:\n            search_term: str: The term to search for.\n            dir_path: str: The path to the directory to search.\n        \"\"\"\n        dir_path = self._try_fix_path(dir_path)\n        if not dir_path.is_dir():\n            raise FileNotFoundError(f\"Directory {dir_path} not found\")\n\n        matches = []\n        for root, _, files in os.walk(dir_path):\n            for file in files:\n                if file.startswith(\".\"):\n                    continue\n                file_path = Path(root) / file\n                try:\n                    async with aiofiles.open(file_path, \"r\", errors=\"ignore\") as f:\n                        line_num = 0\n                        async for line in f:\n                            line_num += 1\n                            if search_term in line:\n                                matches.append((file_path, line_num, line.strip()))\n                except Exception:\n                    # Skip files that can't be read\n                    continue\n\n        if not matches:\n            return f'No matches found for \"{search_term}\" in {dir_path}'\n\n        num_matches = len(matches)\n        num_files = len(set(match[0] for match in matches))\n\n        if num_files > 100:\n            return f'More than {num_files} files matched for \"{search_term}\" in {dir_path}. Please narrow your search.'\n\n        res_list = [f'[Found {num_matches} matches for \"{search_term}\" in {dir_path}]']\n        for file_path, line_num, line in matches:\n            res_list.append(f\"{file_path} (Line {line_num}): {line}\")\n        res_list.append(f'[End of matches for \"{search_term}\" in {dir_path}]')\n        return \"\\n\".join(res_list)\n\n    async def search_file(self, search_term: str, file_path: Optional[str] = None) -> str:\n        \"\"\"Searches for search_term in file. If file is not provided, searches in the current open file.\n\n        Args:\n            search_term: str: The term to search for.\n            file_path: str | None: The path to the file to search.\n        \"\"\"\n        if file_path is None:\n            file_path = self.current_file\n        else:\n            file_path = self._try_fix_path(file_path)\n        if file_path is None:\n            raise FileNotFoundError(\"No file specified or open. Use the open_file function first.\")\n        if not file_path.is_file():\n            raise FileNotFoundError(f\"File {file_path} not found\")\n\n        matches = []\n        async with aiofiles.open(file_path, \"r\") as file:\n            i = 0\n            async for line in file:\n                i += 1\n                if search_term in line:\n                    matches.append((i, line.strip()))\n\n        res_list = []\n        if matches:\n            res_list.append(f'[Found {len(matches)} matches for \"{search_term}\" in {file_path}]')\n            for match in matches:\n                res_list.append(f\"Line {match[0]}: {match[1]}\")\n            res_list.append(f'[End of matches for \"{search_term}\" in {file_path}]')\n        else:\n            res_list.append(f'[No matches found for \"{search_term}\" in {file_path}]')\n\n        extra = {\"type\": \"search\", \"symbol\": search_term, \"lines\": [i[0] - 1 for i in matches]} if matches else None\n        self.resource.report(file_path, \"path\", extra=extra)\n        return \"\\n\".join(res_list)\n\n    async def find_file(self, file_name: str, dir_path: str = \"./\") -> str:\n        \"\"\"Finds all files with the given name or path in the specified directory.\n\n        Args:\n            file_name: str: The name or path of the file to find.\n                           If it's an absolute path, will match the exact path.\n                           If it's a relative path, will match the path suffix.\n                           If it's just a name, will match file names.\n            dir_path: str: The path to the directory to search.\n        \"\"\"\n        dir_path = self._try_fix_path(dir_path)\n        if not dir_path.is_dir():\n            raise FileNotFoundError(f\"Directory {dir_path} not found\")\n\n        file_path = Path(file_name)\n\n        # TODO: use asyncio.gather to concurrently check multiple paths\n        matches = []\n        for root, _, files in os.walk(dir_path):\n            for file in files:\n                current_path = Path(root) / file\n\n                if file_path.is_absolute():\n                    if current_path == file_path:\n                        matches.append(current_path)\n                elif \"/\" in str(file_path) or \"\\\\\" in str(file_path):\n                    if str(current_path).endswith(str(file_path)):\n                        matches.append(current_path)\n                else:\n                    if current_path.name == file_name:\n                        matches.append(current_path)\n\n        res_list = []\n        if matches:\n            res_list.append(f'[Found {len(matches)} matches for \"{file_name}\" in {dir_path}]')\n            for match in matches:\n                res_list.append(f\"{match}\")\n            res_list.append(f'[End of matches for \"{file_name}\" in {dir_path}]')\n        else:\n            res_list.append(f'[No matches found for \"{file_name}\" in {dir_path}]')\n        return \"\\n\".join(res_list)\n\n    def _try_fix_path(self, path: Union[Path, str]) -> Path:\n        \"\"\"Tries to fix the path if it is not absolute.\"\"\"\n        if not isinstance(path, Path):\n            path = Path(path)\n        if not path.is_absolute():\n            path = self.working_dir / path\n        return path\n\n    @staticmethod\n    def _rm_line_no(content: str) -> str:\n        line_no_pattern = \"\\n[0-9]{3}\\|\"\n        if re.search(line_no_pattern, content):\n            logger.warning(\"Line numbers detected in content and will be removed\")\n            content = re.sub(line_no_pattern, \"\\n\", content)\n        return content\n\n    @staticmethod\n    async def similarity_search(query: str, path: Union[str, Path]) -> List[str]:\n        \"\"\"Given a filename or a pathname, performs a similarity search for a given query across the specified file or path.\n\n        This method searches the index repository for the provided query, classifying the specified\n        files or paths. It performs a search on each cluster of files and handles non-indexed files\n        separately, merging results from structured indices with any direct results from non-indexed files.\n        This function call does not depend on other functions.\n\n        Args:\n            query (str): The search query string to look for in the indexed files.\n            path (Union[str, Path]): A pathname or filename to search within.\n\n        Returns:\n            List[str]: A list of results as strings, containing the text from the merged results\n                        and any direct results from non-indexed files.\n\n        Example:\n            >>> query = \"The problem to be analyzed from the document\"\n            >>> file_or_path = \"The pathname or filename you want to search within\"\n            >>> texts: List[str] = await Editor.similarity_search(query=query, path=file_or_path)\n            >>> print(texts)\n        \"\"\"\n        return await IndexRepo.cross_repo_search(query=query, file_or_path=path)\n\n    @staticmethod\n    def is_large_file(content: str, mix_token_count: int = 0) -> bool:\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n        token_count = len(encoding.encode(content))\n        mix_token_count = mix_token_count or DEFAULT_MIN_TOKEN_COUNT\n        return token_count >= mix_token_count\n\n    @staticmethod\n    def lcs(s1: str, s2: str) -> tuple[str, float, str]:\n        \"\"\"Find the longest common subsequence between two strings and the matching section from s2. Assuming s1 is always a subsequence of s2.\n\n        Args:\n            s1: First string (typically the to_replace content)\n            s2: Second string (typically the file content)\n\n        Returns:\n            tuple[str, float, str]: The LCS string, its similarity score (0-1), and the matching section from s2\n        \"\"\"\n        # Create DP table\n        m, n = len(s1), len(s2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n\n        # Fill DP table\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if s1[i - 1] == s2[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n\n        # Reconstruct LCS and track positions in s2\n        lcs = []\n        i, j = m, n\n        positions = []  # Store positions of matches in s2\n        while i > 0 and j > 0:\n            if s1[i - 1] == s2[j - 1]:\n                lcs.append(s1[i - 1])\n                positions.append(j - 1)\n                i -= 1\n                j -= 1\n            elif dp[i - 1][j] > dp[i][j - 1]:\n                i -= 1\n            else:\n                j -= 1\n\n        lcs = \"\".join(reversed(lcs))\n        positions = list(reversed(positions))\n\n        # Calculate similarity score\n        score = len(lcs) / len(s1)\n\n        # Extract the matching section from s2\n        if positions:\n            start = min(positions)\n            end = max(positions) + 1\n            matching_section = s2[start:end]\n        else:\n            matching_section = \"\"\n        matching_section = matching_section[\n            : int(1.2 * len(s1))\n        ]  # hard truncate to avoid greedily match too long string\n\n        return lcs, score, matching_section\n\n    async def _count_file_lines(self, file_path: Union[Path, str], encoding: Optional[str] = None) -> int:\n        \"\"\"Asynchronously count the total number of lines in a file.\n\n        Args:\n            file_path: Path to the file.\n            encoding: The encoding used to open the file. Defaults to None (system default encoding).\n\n        Returns:\n            int: Total number of lines in the file, minimum 1.\n        \"\"\"\n        total_lines = 0\n        async with aiofiles.open(file_path, \"r\", encoding=encoding) as file:\n            async for _ in file:\n                total_lines += 1\n        return max(1, total_lines)\n\n    async def remove_file(self, file_path: Union[Path, str], missing_ok: bool = True) -> str:\n        \"\"\"Remove a file from the file system.\n\n        Args:\n            file_path: Path to the file to remove.\n            missing_ok: If True, don't raise error if file doesn't exist.\n        \"\"\"\n        path = self._try_fix_path(file_path)\n        path.unlink(missing_ok=missing_ok)\n        await self.resource.async_report(file_path, \"delete\")\n        return f\"File {path} removed successfully\"\n",
                        "tags": []
                    },
                    "RoleZero": {
                        "name": "RoleZero",
                        "path": "metagpt/roles/di/role_zero.py",
                        "schemas": {
                            "type": "class",
                            "description": "A role who can think and act dynamically",
                            "methods": {
                                "ask_human": {
                                    "type": "async_function",
                                    "description": "Use this when you fail the current task or if you are unsure of the situation encountered. Your response should contain a brief summary of your situation, ended with a clear and concise question.",
                                    "signature": "(self, question: 'str') -> 'str'",
                                    "parameters": ""
                                },
                                "reply_to_human": {
                                    "type": "async_function",
                                    "description": "Reply to human user with the content provided. Use this when you have a clear answer or solution to the user's question.",
                                    "signature": "(self, content: 'str') -> 'str'",
                                    "parameters": ""
                                }
                            },
                            "tool_path": "metagpt/roles/di/role_zero.py"
                        },
                        "code": "@register_tool(include_functions=[\"ask_human\", \"reply_to_human\"])\nclass RoleZero(Role):\n    \"\"\"A role who can think and act dynamically\"\"\"\n\n    # Basic Info\n    name: str = \"Zero\"\n    profile: str = \"RoleZero\"\n    goal: str = \"\"\n    system_msg: Optional[list[str]] = None  # Use None to conform to the default value at llm.aask\n    system_prompt: str = SYSTEM_PROMPT  # Use None to conform to the default value at llm.aask\n    cmd_prompt: str = CMD_PROMPT\n    instruction: str = ROLE_INSTRUCTION\n    task_type_desc: Optional[str] = None\n    example: str = \"\"  # global think & act demonstration for the role\n\n    # React Mode\n    react_mode: Literal[\"react\"] = \"react\"\n    max_react_loop: int = 50\n    # consecutive react count after receiving a message, used for triggering human intervention\n    consecutive_react_cnt: int = 0\n    max_consecutive_react_limit: int = 10\n\n    # Tools\n    tools: list[str] = []  # Use special symbol [\"<all>\"] to indicate use of all registered tools\n    mcp_tools: list[str] = []\n    tool_recommender: Optional[ToolRecommender] = None\n    tool_execution_map: Annotated[dict[str, Callable], Field(exclude=True)] = {}\n    special_tool_commands: list[str] = [\"Plan.finish_current_task\", \"end\", \"Terminal.run\", \"RoleZero.ask_human\"]\n\n    # Equipped with three basic tools by default for optional use\n    editor: Annotated[Union[Editor, RemoteProxy], BeforeValidator(ensure_editor_instance)] = Field(\n        default_factory=resolve_editor\n    )\n    browser: Annotated[Union[Browser, RemoteProxy], BeforeValidator(ensure_browser_instance)] = Field(\n        default_factory=resolve_browser\n    )\n    terminal: Annotated[Union[Terminal, RemoteProxy], BeforeValidator(ensure_terminal_instance)] = Field(\n        default_factory=resolve_terminal\n    )\n    working_dir: str = Field(default_factory=lambda: str(DEFAULT_WORKSPACE_ROOT.resolve()))\n\n    # Others\n    observe_all_msg_from_buffer: bool = True\n    command_rsp: str = \"\"  # the raw string containing the commands\n    commands: list[dict] = []  # commands to be executed\n    memory_k: int = 30  # number of memories (messages) to use as historical context\n    use_fixed_sop: bool = False\n    use_summary: bool = True  # whether to summarize at the end\n    browser_is_active: bool = False  # Track browser state to check if it is open\n    think_task: Optional[asyncio.Task] = Field(default=None, exclude=True)  # Track the background thinking task\n\n    # @model_validator(mode=\"after\")\n    # def validate_and_init_mcp_tools(self) -> \"RoleZero\":\n    #     \"\"\"Initialize mcp_tools if empty and MCP servers are enabled\"\"\"\n    #     if not self.mcp_tools:\n    #         if self.config.mcp.has_enabled_servers():\n    #             self.mcp_tools = mcp_manager.list_tool_names()\n    #     return self\n\n    @model_validator(mode=\"after\")\n    def set_plan_and_tool(self) -> \"RoleZero\":\n        # We force using this parameter for DataAnalyst\n        assert self.react_mode == \"react\"\n\n        # Roughly the same part as DataInterpreter.set_plan_and_tool\n        self._set_react_mode(react_mode=self.react_mode, max_react_loop=self.max_react_loop)\n\n        tools = self.tools  # + self.mcp_tools\n        if tools and not self.tool_recommender:\n            self.tool_recommender = BM25ToolRecommender(tools=tools, force=True)\n        self.set_actions([RunCommand])\n\n        # HACK: Init Planner, control it through dynamic thinking; Consider formalizing as a react mode\n        self.planner = Planner(goal=\"\", working_memory=self.rc.working_memory, auto_run=True)\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_tool_execution(self) -> \"RoleZero\":\n        # default map, includes all special tool commands\n        self.tool_execution_map = {\n            \"Plan.append_task\": self.planner.plan.append_task,\n            \"Plan.reset_task\": self.planner.plan.reset_task,\n            \"Plan.replace_task\": self.planner.plan.replace_task,\n            \"Terminal.run\": self.terminal.run,\n            \"Terminal\": self.terminal.run,  # alias\n            \"Terminal.run_command\": self.terminal.run,  # alias\n            \"Terminal.preview\": self.terminal.preview,\n            \"RoleZero.ask_human\": self.ask_human,\n            \"RoleZero.reply_to_human\": self.reply_to_human,\n            \"SearchEnhancedQA.run\": SearchEnhancedQA().run,\n            \"Plan.finish_current_task\": self.planner.plan.finish_current_task,\n            \"end\": self._end,\n        }\n        self.tool_execution_map.update(\n            {\n                f\"Browser.{i}\": getattr(self.browser, i)\n                for i in [\n                    \"click\",\n                    \"close_tab\",\n                    \"go_back\",\n                    \"go_forward\",\n                    \"goto\",\n                    \"hover\",\n                    \"press\",\n                    \"scroll\",\n                    \"tab_focus\",\n                    \"type\",\n                    \"take_screenshot\",\n                ]\n            }\n        )\n        self.tool_execution_map.update(\n            {\n                f\"Editor.{i}\": getattr(self.editor, i)\n                for i in [\n                    \"append_file\",\n                    \"create_file\",\n                    \"edit_file_by_replace\",\n                    \"find_file\",\n                    \"goto_line\",\n                    \"insert_content_at_line\",\n                    \"open_file\",\n                    \"read\",\n                    \"scroll_down\",\n                    \"scroll_up\",\n                    \"search_dir\",\n                    \"search_file\",\n                    \"similarity_search\",\n                    # \"set_workdir\",\n                    \"write\",\n                ]\n            }\n        )\n\n        for tool_name in self.mcp_tools:\n            self.tool_execution_map[tool_name] = mcp_manager.get_tool_executor(tool_name)\n\n        # can be updated by subclass\n        self._update_tool_execution()\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_tool_working_dir(self) -> \"RoleZero\":\n        asyncio.create_task(self._update_workdir_for_tools(self.working_dir))\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_longterm_memory(self) -> \"RoleZero\":\n        \"\"\"Set up special memory for the role if enabled in the configuration.\"\"\"\n\n        # We set Role.rc.memory as SerializeAsAny to retain all fields in Memory subclasses, ensuring model_dump() includes subclass fields.\n        # However, for some of the fields, we want to use the latest values from the config.\n        # To avoid repeated args, we init the subclass first, then update the fields with the config\n        if self.config.role_zero.enable_compressable_memory:\n            kvs = self.rc.memory.model_dump()\n            kvs[\"compress_model_name\"] = self.config.role_zero.compress_model_name\n            self.rc.memory = CompressableMemory(**kvs)\n            self.rc.memory.memory_k = self.config.role_zero.memory_k\n            self.rc.memory.compress_k = self.config.role_zero.compress_k\n            self.rc.memory.compress_trigger_token_threshold = self.config.role_zero.compress_trigger_token_threshold\n            logger.info(f\"Compressable memory set for role '{self.name}'\")\n\n        elif self.config.role_zero.enable_longterm_memory:\n            # Use config.role_zero to initialize long-term memory\n            # The role name will be used as the collection name.\n            self.rc.memory = RoleZeroLongTermMemory(**self.rc.memory.model_dump())\n            memory = self.rc.memory\n            memory.persist_path = self.config.role_zero.longterm_memory_persist_path\n            memory.collection_name = self.name.replace(\" \", \"\")\n            memory.memory_k = self.config.role_zero.memory_k\n            memory.similarity_top_k = self.config.role_zero.similarity_top_k\n            memory.use_llm_ranker = self.config.role_zero.use_llm_ranker\n            logger.info(f\"Long-term memory set for role '{self.name}'\")\n\n        return self\n\n    def _update_tool_execution(self):\n        pass\n\n    def _retrieve_example(self) -> str:\n        \"\"\"Default implementation of example retrieval. Can be overwritten in subclasses.\"\"\"\n        return self.example\n\n    def _get_goal_from_memories(self):\n        \"\"\"Get goal from memories by finding the latest message from TL or UserRequirement\"\"\"\n        memories = self.get_memories()\n        for msg in reversed(memories):\n            if msg.sent_from == \"Mike\" or \"UserRequirement\" in msg.cause_by:\n                return msg.content\n        # Fallback to last message if no matching message found\n        return memories[-1].content if memories else \"\"\n\n    async def _think(self) -> bool:\n        \"\"\"Useful in 'react' mode. Use LLM to decide whether and what to do next.\"\"\"\n        # Compatibility\n        if self.use_fixed_sop:\n            return await super()._think()\n\n        ### 0. Preparation ###\n        if not self.rc.todo:\n            return False\n\n        if not self.planner.plan.goal:\n            self.planner.plan.goal = self._get_goal_from_memories()\n\n        ### 1. Example ###\n        example = self._retrieve_example()\n\n        ### 2. Plan Status ###\n        plan_status, current_task = get_plan_status(planner=self.planner)\n\n        ### 3. Tool/Command Info ###\n        tools = await self.tool_recommender.recommend_tools()\n        tool_info = json.dumps({tool.name: tool.schemas for tool in tools})\n\n        ### Role Instruction ###\n        instruction = self.instruction.strip()\n        system_prompt = self.system_prompt.format(\n            role_info=self._get_prefix(),\n            domain_info=self._get_domain_info(),\n            task_type_desc=self.task_type_desc,\n            available_commands=tool_info,\n            example=example,\n            instruction=instruction,\n        )\n\n        ### Make Decision Dynamically ###\n        prompt = self.cmd_prompt.format(\n            current_state=f\"current directory: {self.working_dir}\",\n            plan_status=plan_status,\n            current_task=current_task,\n        )\n\n        ### Recent Observation ###\n        if self.config.role_zero.enable_compressable_memory:\n            await self.rc.memory.compress(extra_context=super()._get_prefix())\n        memory = self.rc.memory.get(self.memory_k)\n        if self.browser_is_active:\n            memory = await parse_browser_actions(memory, browser=self.browser)\n        # memory = await parse_images(memory, llm=self.llm)\n\n        req = self.llm.format_msg(memory + [UserMessage(content=prompt)])\n\n        state_data = dict(\n            plan_status=plan_status,\n            current_task=current_task,\n            instruction=instruction,\n        )\n        logger.info(f\"{self.name}, ready to think\")\n        # background task to handle LLM thinking process\n        self.create_think_background_task(req, system_prompt, state_data)\n        return True\n\n    def create_think_background_task(self, req, system_prompt, state_data):\n        self.think_task = asyncio.create_task(self._think_background(req, system_prompt, state_data))\n\n    async def _think_background(self, req, system_prompt, state_data):\n        \"\"\"Background task to handle LLM thinking process.\"\"\"\n        async with ThoughtReporter(enable_llm_stream=True) as reporter:\n            await reporter.async_report({\"type\": \"react\"})\n            self.command_rsp = await self.llm_cached_aask(req=req, system_msgs=[system_prompt], state_data=state_data)\n        logger.info(f\"Command response:\\n{self.command_rsp}\")\n        rsp_hist = [mem.content for mem in self.rc.memory.get()]\n        self.command_rsp = await check_duplicates(\n            req=req, command_rsp=self.command_rsp, rsp_hist=rsp_hist, llm=self.llm\n        )\n\n    @exp_cache(context_builder=RoleZeroContextBuilder(), serializer=RoleZeroSerializer())\n    async def llm_cached_aask(self, *, req: list[dict], system_msgs: list[str], **kwargs) -> str:\n        \"\"\"Use `exp_cache` to automatically manage experiences.\n\n        The `RoleZeroContextBuilder` attempts to add experiences to `req`.\n        The `RoleZeroSerializer` extracts essential parts of `req` for the experience pool, trimming lengthy entries to retain only necessary parts.\n        \"\"\"\n        return await self.llm.aask(req, system_msgs=system_msgs)\n\n    def _get_domain_info(self) -> str:\n        return get_time_info() + MGX_INFO + get_enable_supabase_guidance()\n\n    async def _act(self) -> Message:\n        if self.use_fixed_sop:\n            return await super()._act()\n\n        command_outputs = []\n        async for cmd in parse_stream_commands2(role_instance=self, tool_execution_map=self.tool_execution_map):\n            is_success, output = await self._run_command(cmd)\n            command_outputs.append(output)\n            if not is_success:\n                break\n        if not command_outputs:\n            command_outputs = [\"No valid commands found for execution, pay attention to the output format.\"]\n        self.rc.memory.add(AIMessage(content=self.command_rsp))\n        outputs = \"\\n\\n\".join(command_outputs)\n        logger.info(f\"Commands outputs: \\n{outputs}\")\n        self.rc.memory.add(UserMessage(content=outputs, cause_by=RunCommand))\n\n        think_task = self.think_task\n        self.think_task = None\n        await think_task\n\n        return AIMessage(\n            content=f\"I have finished the task, please mark my task as finished. Outputs: {outputs}\",\n            sent_from=self.name,\n            cause_by=RunCommand,\n        )\n\n    async def _react(self) -> Message:\n        # NOTE: Diff 1: Each time landing here means news is observed, set todo to allow news processing in _think\n        self._set_state(0)\n\n        actions_taken = 0\n        self.consecutive_react_cnt = 0\n        rsp = AIMessage(content=\"No actions taken yet\", cause_by=Action)  # will be overwritten after Role _act\n        while actions_taken < self.rc.max_react_loop:\n            # NOTE: Diff 2: Keep observing within _react, news will go into memory, allowing adapting to new info\n            if await self._observe():\n                self.consecutive_react_cnt = 0\n                # Reset state to 0 to override potential `end` command (state = -1) and ensure the loop continues.\n                self._set_state(0)\n            # think\n            has_todo = await self._think()\n            if not has_todo:\n                break\n            # act\n            logger.debug(f\"{self._setting}: {self.rc.state=}, will do {self.rc.todo}\")\n            rsp = await self._act()\n            actions_taken += 1\n            self.consecutive_react_cnt += 1\n\n            # post-check\n            if self.rc.max_react_loop >= 10 and actions_taken >= self.rc.max_react_loop:\n                # If max_react_loop is a small value (e.g. < 10), it is intended to be reached and make the agent stop\n                logger.warning(f\"reached max_react_loop: {actions_taken}\")\n                human_rsp = await self.ask_human(\n                    \"I have reached my max action rounds, do you want me to continue? Yes or no\"\n                )\n                if \"yes\" in human_rsp.lower():\n                    actions_taken = 0\n            if self.consecutive_react_cnt >= self.max_consecutive_react_limit:\n                memory = self.get_memories(k=self.memory_k)\n                context = self.llm.format_msg(memory + [UserMessage(content=SUMMARIZE_STATUS_WHEN_CONSECUTIVE)])\n                question = await self.llm.aask(context)\n                human_rsp = await self.ask_human(question)\n                self.rc.memory.add(UserMessage(content=\"User's extra instruction: \" + human_rsp, cause_by=RunCommand))\n                self.consecutive_react_cnt = 0\n\n        return rsp  # return output from the last action\n\n    async def _run_command(self, cmd) -> tuple[bool, str]:\n        \"\"\"stream command execution\"\"\"\n        output = f\"Command {cmd['command_name']} executed\"\n        try:\n            # handle special command first\n            if self._is_special_command(cmd):\n                special_command_output = await self._run_special_command(cmd)\n                output += \":\" + special_command_output\n                cmd[\"status\"] = \"success\"  # mgx_backend need these fields\n                cmd[\"error_msg\"] = \"\"\n                return True, output\n\n            # run command as specified by tool_execute_map\n            if cmd[\"command_name\"] in self.tool_execution_map:\n                tool_obj = self.tool_execution_map[cmd[\"command_name\"]]\n                if inspect.iscoroutinefunction(tool_obj):\n                    tool_output = await tool_obj(**cmd[\"args\"])\n                else:\n                    tool_output = tool_obj(**cmd[\"args\"])\n                self._try_update_browser_state(cmd[\"command_name\"])\n                if tool_output:\n                    output += f\": {str(tool_output)}\"\n                cmd[\"status\"] = \"success\"\n                cmd[\"error_msg\"] = \"\"\n            else:\n                output = f\"Command {cmd['command_name']} not found.\"\n                cmd[\"status\"] = \"failed\"\n                cmd[\"error_msg\"] = f\"Command {cmd['command_name']} not found.\"\n                return False, output\n\n        except Exception:\n            tb = traceback.format_exc()\n            output += f\": {tb}\"\n            cmd[\"status\"] = \"failed\"\n            cmd[\"error_msg\"] = f\"Command {cmd['command_name']} failed: {tb}\"\n            return False, output\n\n        return True, output\n\n    def _is_special_command(self, cmd) -> bool:\n        return cmd[\"command_name\"] in self.special_tool_commands\n\n    async def _run_special_command(self, cmd) -> str:\n        \"\"\"command requiring special check or parsing\"\"\"\n        command_output = \"\"\n\n        if cmd[\"command_name\"] == \"Plan.finish_current_task\":\n            if not self.planner.plan.is_plan_finished():\n                self.planner.plan.finish_current_task()\n            command_output = (\n                \"Current task is finished. If you no longer need to take action, use the command 'end' to stop.\"\n            )\n\n        elif cmd[\"command_name\"] == \"end\":\n            command_output = await self._end()\n        elif cmd[\"command_name\"] == \"RoleZero.ask_human\":\n            args = cmd[\"args\"]\n            # fault tolerance for llm hallucination\n            question = args.get(\"question\") or args.get(\"content\") or args.get(\"message\")\n            assert question, \"Use kwarg 'question' to provide the question.\"\n            human_response = await self.ask_human(question=question)\n            self.consecutive_react_cnt = 0\n            if human_response.strip().lower().endswith((\"stop\", \"<stop>\")):\n                human_response += \"The user has asked me to stop because I have encountered a problem.\"\n                self.rc.memory.add(UserMessage(content=human_response, cause_by=RunCommand))\n                end_output = \"\\nCommand end executed:\"\n                end_output += await self._end()\n                return end_output\n            return human_response\n        # output from bash.run may be empty, add decorations to the output to ensure visibility.\n        elif \"Terminal\" in cmd[\"command_name\"]:  # Terminal.run and its alias\n            tool_obj = self.tool_execution_map[cmd[\"command_name\"]]\n            tool_output = await tool_obj(**cmd[\"args\"])\n            command_output = format_terminal_output(cmd=cmd, raw_output=tool_output)\n            self.working_dir = await self.terminal.get_cwd()\n            await self._update_workdir_for_tools(self.working_dir)\n            token_count = count_string_tokens(command_output, model_name=\"default\")\n            if token_count >= self.config.llm.max_token * 0.9:\n                command_output = \"OutputTooLongException: the content is too long to use. Please optimize the command parameters to reduce the amount of output.\"\n\n        return command_output\n\n    async def _update_workdir_for_tools(self, new_workdir: str) -> None:\n        \"\"\"Update working directory for all relevant tools.\n        This method can be overridden by subclasses to update additional tools.\n        \"\"\"\n        await self.editor.set_workdir(new_workdir)\n\n    async def ask_human(self, question: str) -> str:\n        \"\"\"Use this when you fail the current task or if you are unsure of the situation encountered. Your response should contain a brief summary of your situation, ended with a clear and concise question.\"\"\"\n        # NOTE: Can be overwritten in remote setting\n        from metagpt.environment.mgx.mgx_env import MGXEnv  # avoid circular import\n\n        if not isinstance(self.rc.env, MGXEnv):\n            return \"Not in MGXEnv, command will not be executed.\"\n        return await self.rc.env.ask_human(question, sent_from=self)\n\n    async def reply_to_human(self, content: str) -> str:\n        \"\"\"Reply to human user with the content provided. Use this when you have a clear answer or solution to the user's question.\"\"\"\n        # NOTE: Can be overwritten in remote setting\n        from metagpt.environment.mgx.mgx_env import MGXEnv  # avoid circular import\n\n        if not isinstance(self.rc.env, MGXEnv):\n            return \"Not in MGXEnv, command will not be executed.\"\n        return await self.rc.env.reply_to_human(content, sent_from=self)\n\n    async def _end(self, **kwarg):\n        self._set_state(-1)\n        memory = self.rc.memory.get(self.memory_k)\n        outputs = \"\"\n        # Summary of the Completed Task and Deliverables\n        if self.use_summary:\n            logger.info(\"end current run and summarize\")\n            async with ThoughtReporter() as reporter:\n                await reporter.async_report({\"type\": \"summary\"})\n                outputs = await self.llm.aask(self.llm.format_msg(memory + [UserMessage(SUMMARY_PROMPT)]))\n        return outputs\n\n    def _try_update_browser_state(self, command_name: str) -> None:\n        \"\"\"Update browser_is_active based on the command_name.\"\"\"\n\n        if command_name == \"Browser.goto\":\n            self.browser_is_active = True\n        elif command_name == \"Browser.close_tab\":\n            self.browser_is_active = False\n",
                        "tags": []
                    },
                    "Terminal": {
                        "name": "Terminal",
                        "path": "metagpt/tools/libs/terminal.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool for running terminal commands. Don't initialize a new instance of this class if one already exists.",
                            "methods": {
                                "run": {
                                    "type": "async_function",
                                    "description": "Executes a specified command in the terminal and streams the output back in real time. ",
                                    "signature": "(self, cmd: str) -> str",
                                    "parameters": "Args: cmd (str): The command to execute in the terminal. Returns: str: The command's output."
                                }
                            },
                            "tool_path": "metagpt/tools/libs/terminal.py"
                        },
                        "code": "@register_tool(include_functions=[\"run\"])\nclass Terminal(BaseModel):\n    \"\"\"A tool for running terminal commands. Don't initialize a new instance of this class if one already exists.\"\"\"\n\n    tabs: dict[str, Tab] = {}\n    current_tab_id: str = \"\"\n    current_tab: Optional[Tab] = Field(None, exclude=True)\n    #  The cmd in forbidden_terminal_commands will be replace by pass ana return the advise. example:{\"cmd\":\"forbidden_reason/advice\"}\n    forbidden_commands: dict[str, str] = {\n        # \"run dev\": \"Use Deployer.deploy_to_public instead.\",\n        \"run preview\": \"Use Deployer.deploy_to_public instead.\",\n        # serve cmd have a space behind it,\n        \"serve \": \"Use Deployer.deploy_to_public instead.\",\n        \"python -m http.server\": \"Use python -u -m http.server port_number instead.\",  # force buffer flush for http.server\n    }\n    timeout: float = 300.0  # timeout for reading output\n    working_dir: str = Field(default_factory=lambda: str(DEFAULT_WORKSPACE_ROOT.absolute()))\n\n    @model_validator(mode=\"after\")\n    def valid_current_tab(self):\n        if self.current_tab_id:\n            self.current_tab = self.tabs[self.current_tab_id]\n        return self\n\n    async def switch_tab(self, tab_id: str = \"\") -> str:\n        \"\"\"Switch tab based on tab_id. Useful for checking out new output from detached tabs or typing on desired tabs.\"\"\"\n        # NOTE: Hide from agent for now (not registered), need to solve the echo END_MARKER_VALUE problem\n        if tab_id in self.tabs:\n            self.current_tab = self.tabs[tab_id]\n            self.current_tab_id = tab_id\n            tab_new_output = self.current_tab.read_background_output()\n            return f\"Switched to tab {tab_id}, pwd is {self.current_tab.cwd}, the tab has new output: {tab_new_output}\"\n        return f\"Tab {tab_id} not found, created tabs are {list(self.tabs.keys())}\"\n\n    async def _create_new_tab(self) -> str:\n        \"\"\"create a new tab and switch to it\"\"\"\n        new_tab_id = f\"{len(self.tabs):02}\"\n        new_tab = Tab(tab_id=new_tab_id, cwd=self.cwd)\n        await new_tab.start()\n        self.tabs.update({new_tab_id: new_tab})\n        switch_tab_info = await self.switch_tab(new_tab_id)\n        return f\"Tab {new_tab_id} created. \" + switch_tab_info\n\n    @property\n    def cwd(self):\n        return self.current_tab.cwd if self.current_tab else self.working_dir\n\n    async def get_cwd(self):\n        return self.cwd\n\n    async def run(self, cmd: str) -> str:\n        \"\"\"\n        Executes a specified command in the terminal and streams the output back in real time.\n\n        Args:\n            cmd (str): The command to execute in the terminal.\n\n        Returns:\n            str: The command's output.\n        \"\"\"\n        if not self.current_tab:\n            await self._create_new_tab()\n\n        output = \"\"\n        # Precheck for forbidden commands\n        commands = re.split(r\"\\s*&&\\s*\", cmd)\n        for cmd_name, reason in self.forbidden_commands.items():\n            for command in commands:\n                if cmd_name in command:\n                    output += f\"{command} is forbidden. {reason}\\n\"\n        if output:\n            output = f\"Command not executed due to forbidden syntax:\\n{output}\\nPlease rewrite the whole command for execution.\"\n            return output\n\n        cmd = \" && \".join(commands)\n\n        # Send the command\n        current_tab = self.current_tab\n        if current_tab.is_running:\n            return (\n                \"Cannot execute the command because a previous one is still running in the current terminal tab\"\n                f\" (ID: {self.current_tab_id}).\"\n            )\n\n        # clear the output queue before execute command\n        output_queue = current_tab.output_queue\n        while not output_queue.empty():\n            output_queue.get_nowait()\n\n        await current_tab.execute(cmd)\n\n        tmp = []\n        is_service_flag = False\n        while True:\n            try:\n                # shorter timeout for service process by detecting output patterns such as localhost:port, ip:port\n                timeout = self.timeout if not is_service_flag else 3\n                line = await asyncio.wait_for(output_queue.get(), timeout=timeout)\n                if line is None:\n                    # ensure the read_and_process_output task is done\n                    await current_tab.task\n                    break\n                is_service_flag = is_service_flag or is_service_process(line)  # if True already, skip checking\n                tmp.append(line)\n            except asyncio.TimeoutError:\n                output_so_far = \"\".join(tmp)\n                if (returncode := current_tab.process.returncode) is not None:\n                    msg = f\"The terminal has exited with code {returncode}\"\n                    logger.warning(msg)\n                    return f\"{msg}, currently with output: {output_so_far}\"\n\n                current_tab.update_cwd()  # update cwd for the command still running\n                logger.info(f\"No more output after {timeout}s, detached from current tab and switched to a new tab\")\n\n                detached_tab_id = self.current_tab_id\n                new_tab_info = await self._create_new_tab()\n                instruction = DETACH_PROMPT.format(\n                    detached_tab_id=detached_tab_id,\n                    output_so_far=output_so_far,\n                    new_tab_info=new_tab_info,\n                )\n                # print(instruction)\n                return instruction\n        return \"\".join(tmp)\n\n    async def preview(self, tab_id: str, port: int, proj_name: str) -> str:\n        \"\"\"Preview a web project by forwarding a local port to public. Specify the id of the tab that runs the service, which is usually not the current tab but some detached tab.\"\"\"\n        if tab_id not in self.tabs:\n            return f\"Tab {tab_id} not found, created tabs are {list(self.tabs.keys())}, specify the correct tab_id that runs the service.\"\n        return await self.tabs[tab_id].preview(port, proj_name)\n\n    async def execute_in_conda_env(self, cmd: str, env) -> str:\n        \"\"\"\n        Executes a given command within a specified Conda environment automatically without\n        the need for manual activation. Users just need to provide the name of the Conda\n        environment and the command to execute.\n\n        Args:\n            cmd (str): The command to execute within the Conda environment.\n            env (str, optional): The name of the Conda environment to activate before executing the command.\n                                 If not specified, the command will run in the current active environment.\n\n        Returns:\n            str: The command's output, or an empty string if `daemon` is True, with output processed\n                 asynchronously in that case.\n\n        Note:\n            This function wraps `run`, prepending the necessary Conda activation commands\n            to ensure the specified environment is active for the command's execution.\n        \"\"\"\n        cmd = f\"conda run -n {env} {cmd}\"\n        return await self.run(cmd)\n\n    @handle_exception\n    async def close(self):\n        for tab in self.tabs.values():\n            await tab.close()\n        self.tabs = {}\n\n    @handle_exception\n    def __del__(self):\n        \"\"\"Close tabls when Terminal instance is destroyed\"\"\"\n        if self.tabs:\n            loop = asyncio.get_running_loop()\n            loop.create_task(self.close())\n",
                        "tags": []
                    },
                    "SearchEnhancedQA": {
                        "name": "SearchEnhancedQA",
                        "path": "metagpt/actions/search_enhanced_qa.py",
                        "schemas": {
                            "type": "class",
                            "description": "Question answering and info searching through search engine.",
                            "methods": {
                                "run": {
                                    "type": "async_function",
                                    "description": "Answer a query by leveraging web search results. ",
                                    "signature": "(self, query: 'str', rewrite_query: 'bool' = True) -> 'str'",
                                    "parameters": "Args: query (str): The original user query. rewrite_query (bool): Whether to rewrite the query for better web search results. Defaults to True. Returns: str: A detailed answer based on web search results. Raises: ValueError: If the query is invalid."
                                }
                            },
                            "tool_path": "metagpt/actions/search_enhanced_qa.py"
                        },
                        "code": "@register_tool(include_functions=[\"run\"])\nclass SearchEnhancedQA(Action):\n    \"\"\"Question answering and info searching through search engine.\"\"\"\n\n    name: str = \"SearchEnhancedQA\"\n    desc: str = \"Integrating search engine results to anwser the question.\"\n\n    collect_links_action: CollectLinks = Field(\n        default_factory=CollectLinks, description=\"Action to collect relevant links from a search engine.\"\n    )\n    web_browse_and_summarize_action: WebBrowseAndSummarize = Field(\n        default=None,\n        description=\"Action to explore the web and provide summaries of articles and webpages.\",\n    )\n    per_page_timeout: float = Field(\n        default=15, description=\"The maximum time for fetching a single page is in seconds. Defaults to 15s.\"\n    )\n    java_script_enabled: bool = Field(\n        default=False, description=\"Whether or not to enable JavaScript in the web browser context. Defaults to False.\"\n    )\n    user_agent: str = Field(\n        default=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.81\",\n        description=\"Specific user agent to use in browser.\",\n    )\n    extra_http_headers: dict = Field(\n        default={\"sec-ch-ua\": 'Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24'},\n        description=\"An object containing additional HTTP headers to be sent with every request.\",\n    )\n    max_chars_per_webpage_summary: int = Field(\n        default=4000, description=\"Maximum summary length for each web page content.\"\n    )\n    max_search_results: int = Field(\n        default=15,\n        description=\"Maximum number of search results (links) to collect using the collect_links_action. This controls the number of potential sources for answering the question.\",\n    )\n\n    _reporter: ThoughtReporter = PrivateAttr(ThoughtReporter())\n\n    @model_validator(mode=\"after\")\n    def initialize(self):\n        if self.web_browse_and_summarize_action is None:\n            web_browser_engine = WebBrowserEngine.from_browser_config(\n                self.config.browser,\n                proxy=self.config.proxy,\n                java_script_enabled=self.java_script_enabled,\n                extra_http_headers=self.extra_http_headers,\n                user_agent=self.user_agent,\n            )\n\n            self.web_browse_and_summarize_action = WebBrowseAndSummarize(web_browser_engine=web_browser_engine)\n\n        return self\n\n    async def run(self, query: str, rewrite_query: bool = True) -> str:\n        \"\"\"Answer a query by leveraging web search results.\n\n        Args:\n            query (str): The original user query.\n            rewrite_query (bool): Whether to rewrite the query for better web search results. Defaults to True.\n\n        Returns:\n            str: A detailed answer based on web search results.\n\n        Raises:\n            ValueError: If the query is invalid.\n        \"\"\"\n        async with self._reporter:\n            await self._reporter.async_report({\"type\": \"search\", \"stage\": \"init\"})\n            self._validate_query(query)\n\n            processed_query = await self._process_query(query, rewrite_query)\n            context = await self._build_context(processed_query)\n\n            return await self._generate_answer(processed_query, context)\n\n    def _validate_query(self, query: str) -> None:\n        \"\"\"Validate the input query.\n\n        Args:\n            query (str): The query to validate.\n\n        Raises:\n            ValueError: If the query is invalid.\n        \"\"\"\n\n        if not query.strip():\n            raise ValueError(\"Query cannot be empty or contain only whitespace.\")\n\n    async def _process_query(self, query: str, should_rewrite: bool) -> str:\n        \"\"\"Process the query, optionally rewriting it.\"\"\"\n\n        if should_rewrite:\n            return await self._rewrite_query(query)\n\n        return query\n\n    async def _rewrite_query(self, query: str) -> str:\n        \"\"\"Write a better search query for web search engine.\n\n        If the rewrite process fails, the original query is returned.\n\n        Args:\n            query (str): The original search query.\n\n        Returns:\n            str: The rewritten query if successful, otherwise the original query.\n        \"\"\"\n\n        prompt = REWRITE_QUERY_PROMPT.format(q=query)\n\n        try:\n            resp = await self._aask(prompt)\n            rewritten_query = self._extract_rewritten_query(resp)\n\n            logger.info(f\"Query rewritten: '{query}' -> '{rewritten_query}'\")\n            return rewritten_query\n        except Exception as e:\n            logger.warning(f\"Query rewrite failed. Returning original query. Error: {e}\")\n            return query\n\n    def _extract_rewritten_query(self, response: str) -> str:\n        \"\"\"Extract the rewritten query from the LLM's JSON response.\"\"\"\n\n        resp_json = json.loads(CodeParser.parse_code(response, lang=\"json\"))\n        return resp_json[\"query\"]\n\n    async def _build_context(self, query: str) -> str:\n        \"\"\"Construct a context string from web search citations.\n\n        Args:\n            query (str): The search query.\n\n        Returns:\n            str: Formatted context with numbered citations.\n        \"\"\"\n\n        citations = await self._search_citations(query)\n        context = \"\\n\\n\".join([f\"[[citation:{i+1}]] {c}\" for i, c in enumerate(citations)])\n\n        return context\n\n    async def _search_citations(self, query: str) -> list[str]:\n        \"\"\"Perform web search and summarize relevant content.\n\n        Args:\n            query (str): The search query.\n\n        Returns:\n            list[str]: Summaries of relevant web content.\n        \"\"\"\n\n        relevant_urls = await self._collect_relevant_links(query)\n        await self._reporter.async_report({\"type\": \"search\", \"stage\": \"searching\", \"urls\": relevant_urls})\n        if not relevant_urls:\n            logger.warning(f\"No relevant URLs found for query: {query}\")\n            return []\n\n        logger.info(f\"The Relevant links are: {relevant_urls}\")\n\n        web_summaries = await self._summarize_web_content(relevant_urls)\n        if not web_summaries:\n            logger.warning(f\"No summaries generated for query: {query}\")\n            return []\n\n        citations = list(web_summaries.values())\n\n        return citations\n\n    async def _collect_relevant_links(self, query: str) -> list[str]:\n        \"\"\"Search and rank URLs relevant to the query.\n\n        Args:\n            query (str): The search query.\n\n        Returns:\n            list[str]: Ranked list of relevant URLs.\n        \"\"\"\n\n        return await self.collect_links_action._search_and_rank_urls(\n            topic=query, query=query, max_num_results=self.max_search_results\n        )\n\n    async def _summarize_web_content(self, urls: list[str]) -> dict[str, str]:\n        \"\"\"Fetch and summarize content from given URLs.\n\n        Args:\n            urls (list[str]): List of URLs to summarize.\n\n        Returns:\n            dict[str, str]: Mapping of URLs to their summaries.\n        \"\"\"\n\n        contents = await self._fetch_web_contents(urls)\n        logger.info(\"Fetching web contents done\")\n\n        summaries = {}\n        await self._reporter.async_report(\n            {\"type\": \"search\", \"stage\": \"browsing\", \"pages\": [i.model_dump() for i in contents]}\n        )\n        for content in contents:\n            url = content.url\n            inner_text = content.inner_text.replace(\"\\n\", \"\")\n            if self.web_browse_and_summarize_action._is_content_invalid(inner_text):\n                logger.warning(f\"Invalid content detected for URL {url}: {inner_text[:10]}...\")\n                continue\n\n            summary = inner_text[: self.max_chars_per_webpage_summary]\n            summaries[url] = summary\n\n        return summaries\n\n    async def _fetch_web_contents(self, urls: list[str]) -> list[WebPage]:\n        return await self.web_browse_and_summarize_action._fetch_web_contents(\n            *urls, per_page_timeout=self.per_page_timeout\n        )\n\n    async def _generate_answer(self, query: str, context: str) -> str:\n        \"\"\"Generate an answer using the query and context.\n\n        Args:\n            query (str): The user's question.\n            context (str): Relevant information from web search.\n\n        Returns:\n            str: Generated answer based on the context.\n        \"\"\"\n\n        system_prompt = SEARCH_ENHANCED_QA_SYSTEM_PROMPT.format(context=context)\n\n        async with ThoughtReporter(uuid=self._reporter.uuid, enable_llm_stream=True) as reporter:\n            await reporter.async_report({\"type\": \"search\", \"stage\": \"answer\"})\n            rsp = await self._aask(query, [system_prompt])\n        return rsp\n",
                        "tags": []
                    },
                    "SupabaseManager": {
                        "name": "SupabaseManager",
                        "path": "metagpt/tools/libs/supabase_manager.py",
                        "schemas": {
                            "type": "class",
                            "description": "A management tool for interacting with Supabase database through Management API.",
                            "methods": {
                                "deploy_edge_function": {
                                    "type": "async_function",
                                    "description": "Deploy a new edge function or update existing ones to Supabase. ",
                                    "signature": "(self, function_slug: str, function_code: str, project_ref: Optional[str] = None, timeout: int = 10, verify_jwt: bool = False) -> Any",
                                    "parameters": "Args: function_slug: A unique identifier for the function, used in the URL to access it. It should be lowercase and can include hyphens instead of spaces for readability. function_code: The code of the function to deploy. project_ref: Supabase project ref, defaults to config.project_ref. timeout: Request timeout in seconds, defaults to 10. verify_jwt: Whether to verify JWT tokens for this function, defaults to False. Returns: dict: Response from the edge function deployment"
                                },
                                "execute_sql": {
                                    "type": "async_function",
                                    "description": "Execute SQL query on Supabase database. ",
                                    "signature": "(self, sql: str, project_ref: Optional[str] = None, timeout: int = 10) -> Any",
                                    "parameters": "Args: sql: SQL query to execute. project_ref: Supabase project ref, defaults to config.project_ref. timeout: Request timeout in seconds, defaults to 10. Returns: dict: Response from the database query Example: # Create a table with indexes and RLS policies. >>> sql = ''' ... BEGIN; ... -- Create scores table ... CREATE TABLE IF NOT EXISTS scores ( ... id UUID DEFAULT gen_random_uuid() PRIMARY KEY, ... user_id UUID REFERENCES auth.users NOT NULL, ... score INTEGER NOT NULL, ... created_at TIMESTAMP WITH TIME ZONE DEFAULT TIMEZONE('utc'::text, NOW()) NOT NULL ... ); ... ... -- Create indexes for query optimization ... CREATE INDEX IF NOT EXISTS scores_user_idx ON scores(user_id); ... CREATE INDEX IF NOT EXISTS scores_score_idx ON scores(score DESC); ... ... -- Setup Row Level Security (RLS) ... ALTER TABLE scores ENABLE ROW LEVEL SECURITY; ... CREATE POLICY \"allow_read_all_scores\" ON scores FOR SELECT USING (true); ... CREATE POLICY \"allow_insert_own_scores\" ON scores FOR INSERT TO authenticated with check ( (select auth.uid()) = user_id ); ... COMMIT; ... ''' >>> result = await execute_sql(sql) # Create storage bucket with RLS policies >>> bucket_sql = ''' ... BEGIN; ... -- Create storage bucket ... INSERT INTO storage.buckets (id, name, public) VALUES ('profile_pictures', 'profile_pictures', true) ON CONFLICT (id) DO NOTHING; ... ... -- Setup Row Level Security (RLS) for storage ... CREATE POLICY \"allow_public_read_profiles\" ON storage.objects FOR SELECT TO public USING (bucket_id = 'profile_pictures'); ... CREATE POLICY \"allow_users_upload_own_profile\" ON storage.objects FOR INSERT TO authenticated WITH CHECK (bucket_id = 'profile_pictures' AND (storage.foldername(name))[1] = auth.uid()::text); ... COMMIT; ... ''' >>> bucket_result = await execute_sql(bucket_sql)"
                                },
                                "get_session_schemas": {
                                    "type": "async_function",
                                    "description": "Get database schemas for a specific session_id. ",
                                    "signature": "(self, session_id: str, project_ref: Optional[str] = None, db_name: str = 'public', timeout: int = 10) -> list[dict[str, str]]",
                                    "parameters": "Args: session_id: A unique session identifier generated for each chat. Used for creating isolated database tables and managing development environments. project_ref: Supabase project ref, defaults to config.project_ref. db_name: Database schema name, defaults to 'public'. timeout: Request timeout in seconds, defaults to 10. Returns: list: A list of tables with their column definitions. Examples: # Get tables for specific session_id >>> session_schemas = await get_session_schemas(session_id=\"abc12\") >>> print(session_schemas) [ { \"table_name\": \"todo_abc12_tasks\", \"columns\": \"id uuid NOT NULL user_id uuid NOT NULL score integer NOT NULL\" }, { \"table_name\": \"notes_abc12_items\", \"columns\": \"id uuid NOT NULL user_id uuid NOT NULL content text NOT NULL\" } ]"
                                },
                                "list_edge_functions": {
                                    "type": "async_function",
                                    "description": "List all edge functions deployed to Supabase with their code in local working directory. Returns: list[dict]: A list of dictionaries containing function slugs and their code. Each dictionary has the format {\"function_slug\": str, \"function_code\": str}",
                                    "signature": "(self) -> list[dict]",
                                    "parameters": ""
                                }
                            },
                            "tool_path": "metagpt/tools/libs/supabase_manager.py"
                        },
                        "code": "@register_tool(include_functions=[\"execute_sql\", \"get_session_schemas\", \"deploy_edge_function\", \"list_edge_functions\"])\nclass SupabaseManager(BaseModel):\n    \"\"\"A management tool for interacting with Supabase database through Management API.\"\"\"\n\n    config: SupabaseConfig = Field(\n        default_factory=lambda: Config.default().supabase, description=\"The Supabase config\", exclude=True\n    )\n    working_dir: Path = Field(default=DEFAULT_WORKSPACE_ROOT, description=\"The working directory\")\n    function_slugs: set[str] = Field(\n        default_factory=set,\n        description=\"A set of slugs that uniquely identify deployed edge functions, used to reference and read the corresponding locally stored function code.\",\n    )\n    reported_env_vars: defaultdict[str, set[str]] = Field(\n        default_factory=lambda: defaultdict(set),\n        description=\"A dictionary tracking reported environment variables per session_id to avoid duplicates.\",\n    )\n\n    _reporter: ActionReporter = PrivateAttr(ActionReporter())\n\n    @property\n    def is_supabase_enabled(self) -> bool:\n        logger.debug(f\"Supabase enabled: {self.config.enable}, project_ref: {self.config.project_ref}\")\n        return self.config.enable\n\n    @property\n    def default_headers(self) -> dict:\n        return {\"Authorization\": f\"Bearer {self.config.access_token}\", \"Content-Type\": \"application/json\"}\n\n    async def set_workdir(self, path: str) -> None:\n        self.working_dir = Path(path)\n\n    async def execute_sql(self, sql: str, project_ref: Optional[str] = None, timeout: int = 10) -> Any:\n        \"\"\"Execute SQL query on Supabase database.\n\n        Args:\n            sql: SQL query to execute.\n            project_ref: Supabase project ref, defaults to config.project_ref.\n            timeout: Request timeout in seconds, defaults to 10.\n\n        Returns:\n            dict: Response from the database query\n\n        Example:\n            # Create a table with indexes and RLS policies.\n            >>> sql = '''\n            ... BEGIN;\n            ... -- Create scores table\n            ... CREATE TABLE IF NOT EXISTS scores (\n            ...     id UUID DEFAULT gen_random_uuid() PRIMARY KEY,\n            ...     user_id UUID REFERENCES auth.users NOT NULL,\n            ...     score INTEGER NOT NULL,\n            ...     created_at TIMESTAMP WITH TIME ZONE DEFAULT TIMEZONE('utc'::text, NOW()) NOT NULL\n            ... );\n            ...\n            ... -- Create indexes for query optimization\n            ... CREATE INDEX IF NOT EXISTS scores_user_idx ON scores(user_id);\n            ... CREATE INDEX IF NOT EXISTS scores_score_idx ON scores(score DESC);\n            ...\n            ... -- Setup Row Level Security (RLS)\n            ... ALTER TABLE scores ENABLE ROW LEVEL SECURITY;\n            ... CREATE POLICY \"allow_read_all_scores\" ON scores FOR SELECT USING (true);\n            ... CREATE POLICY \"allow_insert_own_scores\" ON scores FOR INSERT TO authenticated with check ( (select auth.uid()) = user_id );\n            ... COMMIT;\n            ... '''\n            >>> result = await execute_sql(sql)\n\n            # Create storage bucket with RLS policies\n            >>> bucket_sql = '''\n            ... BEGIN;\n            ... -- Create storage bucket\n            ... INSERT INTO storage.buckets (id, name, public) VALUES ('profile_pictures', 'profile_pictures', true) ON CONFLICT (id) DO NOTHING;\n            ...\n            ... -- Setup Row Level Security (RLS) for storage\n            ... CREATE POLICY \"allow_public_read_profiles\" ON storage.objects FOR SELECT TO public USING (bucket_id = 'profile_pictures');\n            ... CREATE POLICY \"allow_users_upload_own_profile\" ON storage.objects FOR INSERT TO authenticated WITH CHECK (bucket_id = 'profile_pictures' AND (storage.foldername(name))[1] = auth.uid()::text);\n            ... COMMIT;\n            ... '''\n            >>> bucket_result = await execute_sql(bucket_sql)\n        \"\"\"\n        url = f\"{self.config.management_base_url}/projects/{project_ref or self.config.project_ref}/database/query\"\n\n        return await apost(\n            url=url,\n            json={\"query\": sql},\n            as_json=True,\n            headers=self.default_headers,\n            timeout=timeout,\n            raise_for_status=True,\n        )\n\n    async def get_session_schemas(\n        self,\n        session_id: str,\n        project_ref: Optional[str] = None,\n        db_name: str = \"public\",\n        timeout: int = 10,\n    ) -> list[dict[str, str]]:\n        \"\"\"Get database schemas for a specific session_id.\n\n        Args:\n            session_id: A unique session identifier generated for each chat. Used for creating isolated database tables and managing development environments.\n            project_ref: Supabase project ref, defaults to config.project_ref.\n            db_name: Database schema name, defaults to 'public'.\n            timeout: Request timeout in seconds, defaults to 10.\n\n        Returns:\n            list: A list of tables with their column definitions.\n\n        Examples:\n            # Get tables for specific session_id\n            >>> session_schemas = await get_session_schemas(session_id=\"abc12\")\n            >>> print(session_schemas)\n            [\n                {\n                    \"table_name\": \"todo_abc12_tasks\",\n                    \"columns\": \"id uuid NOT NULL\\nuser_id uuid NOT NULL\\nscore integer NOT NULL\"\n                },\n                {\n                    \"table_name\": \"notes_abc12_items\",\n                    \"columns\": \"id uuid NOT NULL\\nuser_id uuid NOT NULL\\ncontent text NOT NULL\"\n                }\n            ]\n        \"\"\"\n\n        return await self._get_database_schemas(\n            session_id=session_id, project_ref=project_ref or self.config.project_ref, db_name=db_name, timeout=timeout\n        )\n\n    async def deploy_edge_function(\n        self,\n        function_slug: str,\n        function_code: str,\n        project_ref: Optional[str] = None,\n        timeout: int = 10,\n        verify_jwt: bool = False,\n    ) -> Any:\n        \"\"\"Deploy a new edge function or update existing ones to Supabase.\n\n        Args:\n            function_slug: A unique identifier for the function, used in the URL to access it. It should be lowercase and can include hyphens instead of spaces for readability.\n            function_code: The code of the function to deploy.\n            project_ref: Supabase project ref, defaults to config.project_ref.\n            timeout: Request timeout in seconds, defaults to 10.\n            verify_jwt: Whether to verify JWT tokens for this function, defaults to False.\n\n        Returns:\n            dict: Response from the edge function deployment\n        \"\"\"\n\n        url = f\"{self.config.management_base_url}/projects/{project_ref or self.config.project_ref}/functions/deploy\"\n\n        # Need form data\n        form_data, headers = await self._prepare_edge_function_form_data(\n            function_slug=function_slug, function_code=function_code, verify_jwt=verify_jwt\n        )\n\n        params = {\"slug\": function_slug}\n\n        result = await apost(\n            url=url,\n            data=form_data,\n            params=params,\n            headers=headers,\n            timeout=timeout,\n            as_json=True,\n            raise_for_status=True,\n        )\n\n        # Report and save\n        await asyncio.gather(\n            self._report_edge_function_deployment(function_slug, function_code),\n            self._save_edge_function_code(function_slug, function_code),\n        )\n\n        return result\n\n    async def list_edge_functions(self) -> list[dict]:\n        \"\"\"List all edge functions deployed to Supabase with their code in local working directory.\n\n        Returns:\n            list[dict]: A list of dictionaries containing function slugs and their code.\n            Each dictionary has the format {\"function_slug\": str, \"function_code\": str}\n        \"\"\"\n\n        if not self.function_slugs:\n            return []\n\n        tasks = []\n        for slug in self.function_slugs:\n            tasks.append(self._read_edge_function_code(slug))\n\n        function_codes = await asyncio.gather(*tasks)\n\n        return [\n            {\"function_slug\": slug, \"function_code\": code} for slug, code in zip(self.function_slugs, function_codes)\n        ]\n\n    @handle_exception\n    async def _report_edge_function_deployment(self, function_slug: str, function_code: str) -> None:\n        async with self._reporter:\n            env_code = self._get_env_code_with_deduplication(function_code)\n            data = {\"slug\": function_slug, \"code\": env_code, \"session_id\": self.config.session_id}\n            await self._reporter.async_report({\"type\": \"deploy_edge_function\", \"data\": data})\n\n    def _get_env_code_with_deduplication(self, function_code: str) -> str:\n        \"\"\"Get environment code with deduplication, fallback to function code on error.\"\"\"\n        try:\n            env_vars = self._extract_env_vars(function_code)\n            session_id = self.config.session_id\n\n            # Filter out already reported env vars for this session\n            new_env_vars = [env_var for env_var in env_vars if env_var not in self.reported_env_vars[session_id]]\n\n            if new_env_vars:\n                # Add new env vars to reported set\n                self.reported_env_vars[session_id].update(new_env_vars)\n                return \", \".join(new_env_vars)\n\n            return \"\"\n        except Exception:\n            return function_code\n\n    def _extract_env_vars(self, code: str) -> list[str]:\n        \"\"\"Extract Deno.env.get() calls from code.\"\"\"\n        # Match patterns like Deno.env.get('VARIABLE_NAME')\n        pattern = r\"Deno\\.env\\.get\\(['\\\"]([^'\\\"]+)['\\\"]\\)\"\n        matches = re.finditer(pattern, code)\n\n        return [match.group(0) for match in matches]\n\n    @handle_exception\n    async def _save_edge_function_code(self, function_slug: str, function_code: str) -> None:\n        self.function_slugs.add(function_slug)\n\n        file_path = self._get_edge_function_file_path(function_slug)\n        await awrite(file_path, function_code)\n\n    @handle_exception(default_return=\"\")\n    async def _read_edge_function_code(self, function_slug: str) -> str:\n        file_path = self._get_edge_function_file_path(function_slug)\n        return await aread(file_path)\n\n    def _get_edge_function_file_path(self, function_slug: str) -> Path:\n        return self.working_dir / \"supabase\" / \"functions\" / function_slug / \"index.ts\"\n\n    async def _get_database_schemas(\n        self,\n        project_ref: Optional[str] = None,\n        db_name: str = \"public\",\n        timeout: int = 10,\n        session_id: Optional[str] = None,\n    ) -> list[dict[str, str]]:\n        \"\"\"Get complete database schema information from Supabase including table names and their columns.\n\n        Args:\n            project_ref: Supabase project ref, defaults to config.supabase.project_ref.\n            db_name: Database schema name, defaults to 'public'.\n            timeout: Request timeout in seconds, defaults to 10.\n            session_id: A unique session identifier generated for each chat. Used for creating isolated database tables and managing development environments.\n\n        Returns:\n            list: A list of tables with their column definitions.\n\n        Examples:\n            # Get all tables\n            >>> schemas = await _get_database_schemas()\n            >>> print(schemas)\n            [\n                {\n                    \"table_name\": \"demo\",\n                    \"columns\": \"id uuid NOT NULL\\nuser_id uuid NOT NULL\"\n                }\n            ]\n\n            # Get tables for specific session_id\n            >>> session_schemas = await _get_database_schemas(session_id=\"abc12\")\n            >>> print(session_schemas)\n            [\n                {\n                    \"table_name\": \"todo_abc12_tasks\",\n                    \"columns\": \"id uuid NOT NULL\\nuser_id uuid NOT NULL\\nscore integer NOT NULL\"\n                },\n                {\n                    \"table_name\": \"notes_abc12_items\",\n                    \"columns\": \"id uuid NOT NULL\\nuser_id uuid NOT NULL\\ncontent text NOT NULL\"\n                }\n            ]\n        \"\"\"\n\n        query = f\"\"\"\n        SELECT \n            t.table_name,\n            string_agg(\n                c.column_name || ' ' || c.data_type || \n                CASE WHEN c.is_nullable = 'NO' THEN ' NOT NULL' ELSE '' END,\n                E'\\n'\n            ) as columns\n        FROM \n            information_schema.tables t\n            JOIN information_schema.columns c \n                ON t.table_name = c.table_name \n                AND t.table_schema = c.table_schema\n        WHERE \n            t.table_schema = '{db_name}'\n            AND t.table_type = 'BASE TABLE'\n        GROUP BY \n            t.table_name\n        ORDER BY \n            t.table_name;\n        \"\"\"\n        url = f\"{self.config.management_base_url}/projects/{project_ref}/database/query\"\n\n        table_schemas = await apost(\n            url,\n            json={\"query\": query},\n            as_json=True,\n            headers=self.default_headers,\n            timeout=timeout,\n            raise_for_status=True,\n        )\n\n        if session_id:\n            table_schemas = [schema for schema in table_schemas if session_id in schema[\"table_name\"]]\n\n        return table_schemas\n\n    async def _prepare_edge_function_form_data(\n        self,\n        function_slug: str,\n        function_code: str,\n        verify_jwt: bool = False,\n    ) -> tuple[aiohttp.FormData, dict]:\n        \"\"\"Prepare form data for edge function deployment.\"\"\"\n\n        # the function name is the same with the function slug\n        metadata = {\n            \"name\": function_slug,\n            \"entrypoint_path\": \"index.ts\",\n            \"verify_jwt\": verify_jwt,\n        }\n\n        form_data = aiohttp.FormData()\n        form_data.add_field(\"metadata\", json.dumps(metadata), content_type=\"application/json\")\n        form_data.add_field(\n            \"file\", BytesIO(function_code.encode(\"utf-8\")), filename=\"index.ts\", content_type=\"text/plain\"\n        )\n\n        headers = self.default_headers.copy()\n        headers.pop(\"Content-Type\", None)\n\n        return form_data, headers\n",
                        "tags": []
                    },
                    "Browser": {
                        "name": "Browser",
                        "path": "metagpt/tools/libs/browser.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool for browsing the web. Don't initialize a new instance of this class if one already exists. Note: If you plan to use the browser to assist you in completing tasks, then using the browser should be a standalone task, executing actions each time based on the content seen on the webpage before proceeding to the next step. ## Example Issue: The details of the latest issue in the geekan/MetaGPT repository. Plan: Use a browser to view the details of the latest issue in the geekan/MetaGPT repository. Solution: Let's first open the issue page of the MetaGPT repository with the `Browser.goto` command >>> await browser.goto(\"https://github.com/geekan/MetaGPT/issues\") From the output webpage, we've identified that the latest issue can be accessed by clicking on the element with ID \"1141\". >>> await browser.click(1141) Finally, we have found the webpage for the latest issue, we can close the tab and finish current task. >>> await browser.close_tab()",
                            "methods": {
                                "click": {
                                    "type": "async_function",
                                    "description": "clicks on an element with a specific id on the webpage.",
                                    "signature": "(self, element_id: 'int')",
                                    "parameters": ""
                                },
                                "goto": {
                                    "type": "async_function",
                                    "description": "Navigate to a specific URL.",
                                    "signature": "(self, url: 'str', timeout: 'float' = 90000)",
                                    "parameters": ""
                                },
                                "scroll": {
                                    "type": "async_function",
                                    "description": "Scroll the page up or down.",
                                    "signature": "(self, direction: \"Literal['down', 'up']\")",
                                    "parameters": ""
                                }
                            },
                            "tool_path": "metagpt/tools/libs/browser.py"
                        },
                        "code": "@register_tool(\n    tags=[\"web\", \"browse\"],\n    include_functions=[\n        \"click\",\n        \"close_tab\",\n        \"go_back\",\n        \"go_forward\",\n        \"goto\",\n        \"hover\",\n        \"press\",\n        \"scroll\",\n        \"tab_focus\",\n        \"type\",\n    ],\n)\nclass Browser(BaseModel):\n    \"\"\"A tool for browsing the web. Don't initialize a new instance of this class if one already exists.\n\n    Note: If you plan to use the browser to assist you in completing tasks, then using the browser should be a standalone\n    task, executing actions each time based on the content seen on the webpage before proceeding to the next step.\n\n    ## Example\n    Issue: The details of the latest issue in the geekan/MetaGPT repository.\n    Plan: Use a browser to view the details of the latest issue in the geekan/MetaGPT repository.\n    Solution:\n    Let's first open the issue page of the MetaGPT repository with the `Browser.goto` command\n\n    >>> await browser.goto(\"https://github.com/geekan/MetaGPT/issues\")\n\n    From the output webpage, we've identified that the latest issue can be accessed by clicking on the element with ID \"1141\".\n\n    >>> await browser.click(1141)\n\n    Finally, we have found the webpage for the latest issue, we can close the tab and finish current task.\n\n    >>> await browser.close_tab()\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    playwright: Optional[Playwright] = Field(default=None, exclude=True)\n    browser_instance: Optional[Browser_] = Field(default=None, exclude=True)\n    browser_ctx: Optional[BrowserContext] = Field(default=None, exclude=True)\n    page: Optional[Page] = Field(default=None, exclude=True)\n    accessibility_tree: list = Field(default_factory=list)\n    headless: bool = Field(default=True)\n    proxy: Optional[dict] = Field(default_factory=get_proxy_from_env)\n    is_empty_page: bool = Field(default=True)\n    reporter: BrowserReporter = Field(default_factory=BrowserReporter)\n    url: Optional[str] = Field(default=None)\n\n    _recover_task: Optional[asyncio.Task] = PrivateAttr(None)\n\n    @model_validator(mode=\"after\")\n    def valid_page(self):\n        if not self.is_empty_page:\n            if self.url:\n                loop = asyncio.get_running_loop()\n                # Create an asynchronous task to perform the recovery in the background to avoid blocking the startup\n                task = self._recover_task = loop.create_task(self._recover())\n\n                def callback(task):\n                    self._recover_task = None\n\n                task.add_done_callback(callback)\n            else:\n                self.is_empty_page = True\n        return self\n\n    async def _recover(self):\n        try:\n            await self.start()\n            page = self.page\n            await page.goto(self.url)\n            await self._wait_until_page_idle(page)\n            self.accessibility_tree = await get_accessibility_tree(page)\n        except Exception:\n            logger.exception(f\"fail to recover the page {self.url}\")\n            self.is_empty_page = True\n\n    async def _check_recover(self):\n        if self._recover_task:\n            await self._recover_task\n\n    async def start(self) -> None:\n        \"\"\"Starts Playwright and launches a browser\"\"\"\n        if self.playwright is None:\n            self.playwright = playwright = await async_playwright().start()\n            browser = self.browser_instance = await playwright.chromium.launch(headless=self.headless, proxy=self.proxy)\n            browser_ctx = self.browser_ctx = await browser.new_context()\n            self.page = await browser_ctx.new_page()\n\n    async def stop(self):\n        if self.playwright:\n            playwright = self.playwright\n            self.playwright = None\n            self.browser_instance = None\n            self.browser_ctx = None\n            await playwright.stop()\n\n    async def click(self, element_id: int):\n        \"\"\"clicks on an element with a specific id on the webpage.\"\"\"\n        await self._check_recover()\n        await click_element(self.page, get_backend_node_id(element_id, self.accessibility_tree))\n        return await self._wait_page()\n\n    async def type(self, element_id: int, content: str, press_enter_after: bool = False):\n        \"\"\"Use this to type the content into the field with id.\"\"\"\n        await self._check_recover()\n        if press_enter_after:\n            content += \"\\n\"\n        await click_element(self.page, get_backend_node_id(element_id, self.accessibility_tree))\n        await type_text(self.page, content)\n        return await self._wait_page()\n\n    async def hover(self, element_id: int):\n        \"\"\"Hover over an element with id.\"\"\"\n        await self._check_recover()\n        await hover_element(self.page, get_backend_node_id(element_id, self.accessibility_tree))\n        return await self._wait_page()\n\n    async def press(self, key_comb: str):\n        \"\"\"Simulates the pressing of a key combination on the keyboard (e.g., Ctrl+v).\"\"\"\n        await self._check_recover()\n        await key_press(self.page, key_comb)\n        return await self._wait_page()\n\n    async def scroll(self, direction: Literal[\"down\", \"up\"]):\n        \"\"\"Scroll the page up or down.\"\"\"\n        await self._check_recover()\n        await scroll_page(self.page, direction)\n        return await self._wait_page()\n\n    async def goto(self, url: str, timeout: float = 90000):\n        \"\"\"Navigate to a specific URL.\"\"\"\n        await self._check_recover()\n        if self.page is None:\n            await self.start()\n        async with self.reporter as reporter:\n            await reporter.async_report(url, \"url\")\n            await self.page.goto(url, timeout=timeout)\n            self.is_empty_page = False\n            self.url = self.page.url\n            return await self._wait_page()\n\n    async def go_back(self):\n        \"\"\"Navigate to the previously viewed page.\"\"\"\n        await self._check_recover()\n        await self.page.go_back()\n        return await self._wait_page()\n\n    async def go_forward(self):\n        \"\"\"Navigate to the next page (if a previous 'go_back' action was performed).\"\"\"\n        await self._check_recover()\n        await self.page.go_forward()\n        return await self._wait_page()\n\n    async def tab_focus(self, page_number: int):\n        \"\"\"Open a new, empty browser tab.\"\"\"\n        await self._check_recover()\n        page = self.browser_ctx.pages[page_number]\n        await page.bring_to_front()\n        return await self._wait_page()\n\n    async def close_tab(self):\n        \"\"\"Close the currently active tab.\"\"\"\n        await self._check_recover()\n        await self.page.close()\n        if len(self.browser_ctx.pages) > 0:\n            self.page = self.browser_ctx.pages[-1]\n        else:\n            self.page = await self.browser_ctx.new_page()\n            self.is_empty_page = True\n            self.url = \"\"\n        return await self._wait_page()\n\n    async def view(self):\n        await self._check_recover()\n        observation = parse_accessibility_tree(self.accessibility_tree)\n        return f\"Current Browser Viewer\\n URL: {self.page.url}\\nOBSERVATION:\\n{observation[0]}\\n\"\n\n    async def check_if_page_is_empty(self):\n        return self.is_empty_page\n\n    async def take_screenshot(self, url, timeout=90000, save_path=\"\") -> str:\n        logger.info(f\"Take screenshot for {url}\")\n        await self.goto(url, timeout=timeout)\n        await self.page.wait_for_load_state(\"networkidle\")\n        save_path = save_path or f\"{TMP}/test.png\"\n        await self.page.screenshot(path=save_path, full_page=True, type=\"png\")\n        return encode_image(save_path)\n\n    async def _wait_page(self):\n        page = self.page\n        await self._wait_until_page_idle(page)\n        self.accessibility_tree = await get_accessibility_tree(page)\n        await self.reporter.async_report(page, \"page\")\n        return f\"SUCCESS, URL: {page.url} have been loaded.\"\n\n    def _register_page_event(self, page: Page):\n        page.last_busy_time = time.time()\n        page.requests = set()\n        page.on(\"domcontentloaded\", self._update_page_last_busy_time)\n        page.on(\"load\", self._update_page_last_busy_time)\n        page.on(\"request\", self._on_page_request)\n        page.on(\"requestfailed\", self._on_page_requestfinished)\n        page.on(\"requestfinished\", self._on_page_requestfinished)\n        page.on(\"frameattached\", self._on_frame_change)\n        page.on(\"framenavigated\", self._on_frame_change)\n\n    async def _wait_until_page_idle(self, page) -> None:\n        if not hasattr(page, \"last_busy_time\"):\n            self._register_page_event(page)\n        else:\n            page.last_busy_time = time.time()\n        while time.time() - page.last_busy_time < 0.5:\n            await page.wait_for_timeout(100)\n\n    async def _update_page_last_busy_time(self, page: Page):\n        page.last_busy_time = time.time()\n\n    async def _on_page_request(self, request: Request):\n        page = request.frame.page\n        page.requests.add(request)\n        await self._update_page_last_busy_time(page)\n\n    async def _on_page_requestfinished(self, request: Request):\n        request.frame.page.requests.discard(request)\n\n    async def _on_frame_change(self, frame: Frame):\n        await self._update_page_last_busy_time(frame.page)\n\n    async def __aenter__(self):\n        await self.start()\n        return self\n\n    async def __aexit__(self, *args, **kwargs):\n        await self.stop()\n",
                        "tags": [
                            "web",
                            "browse"
                        ]
                    },
                    "Previewer": {
                        "name": "Previewer",
                        "path": "metagpt/tools/libs/previewer.py",
                        "schemas": {
                            "type": "class",
                            "description": "Preview a project.",
                            "methods": {
                                "preview_project": {
                                    "type": "async_function",
                                    "description": "Put a project into preview. You must specify the appropriate preparation commands. Some commands can be None if not applicable or necessary. ",
                                    "signature": "(self, project_path: str, install: str = None, dev: str = None, build: str = None, build_path: str = None) -> str",
                                    "parameters": "Args: project_path: the path to the project install: the command to install the project dev: the command to launch the project (usually as a service) in dev mode build: the command to build the project build_path: the path to the built assets, if applicable, e.g. dist"
                                }
                            },
                            "tool_path": "metagpt/tools/libs/previewer.py"
                        },
                        "code": "@register_tool(include_functions=[\"preview_project\"])\nclass Previewer(BaseModel):\n    \"\"\"Preview a project.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    has_previewed: bool = False\n    preview_url: str = \"\"\n    dev_proc: asyncio.subprocess.Process = Field(default=None, exclude=True)\n    project_path: str = \"\"\n    use_dev: bool = True  # turn this off for batch testing\n\n    async def preview_project(\n        self,\n        project_path: str,\n        install: str = None,\n        dev: str = None,\n        build: str = None,\n        build_path: str = None,\n    ) -> str:\n        \"\"\"\n        Put a project into preview. You must specify the appropriate preparation commands. Some commands can be None if not applicable or necessary.\n\n        Args:\n            project_path: the path to the project\n            install: the command to install the project\n            dev: the command to launch the project (usually as a service) in dev mode\n            build: the command to build the project\n            build_path: the path to the built assets, if applicable, e.g. dist\n        \"\"\"\n\n        # Convert string \"None\" to None for all parameters\n        def convert_none_string(value):\n            return None if value == \"None\" else value\n\n        yaml_config = {\n            \"install\": convert_none_string(install),\n            \"dev\": convert_none_string(dev),\n            \"build\": convert_none_string(build),\n            \"build_path\": convert_none_string(build_path),\n        }\n        await awrite(f\"{project_path}/.mgx/config.yaml\", yaml.dump(yaml_config))\n        return await self.preview(project_path)\n\n    async def preview(self, project_path: str, config_path: str = \".mgx/config.yaml\") -> str:\n        \"\"\"\n        Preview a project. The config is a yaml file under the project path in the form of\n\n        install: install command\n        dev: command to launch the service in dev mode to check for real-time changes\n        build: command to build the project for stable deployment\n        build_path: the path to the built assets, if applicable, e.g. dist\n\n        command can be null if not applicable\n        \"\"\"\n        # This is a simplified implementation for local testing and should be rewritten.\n\n        if self.has_previewed:\n            return \"Project already previewed, don't preview it again even if you modify the project.\"\n\n        if not os.path.exists(f\"{project_path}/{config_path}\"):\n            logger.warning(\"preview config not found\")\n            return \"\"\n\n        with open(f\"{project_path}/{config_path}\", \"r\") as f:\n            config = yaml.safe_load(f)\n\n        install_cmd = config[\"install\"] or \"true\"  # do nothing if null\n        dev_cmd = config[\"dev\"] or \"true\"  # do nothing if null\n        cmd = f\"cd {project_path} && {install_cmd}\"\n        if self.use_dev:\n            cmd += f\" && {dev_cmd}\"\n\n        logger.debug(f\"previewing {cmd}\")\n        self.dev_proc = await asyncio.create_subprocess_shell(\n            cmd,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.STDOUT,\n            start_new_session=True,  # allows killing all subprocesses when this process is killed\n        )\n        self.project_path = project_path\n\n        self.has_previewed = True\n\n        return (\n            f\"Executed: {cmd}. Project previewed successfully. Don't preview it again even if you modify the project.\"\n        )\n\n    async def get_preview_info(self) -> dict:\n        from metagpt.tools.libs.browser import Browser\n\n        preview_info = {\"screenshot_path\": \"tmp/test.png\", \"url\": \"http://localhost:5173/\"}\n        # Algorithm self-testing to get screenshots.\n        await Browser().take_screenshot(url=preview_info.get(\"url\"), save_path=preview_info.get(\"screenshot_path\"))\n        return preview_info\n\n    async def get_url(self) -> str:\n        # for local testing\n        if self.preview_url:\n            return self.preview_url\n\n        if not self.dev_proc or not self.dev_proc.stdout:\n            return \"No running preview process. Please use preview_project first.\"\n\n        url_pattern = re.compile(r\"http://[^\\s]+\")\n\n        while True:\n            line = await self.dev_proc.stdout.readline()\n            if not line:\n                break  # EOF or process ended\n            decoded_line = line.decode()\n            logger.info(decoded_line.strip())\n\n            match = url_pattern.search(decoded_line)\n            if match:\n                self.preview_url = match.group(0)\n                break\n\n        return self.preview_url\n\n    def cleanup(self):\n        \"\"\"clean up all resources\"\"\"\n        # for local testing\n        if self.dev_proc:\n            try:\n                os.killpg(self.dev_proc.pid, signal.SIGTERM)\n                logger.info(\"Previewer service cleaned up\")\n            except Exception:\n                logger.warning(f\"No previewer service to clean up: {traceback.format_exc()}\")\n            self.dev_proc = None\n\n    async def run_build_preview(self):\n        # for local testing\n        self.cleanup()  # clean the previous pnpm install process\n        cmd = f\"cd {self.project_path} && pnpm install && pnpm run build && pnpm preview\"\n        self.dev_proc = await asyncio.create_subprocess_shell(\n            cmd,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.STDOUT,\n            start_new_session=True,  # allows killing all subprocesses when this process is killed\n        )\n",
                        "tags": []
                    },
                    "FrontendEngineer": {
                        "name": "FrontendEngineer",
                        "path": "metagpt/roles/di/frontend_engineer.py",
                        "schemas": {
                            "type": "class",
                            "description": "A role who can think and act dynamically",
                            "methods": {
                                "draft_plan": {
                                    "type": "async_function",
                                    "description": "Present a development plan to human for confirmation before starting development. ",
                                    "signature": "(self, content: str) -> str",
                                    "parameters": "Args: content: A string containing numbered plan items (e.g., \"1. Task A 2. Task B 3. Task C\") Returns: str: The human's response/confirmed plan, or error message if format is incorrect"
                                },
                                "search_template": {
                                    "type": "async_function",
                                    "description": "Search for a suitable template to start development. The query should be very concise, focusing only on key points that can impact template selection and leaving out all other details. Key points include the application type (website, app, game, dashboard, slides, video, etc.) and the tech stack, if any (don't specify one if not instructed). Example queries: - web game - personal website with Shadcn-UI and Tailwind CSS - Slides for presentation with Slidev - one simple html web page",
                                    "signature": "(self, query: str) -> str",
                                    "parameters": ""
                                }
                            },
                            "tool_path": "metagpt/roles/di/frontend_engineer.py"
                        },
                        "code": "@register_tool(include_functions=[\"search_template\", \"draft_plan\"])\nclass FrontendEngineer(Engineer2):\n    example: str = FE_EXAPMLE\n\n    cmd_prompt: str = FE_CMD_PROMPT\n    previewer: Previewer = Field(default_factory=Previewer)\n    tools: list[str] = [\n        \"Editor:read,write,similarity_search\",\n        \"RoleZero\",\n        \"Terminal:run\",\n        \"SearchEnhancedQA\",\n        \"SupabaseManager\",\n        \"Browser:click,goto,scroll\",\n        \"Previewer\",\n        \"FrontendEngineer\",\n        \"ImageCreator\",\n        \"CheckUI:run\",\n        \"ImageGetter\",\n    ]\n    warning_prompt: str = \"\"\n    image_getter: Optional[ImageGetter] = Field(default=None, exclude=True)\n\n    # Regarding template use:\n    # 1. Set use_search_template to False to disable template\n    # 2. Set template_tool to FixedSearchTemplate() to skip RAG and use a fixed template\n    # 3. Set template_tool to None (unchanged) or SearchTemplate() to perform a full template search\n    use_search_template: bool = True\n    is_first_dev_request: bool = True\n    template_tool: BaseSearchTemplate = None\n    template_info: str = GENERAL_WEB_APP_TEMPLATE_PROMPT\n\n    # a larger limit to allow for complex development\n    max_consecutive_react_limit: int = 20\n    max_react_loop: int = 200\n\n    # Image generation tools\n    image_creator: ImageCreator = Field(default_factory=ImageCreator, exclude=True)\n\n    def _update_tool_execution(self):\n        super()._update_tool_execution()\n        self.tool_execution_map[\"Previewer.preview_project\"] = self.previewer.preview_project\n        self.tool_execution_map[\"FrontendEngineer.search_template\"] = self.search_template\n        self.tool_execution_map.update({\"Editor.write\": self.write_check_images})\n        self.tool_execution_map[\"FrontendEngineer.draft_plan\"] = self.draft_plan\n        self.tool_execution_map[\"ImageCreator.generate_image\"] = self.image_creator.generate_image\n\n        if self.config.useragent.enable:\n            self.tool_execution_map[\"CheckUI.run\"] = CheckUI().run\n\n        if ImageGetter.is_available():\n            if self.image_getter is None:\n                self.image_getter = ImageGetter(project_dir=Path(self.working_dir))\n            self.tool_execution_map[\"ImageGetter.get\"] = self.image_getter.get\n            self.tool_execution_map[\"ImageGetter.process\"] = self.image_getter.process\n        else:\n            logger.warning(\"ImageGetter is not available (missing API key or invalid api_type)\")\n\n    @model_validator(mode=\"after\")\n    def set_search_template_tool(self):\n        if self.template_tool is None and self.use_search_template:\n            self.template_tool = SearchTemplate(output_dir=self.working_dir)\n            logger.info(\"SearchTemplate set\")\n        else:\n            logger.warning(\"SearchTemplate not set\")\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_image_creator_dependencies(self) -> \"FrontendEngineer\":\n        \"\"\"Set up image creator with necessary dependencies.\"\"\"\n        self.image_creator.working_dir = self.working_dir\n        return self\n\n    async def _update_workdir_for_tools(self, new_workdir: str) -> None:\n        \"\"\"Update working directory for all relevant tools including ImageGetter, ImageCreator .\n\n        This ensures ImageGetter creates public/ directory in the correct location\n        (e.g., inside prototype_template, not at the workspace root).\n        \"\"\"\n        # Update editor (parent class behavior)\n        await super()._update_workdir_for_tools(new_workdir)\n\n        # Update image_creator's working_dir\n        self.image_creator.working_dir = new_workdir\n\n        # Update ImageGetter's project_dir to the new working directory\n        if self.image_getter:\n            self.image_getter.update_project_dir(Path(new_workdir))\n\n    async def _think(self) -> bool:\n        self.instruction = FRONTEND_ENGINEER_PROMPT.format(\n            template_info=self.template_info, backend_info=get_backend_prompt_for_fe()\n        )\n        self._format_appendix()\n\n        res = await super()._think()\n        return res\n\n    def _format_appendix(self) -> str:\n        \"\"\"Format appendix with current paths.\"\"\"\n        if self.rc.memory.appendix:\n            return self.rc.memory.appendix\n\n        logger.info(f\"Extracted workspace info: {self.working_dir}\")\n        # Set up file paths based on workspace structure\n        workspace = Path(self.working_dir)\n        design_docs_dir = workspace / \"docs/design\"\n        tasks_dir = workspace / \"docs/tasks\"\n        if not tasks_dir.exists():\n            return \"\"\n        report_docs_dir = workspace / \"docs/report\"\n        alex_todo_filename = tasks_dir / \"Alex_todo.md\"\n        bob_todo_filename = tasks_dir / \"Bob_todo.md\"\n        src_dir = workspace / \"app\"\n        val = APPENDIX.format(\n            src_dir=str(src_dir),\n            design_docs=str(design_docs_dir),\n            bob_todo=str(bob_todo_filename),\n            alex_todo=str(alex_todo_filename),\n            report_docs=str(report_docs_dir),\n            warning_prompt=self.warning_prompt,\n        )\n        self.rc.memory.appendix = val\n        return val\n\n    async def write_check_images(self, path: str, content: str) -> str:\n        \"\"\"Parse the code and check if it contains invalid images paths/URLs. If it does, replace them with the valid images.\"\"\"\n\n        if path.endswith((\".tsx\", \".ts\")):\n            logger.info(f\"Checking duplicate images for TypeScript file: {path}\")\n            # Delegate to image_creator\n            content = await self.image_creator.check_and_fix_duplicate_images(content, path)\n        else:\n            logger.info(f\"Skipping duplicate image check for non-TypeScript file: {path}\")\n\n        # Skip image validation for Markdown files (e.g., todo.md, README.md)\n        # These files often contain image filename plans that haven't been generated yet\n        if path.endswith((\".md\", \".markdown\")):\n            logger.info(f\"Skipping image validation for Markdown file: {path}\")\n            return await self.editor.write(path, content)\n\n        # Use self.image_getter directly (already initialized and updated by _update_workdir_for_tools)\n        if self.image_getter is None:\n            return await self.editor.write(path, content)\n\n        # Use the abstracted image validation and replacement method\n        processed_content = await self.image_getter.validate_and_replace_images(content=content, path=path)\n\n        # Write the processed content to file\n        output = await self.editor.write(path, str(processed_content))\n        return output\n\n    async def search_template(self, query: str) -> str:\n        \"\"\"\n        Search for a suitable template to start development. The query should be very concise, focusing only on key points that can impact template selection and leaving out all other details.\n        Key points include the application type (website, app, game, dashboard, slides, video, etc.) and the tech stack, if any (don't specify one if not instructed).\n        Example queries:\n         - web game\n         - personal website with Shadcn-UI and Tailwind CSS\n         - Slides for presentation with Slidev\n         - one simple html web page\n        \"\"\"\n        if not self.is_first_dev_request:\n            return \"Template search was performed previously. You should develop based on the existing template.\"\n\n        self.is_first_dev_request = False\n\n        name, todo = await build_from_function_sea(query=query, workdir=self.working_dir)\n        if todo:\n            return f\"{name} has todo list: `{todo}`, project workdir: `{self.working_dir}`, {name} need to complete ALL the TODO LIST in `{todo}` FIRST.\"\n\n        # 1. Search for matching template\n        template, extra_user_info = await self.template_tool.search(query)\n        if template:\n            target_dir = await self.template_tool.copy_template(template.template_path)\n            source_template_path = template.template_path\n            template.template_path = target_dir\n            self.template_info = await self.template_tool.get_template_info(\n                template_info=template, source_template_path=source_template_path\n            )\n\n            # Update template part in instruction\n            content = EXRTA_INFO_PROMPT.format(template_name=template.name, target_dir=str(target_dir))\n            if extra_user_info:\n                content = f\"Additional information provided by the user:{extra_user_info}\\n\\n{content}\"\n            logger.info(f\"Template retrieved: {template.name}\")\n\n            # run commands based on template config\n            preview_result = await self.previewer.preview(project_path=str(target_dir))\n            content += f\"\\n\\n{preview_result}\"\n            logger.info(\"Template previewed\")\n\n            # switch the role's working directory\n            await self.terminal.run(f\"cd {str(target_dir)}\")\n            self.working_dir = target_dir\n            await self._update_workdir_for_tools(str(target_dir))\n            logger.info(f\"Working directory updated to: {self.working_dir}\")\n\n            # inform the role everything is ready\n            return content\n        else:\n            return \"No relevant template found\"\n\n    async def draft_plan(self, content: str) -> str:\n        \"\"\"\n        Present a development plan to human for confirmation before starting development.\n\n        Args:\n            content: A string containing numbered plan items (e.g., \"1. Task A\\n2. Task B\\n3. Task C\")\n\n        Returns:\n            str: The human's response/confirmed plan, or error message if format is incorrect\n        \"\"\"\n\n        # Extract lines without numbers for task list\n        lines_without_number = re.findall(r\"^\\d+\\.\\s+(.+)$\", content, re.MULTILINE)\n        user_response = await get_human_input(prompt=content, tasks=lines_without_number)\n        result = parse_plan_confirmation(user_response, role=\"FrontendEngineer\")\n        return result\n",
                        "tags": []
                    },
                    "ImageCreator": {
                        "name": "ImageCreator",
                        "path": "metagpt/tools/image_creator.py",
                        "schemas": {
                            "type": "class",
                            "description": "Tools for creating visual content using Gemini 2.5 Flash Image Preview",
                            "methods": {
                                "generate_image": {
                                    "type": "async_function",
                                    "description": "Generate an image and save it to the assets directory. Designed to be called as a tool by LLM or internally. ",
                                    "signature": "(self, description: str, filename: str, style: str = 'photorealistic') -> dict",
                                    "parameters": "Args: description: Detailed description of the image to generate filename: Target filename (e.g., \"hero-image.jpg\", \"logo.png\") style: Image style (photorealistic, cartoon, sketch, watercolor, minimalist, 3d) Returns: dict: Result with status, path, and message Raises: RuntimeError: If working_dir not set or image generation fails"
                                }
                            },
                            "tool_path": "metagpt/tools/image_creator.py"
                        },
                        "code": "@register_tool(include_functions=[\"generate_image\"])\nclass ImageCreator(BaseModel):\n    \"\"\"Tools for creating visual content using Gemini 2.5 Flash Image Preview\"\"\"\n\n    llm: LLM = Field(default_factory=LLM, exclude=True)\n    api_key: str = Field(default_factory=lambda: Config.default().llm.api_key)\n    base_url: str = Field(default_factory=lambda: Config.default().llm.base_url)\n\n    model: str = \"gemini-2.5-flash-image-preview\"\n\n    # Image duplicate checking attributes\n    image_patterns: ClassVar[list[re.Pattern]] = IMAGE_PATTERNS\n    assets_prefixes: ClassVar[FrozenSet[str]] = ASSETS_PREFIXES\n\n    # Instance attributes for tracking\n    global_image_usage: dict = Field(default_factory=dict, exclude=True)\n    variant_counters: dict = Field(default_factory=lambda: defaultdict(int), exclude=True)\n\n    # working directory\n    working_dir: Optional[str] = Field(default=None, exclude=True)\n\n    async def generate_image(self, description: str, filename: str, style: str = \"photorealistic\") -> dict:\n        \"\"\"\n        Generate an image and save it to the assets directory.\n        Designed to be called as a tool by LLM or internally.\n\n        Args:\n            description: Detailed description of the image to generate\n            filename: Target filename (e.g., \"hero-image.jpg\", \"logo.png\")\n            style: Image style (photorealistic, cartoon, sketch, watercolor, minimalist, 3d)\n\n        Returns:\n            dict: Result with status, path, and message\n\n        Raises:\n            RuntimeError: If working_dir not set or image generation fails\n        \"\"\"\n        if not self.working_dir:\n            raise RuntimeError(\"working_dir not set for ImageCreator\")\n\n        try:\n            enhanced_prompt = self._build_enhanced_prompt(description, style)\n            logger.info(f\"Start creating the image. The prompt is: {enhanced_prompt}...\")\n\n            image_bytes = await self._generate_via_oneapi(enhanced_prompt, size=\"1024x1024\")\n\n            assets_dir = Path(self.working_dir) / \"public\" / \"assets\"\n            assets_dir.mkdir(parents=True, exist_ok=True)\n            image_path = assets_dir / filename\n\n            await self._save_image(image_bytes, str(image_path))\n\n            logger.info(f\"Image created: {image_path}\")\n\n            return {\n                \"status\": \"success\",\n                \"path\": f\"public/assets/{filename}\",\n                \"absolute_path\": str(image_path),\n                \"message\": f\"Image generated and saved to public/assets/{filename}\",\n                \"description\": description,\n                \"style\": style,\n            }\n\n        except Exception as e:\n            error_msg = f\"Image creation failed: {str(e)}\"\n            logger.error(error_msg)\n            raise RuntimeError(error_msg) from e\n\n    def _build_enhanced_prompt(self, description: str, style: Optional[str]) -> str:\n        \"\"\"Building enhanced prompt\"\"\"\n        enhanced = description\n\n        if style:\n            style_guides = {\n                \"photorealistic\": \"photorealistic, high detail, professional photography\",\n                \"cartoon\": \"cartoon style, colorful, animated\",\n                \"sketch\": \"pencil sketch, black and white, artistic\",\n                \"watercolor\": \"watercolor painting, soft edges, artistic\",\n                \"minimalist\": \"minimalist design, clean, simple\",\n                \"3d\": \"3D render, blender, octane render\",\n            }\n            if style in style_guides:\n                enhanced += f\", {style_guides[style]}\"\n\n        return f\"Generate an image: {enhanced}\"\n\n    async def _generate_via_oneapi(self, prompt: str, size: str = \"1024x1024\") -> bytes:\n        \"\"\"Generate images by calling Gemini through oneapi\n\n        Args:\n            prompt: Image generation prompt\n            size: Image size\n\n        Returns:\n            bytes: Generated image byte data\n        \"\"\"\n        url = f\"{self.base_url}/images/generations\"\n\n        payload = {\n            \"model\": self.model,\n            \"prompt\": prompt,\n            \"n\": 1,\n            \"size\": size,\n            \"response_format\": \"b64_json\",\n        }\n\n        data = await apost(\n            url=url,\n            json=payload,\n            headers={\n                \"Authorization\": f\"Bearer {self.api_key}\",\n                \"Content-Type\": \"application/json\",\n            },\n            timeout=300,\n            as_json=True,\n            raise_for_status=True,\n        )\n\n        return self._extract_image_from_response(data)\n\n    def _extract_image_from_response(self, data: dict) -> bytes:\n        \"\"\"Extract image data (OpenAI format) from oneapi response\n\n        Expected response format:\n        {\n            \"created\": 1234567890,\n            \"data\": [\n                {\n                    \"b64_json\": \"base64_encoded_image_data\",\n                    \"revised_prompt\": \"...\"\n                }\n            ]\n        }\n        \"\"\"\n        data_list = data.get(\"data\", [])\n        if not data_list:\n            raise RuntimeError(\"OneAPI returned empty result\")\n\n        first_item = data_list[0]\n        b64_data = first_item.get(\"b64_json\")\n\n        if not b64_data:\n            raise RuntimeError(\"b64_json data not found in OneAPI response\")\n\n        return base64.b64decode(b64_data)\n\n    async def _save_image(self, image_bytes: bytes, save_path: str) -> str:\n        \"\"\"Save image to file\"\"\"\n        save_path = Path(save_path)\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n\n        img = Image.open(BytesIO(image_bytes))\n\n        if save_path.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\"]:\n            save_path = save_path.with_suffix(\".jpg\")\n\n        img.save(str(save_path), quality=95)\n        return str(save_path)\n\n    # ------------------------------------------------------------------\n    # Section: Image Duplicate Check and Replacement Logic\n    # ------------------------------------------------------------------\n\n    async def check_and_fix_duplicate_images(self, content: str, path: Optional[str] = None) -> str:\n        \"\"\"Check for duplicate images globally and fix them by generating new images.\n\n        Args:\n            content: The content to check for duplicate images\n            path: Optional file path for tracking usage\n\n        Returns:\n            str: Content with duplicate images replaced\n        \"\"\"\n        current_image_references = extract_image_references(content, self.image_patterns, self.assets_prefixes)\n        if not current_image_references:\n            return content\n\n        global_duplicates = find_image_duplicates(current_image_references, self.global_image_usage, path)\n\n        if not global_duplicates:\n            update_usage(self.global_image_usage, current_image_references, path)\n            logger.info(f\"No duplicate images found globally in {path}\")\n            return content\n\n        logger.warning(f\"Found duplicate images in {path}. Fixing...\")\n\n        content = await self._fix_global_duplicates(content, global_duplicates)\n\n        final_image_references = extract_image_references(content, self.image_patterns, self.assets_prefixes)\n        update_usage(self.global_image_usage, final_image_references, path)\n\n        return content\n\n    async def _schedule_replacement(self, ref: dict, duplicate_path: str, content: str) -> Optional[dict]:\n        \"\"\"Plan a replacement for a duplicate image using contextual generation.\"\"\"\n        context = slice_around_match(content, ref[\"match\"], radius=60)\n\n        new_image_path = await self._generate_replacement_image(original_path=duplicate_path, context=context)\n\n        if new_image_path:\n            old_reference = ref[\"full_match\"]\n            new_reference = old_reference.replace(ref[\"path\"], new_image_path)\n            return {\"match\": ref[\"match\"], \"new_text\": new_reference, \"new_path\": new_image_path}\n\n        return None\n\n    async def _fix_global_duplicates(self, content: str, global_duplicates: dict) -> str:\n        \"\"\"\n        Fix global duplicate images by generating replacement items and using reverse substitution.\n        \"\"\"\n        replacements = []\n        for duplicate_path, refs in global_duplicates.items():\n            logger.info(f\"Preparing to fix duplicate image: {duplicate_path} (found {len(refs)} references)\")\n\n            is_local_only = duplicate_path not in self.global_image_usage\n\n            if is_local_only and refs:\n                logger.info(\n                    f\"Local duplicate only (not used in other files): keeping first occurrence, replacing {len(refs) - 1} others\"\n                )\n                refs_to_process = refs[1:]\n            else:\n                logger.info(f\"Global duplicate (already used elsewhere): replacing all {len(refs)} occurrences\")\n                refs_to_process = refs\n\n            replacement_count = 0\n            for ref in refs_to_process:\n                replacement_task = await self._schedule_replacement(ref, duplicate_path, content)\n                if replacement_task:\n                    replacements.append(replacement_task)\n                    replacement_count += 1\n                    logger.info(\n                        f\"Schedule Replacement {replacement_count}/{len(refs_to_process)}: {replacement_task['new_path']}\"\n                    )\n\n        replacements.sort(key=lambda x: x[\"match\"].start(), reverse=True)\n\n        if replacements:\n            logger.info(f\"Executing {len(replacements)} image replacements in reverse order.\")\n            for rep in replacements:\n                match, new_text = rep[\"match\"], rep[\"new_text\"]\n                content = content[: match.start()] + new_text + content[match.end() :]\n\n        return content\n\n    async def _generate_replacement_image(self, original_path: str, context: str) -> Optional[str]:\n        \"\"\"\n        Generate a replacement image based on the original path and context.\n        Returns the new image path in /assets/xxx format.\n        \"\"\"\n        path = Path(original_path)\n        name, ext = path.stem, path.suffix\n\n        # Generate variant file names\n        self.variant_counters[name] += 1\n        new_filename = f\"{name}_variant_{self.variant_counters[name]}{ext}\"\n\n        # Generate image description\n        description = await self._generate_image_description(context, original_path)\n\n        # Reuse the generate_image method\n        try:\n            result = await self.generate_image(description=description, filename=new_filename, style=\"photorealistic\")\n            return f\"/assets/{new_filename}\" if result[\"status\"] == \"success\" else None\n        except Exception as e:\n            logger.error(f\"Failed to generate replacement image: {e}\")\n            return None\n\n    async def _generate_image_description(self, context: str, original_path: str) -> str:\n        \"\"\"Generate image description based on context using LLM.\"\"\"\n        path = Path(original_path)\n        filename = path.name\n\n        if not self.llm:\n            name = path.stem\n            logger.warning(\"No LLM available, using filename-based description\")\n            return f\"A {name.replace('-', ' ').replace('_', ' ')} image with photorealistic style\"\n\n        clean_context = \" \".join(context.split())[:500] if len(context) > 500 else context\n\n        prompt = f\"\"\"Based on the following context and original image filename, generate a detailed description for a new similar image.\n\n        Context: {clean_context}\n        Original image: {filename}\n\n        **IMPORTANT**: Generate a description ONLY for the image named \"{filename}\". \n        If the context mentions other image filenames or paths, IGNORE them completely - they are NOT the target image.\n\n        Generate a detailed, descriptive prompt for creating a similar but unique image. Focus on:\n        - Visual elements and composition\n        - Colors and style\n        - Subject matter\n        - Mood or atmosphere\n\n        Make it different enough to be unique but similar in theme and purpose.\n        Keep the description concise but descriptive (under 50 words).\"\"\"\n\n        description = await self.llm.aask(prompt)\n        return description.strip()\n",
                        "tags": []
                    },
                    "CheckUI": {
                        "name": "CheckUI",
                        "path": "metagpt/tools/check_ui_rendering.py",
                        "schemas": {
                            "type": "class",
                            "description": "Check UI rendering quality through screenshot analysis and AI evaluation.",
                            "methods": {
                                "run": {
                                    "type": "async_function",
                                    "description": "Check whether the current UI rendering matches the instruction after you have finished the devlopment and started the development server. Example instruction: - A 2048 game. - A KOL live streaming data dashboard web page.",
                                    "signature": "(self, instruction: str) -> str",
                                    "parameters": ""
                                }
                            },
                            "tool_path": "metagpt/tools/check_ui_rendering.py"
                        },
                        "code": "@register_tool(tags=[\"Check UI rendering\"], include_functions=[\"run\"])\nclass CheckUI(BaseModel):\n    \"\"\"Check UI rendering quality through screenshot analysis and AI evaluation.\"\"\"\n\n    img_llm: LLM = Field(default_factory=LLM, exclude=True)\n    previewer: Previewer = Field(default_factory=Previewer)\n\n    async def _recognize_screenshot(self, app_img_base64: str, instruction: str) -> tuple:\n        \"\"\"Analyze screenshot using AI to evaluate UI quality against instruction.\"\"\"\n        # Recognize screenshot image\n        self.img_llm.model = \"gpt-4o-mini\"\n        self.img_llm.max_completion_token = DEFAULT_MAX_COMPLETION_TOKENS\n        check_report = await self.img_llm.aask(\n            msg=CHECK_APP_STATE.format(instruction=instruction), images=[app_img_base64]\n        )\n\n        # parse app state information\n        app_grade = re.search(r\"<Render Grade>([\\s\\S]*?)<\\/Render Grade>\", check_report)\n\n        if not app_grade:\n            return None, None\n\n        grade = int(app_grade.group(1).strip())\n\n        await ThoughtReporter().async_report({\"type\": \"render_grade\", \"grade\": grade, \"instruction\": instruction})\n\n        return check_report, grade\n\n    async def run(self, instruction: str) -> str:\n        \"\"\"\n        Check whether the current UI rendering matches the instruction after you have finished the devlopment and started the development server.\n\n        Example instruction:\n        - A 2048 game.\n        - A KOL live streaming data dashboard web page.\n        \"\"\"\n\n        passing_grade: float = 2\n        skip_current_stage_msg = \"Errors occurred during checking, end your development\"\n        # Get preview url and screenshot\n        preview_info: dict = await self.previewer.get_preview_info()\n        if not preview_info.get(\"screenshot_path\") or not Path(preview_info.get(\"screenshot_path\")).exists():\n            logger.error(\"Not found Screenshot image.\")\n            return f\"Not found Screenshot image. {skip_current_stage_msg}\"\n        logger.info(f\"screenshot_path: {preview_info.get('screenshot_path')}\")\n        app_img_base64 = encode_image(preview_info.get(\"screenshot_path\"))\n        if not app_img_base64:\n            logger.error(\"Encode screenshot failed.\")\n            return f\"Encode screenshot failed. {skip_current_stage_msg}\"\n\n        # Recognize screenshot image\n        try:\n            check_report, grade = await self._recognize_screenshot(app_img_base64, instruction)\n        except Exception as e:\n            logger.error(f\"Recognize screenshot failed. error: {e}\")\n            return f\"Recognize screenshot failed. {skip_current_stage_msg}\"\n\n        # img_llm fail: end the role.\n        if grade is None:\n            logger.error(\"The image model failed in the task of recognizing the screenshot image.\")\n            return f\"The image model failed in the task of recognizing the screenshot image. {skip_current_stage_msg}\"\n\n        # The goal was achieved: End the role.\n        if grade >= passing_grade:\n            logger.info(f\"The Render Grade >= {passing_grade}.\")\n            return f\"The Render Grade check report: {check_report}. The check has been passed, no further action is required, no need to ask the user.\"\n\n        return f\"{check_report}, Use tool `RoleZero.ask_human` to ask the user if fix is needed: 1) It's working fine. 2) Ask @Alex to fix the issue.\"\n",
                        "tags": [
                            "Check UI rendering"
                        ]
                    },
                    "ImageGetter": {
                        "name": "ImageGetter",
                        "path": "metagpt/tools/libs/image_getter.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool to get/create/process images.",
                            "methods": {
                                "get": {
                                    "type": "async_function",
                                    "description": "Get an image either by searching online or generating with AI. ",
                                    "signature": "(self, search_term: 'str', image_save_path: 'str', mode='search') -> 'str'",
                                    "parameters": "Args: search_term (str): Search query or image description. Must be in English. image_save_path (str): Path where the image will be saved. Must be a absolute path to the public folder. mode (str): How to obtain the image: - \"search\": Search online (falls back to AI generation if search fails) - \"create\": Generate using DALL-E 3 Returns: str: Relative path to the saved image (public/ prefix removed) Raises: ValueError: If an invalid mode is specified RuntimeError: If image retrieval fails"
                                },
                                "process": {
                                    "type": "async_function",
                                    "description": "Process an existing image with filters. ",
                                    "signature": "(self, image_path: 'str', image_save_path: 'str', mode: 'str' = 'rembg') -> 'str'",
                                    "parameters": "Args: image_path (str): Path to the source image image_save_path (str): Path where processed image will be saved. Must be a absolute path to the public folder. mode (str): Processing mode to apply: - \"rembg\": Remove image background Returns: str: Relative path to the processed image (public/ prefix removed)"
                                },
                                "validate_and_replace_images": {
                                    "type": "async_function",
                                    "description": "Parse the content and check if it contains invalid images paths/URLs. If it does, replace them with valid images. ",
                                    "signature": "(self, content: 'str', path: 'str' = None) -> 'str'",
                                    "parameters": "Args: content: The content to process path: Optional file path for logging purposes Returns: str: Content with invalid images replaced"
                                }
                            },
                            "tool_path": "metagpt/tools/libs/image_getter.py"
                        },
                        "code": "@register_tool(include_functions=[\"get\", \"process\", \"validate_and_replace_images\"])\nclass ImageGetter(BaseModel):\n    \"\"\"\n    A tool to get/create/process images.\n    \"\"\"\n\n    search_term_model_name: str = \"gpt-4o-mini\"\n    llm: LLM = Field(default_factory=LLM, exclude=True)\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    image_provider: BaseImageProvider = Field(\n        default=None,\n        exclude=True,\n        description=\"The image getter to use. Choose from Pixabay, Unsplash, or Unsplash API. Defaults to Unsplash API.\",\n    )\n\n    project_dir: Optional[Path] = Field(default=None, exclude=True)\n    # Limit concurrent HTTP probe requests\n    http_head_semaphore: asyncio.Semaphore = Field(default_factory=lambda: asyncio.Semaphore(10), exclude=True)\n\n    @model_validator(mode=\"after\")\n    def set_provider(self) -> \"ImageGetter\":\n        self.llm.model = self.search_term_model_name\n        self.llm.max_completion_token = DEFAULT_MAX_COMPLETION_TOKENS\n        if self.image_provider is None:\n            image_search_config = Config.default().image_search\n            api_type = image_search_config.api_type\n            params = image_search_config.model_dump(exclude=(\"api_type\",), exclude_none=True, exclude_defaults=True)\n            if api_type == \"unsplash\":\n                self.image_provider = UnsplashApi(**params, project_dir=self.project_dir)\n            elif api_type == \"pixabay\":\n                self.image_provider = PixabayAPI(**params, project_dir=self.project_dir)\n            else:\n                raise ValueError(\"No image provider configured. Set image_search.api_type to 'unsplash' or 'pixabay'.\")\n        return self\n\n    def update_project_dir(self, new_project_dir: Path) -> None:\n        \"\"\"Update project_dir for both ImageGetter and its image_provider.\n\n        Args:\n            new_project_dir: The new project directory path to set.\n        \"\"\"\n        self.project_dir = new_project_dir\n        if self.image_provider:\n            self.image_provider.project_dir = new_project_dir\n\n    @classmethod\n    def is_available(cls) -> bool:\n        config = Config.default()\n        api_type = config.image_search.api_type\n        api_key = config.image_search.api_key\n        has_key = False\n        if isinstance(api_key, list):\n            has_key = any(bool(k) for k in api_key)\n        else:\n            has_key = bool(api_key)\n        return (api_type in (\"unsplash\", \"pixabay\")) and has_key\n\n    async def get(self, search_term: str, image_save_path: str, mode=\"search\") -> str:\n        \"\"\"Get an image either by searching online or generating with AI.\n\n        Args:\n            search_term (str): Search query or image description. Must be in English.\n            image_save_path (str): Path where the image will be saved. Must be a absolute path to the public folder.\n            mode (str): How to obtain the image:\n                - \"search\": Search online (falls back to AI generation if search fails)\n                - \"create\": Generate using DALL-E 3\n\n        Returns:\n            str: Relative path to the saved image (public/ prefix removed)\n\n        Raises:\n            ValueError: If an invalid mode is specified\n            RuntimeError: If image retrieval fails\n        \"\"\"\n        return await self.image_provider.get(search_term, image_save_path, mode)\n\n    async def process(self, image_path: str, image_save_path: str, mode: str = \"rembg\") -> str:\n        \"\"\"Process an existing image with filters.\n\n        Args:\n            image_path (str): Path to the source image\n            image_save_path (str): Path where processed image will be saved. Must be a absolute path to the public folder.\n            mode (str): Processing mode to apply:\n                - \"rembg\": Remove image background\n\n        Returns:\n            str: Relative path to the processed image (public/ prefix removed)\n        \"\"\"\n        return await self.image_provider.process(image_path, image_save_path, mode)\n\n    async def get_search_term(self, context: str, img_path: str) -> str:\n        \"\"\"Get the search term from the context. Use llm to infer the search term from the context.\"\"\"\n        prompt = SEARCH_TERM_PROMPT.format(context=context, img_path=img_path)\n        response = await self.llm.aask(prompt)\n        content_match = re.search(r\"<content>(.*?)</content>\", response)\n        if content_match:\n            return content_match.group(1)\n        else:\n            # Fallback if the LLM doesn't follow the format\n            return response.strip()\n\n    async def _invalid_img_replace(self, img_path: str, content: str) -> tuple[str, bool]:\n        \"\"\"\n        Internal method to replace invalid image path with a valid one and return status.\n\n        Args:\n            img_path: the image path string to check and potentially replace\n            content: the full content containing the image path\n            Uses self.project_dir to resolve local files\n\n        Returns:\n            tuple: (valid_image_path, was_invalid)\n                - valid_image_path: the valid image path (original if valid, or new if replaced)\n                - was_invalid: True if the original path was invalid\n        \"\"\"\n        # Image path is already clean (no quotes from regex)\n        clean_img_path = img_path\n\n        # Check if the image URL/path is valid\n        is_valid = False\n        if clean_img_path.startswith(\"http\"):\n            # Use robust HTTP probe (HEAD with retry, fallback to GET on 405)\n            async with self.http_head_semaphore:\n                is_valid = await probe_http_image_is_valid(clean_img_path, timeout_sec=5, max_retries=2)\n        else:\n            # Remove leading slash if it exists to ensure path is treated as relative\n            if self.project_dir is None:\n                raise RuntimeError(\"project_dir must be set before validating local images\")\n\n            rel_path = Path(clean_img_path.lstrip(\"/\"))\n            public_rel = Path(\"public\") / rel_path\n            local_path = self.project_dir / public_rel\n            try:\n                with Image.open(local_path) as img:\n                    img_format = img.format\n                    is_valid = img_format is not None\n            except Exception as e:\n                logger.warning(f\"Invalid image: {public_rel} with error: {e}\")\n                is_valid = False\n\n            # Fallback: if not found/invalid under public path, search by filename under project_dir\n            # If any file with the same basename is a valid image, consider it valid\n            if not is_valid:\n                try:\n                    file_name = public_rel.name\n                    for candidate in self.project_dir.rglob(file_name):\n                        try:\n                            with Image.open(candidate) as img:\n                                if img.format:\n                                    is_valid = True\n                                    logger.info(f\"Found image by filename in project_dir: {candidate} for {public_rel}\")\n                                    break\n                        except Exception:\n                            # Skip unreadable/non-image files with same name\n                            continue\n                except Exception as search_err:\n                    logger.debug(f\"Filename search failed for {public_rel}: {search_err}\")\n\n        # If valid, return original path\n        if is_valid:\n            return img_path, False\n\n        # Use llm to infer the search term from the context\n        context = retrieve_img_context(img_path, content, window_size=200)\n        search_term = await self.get_search_term(context, img_path)\n        img_dir_path = Path(\"public\") / \"images\"\n        if self.project_dir is None:\n            raise RuntimeError(\"project_dir must be set before creating new images\")\n        (self.project_dir / img_dir_path).mkdir(parents=True, exist_ok=True)\n\n        # Clean the search term by removing non-alphanumeric characters\n        filename = re.sub(r\"[^a-zA-Z0-9]\", \"\", search_term)\n        # Avoid filename conflict\n        extension = \".jpg\"\n        proposed = img_dir_path / f\"{filename}{extension}\"\n        if not filename or (self.project_dir / proposed).exists():\n            new_rel_path = img_dir_path / f\"photo{int(time.time())}{extension}\"\n        else:\n            new_rel_path = proposed\n        new_img_path = new_rel_path.as_posix()\n\n        try:\n            relative_path = await self.get(search_term=search_term, image_save_path=new_img_path, mode=\"search\")\n            # Strip public prefix for projects using build tools (Vite, etc.) that map public/ to root\n            # Check if project has a public directory (covers shadcn-ui, presentation, prototype_template, etc.)\n            if self.project_dir and (self.project_dir / \"public\").exists():\n                relative_path = strip_public_prefix(relative_path)\n            return f\"{relative_path}\", True\n        except Exception as e:\n            logger.warning(f\"Failed to get image for {search_term}: {e}\")\n            return img_path, True  # Return original but mark as invalid\n\n    @staticmethod\n    def _is_potential_image(url: str) -> bool:\n        \"\"\"Simplified check: common image extensions and special-case hosts allowing no extension.\"\"\"\n        if not url:\n            return False\n        u = url.strip()\n\n        # Special-case hosts: even without extension are considered images\n        if u.startswith(\"http\") and any(h in u for h in IMAGE_HOSTS_ALLOW_NO_EXT):\n            return True\n\n        # Exclude common non-resource schemes to avoid data:, blob: and JS links\n        if u.startswith(NON_RESOURCE_SCHEMES):\n            return False\n\n        return bool(IMAGE_SUFFIX_PATTERN.search(u))\n\n    async def validate_and_replace_images(self, content: str, path: str = None) -> str:\n        \"\"\"\n        Parse the content and check if it contains invalid images paths/URLs.\n        If it does, replace them with valid images.\n\n        Args:\n            content: The content to process\n            path: Optional file path for logging purposes\n\n        Returns:\n            str: Content with invalid images replaced\n        \"\"\"\n        # Only match \"explicit image references\" in the content to reduce false positives, supporting:\n        #  - Absolute URLs: http(s)://... (even without an extension, extract first, then filter/allow by _is_potential_image)\n        #  - Relative paths: paths starting with /, ./, or ../\n        #  - Bare filenames/paths: must contain an image extension (see IMAGE_EXT_RE), may include query strings/fragments (? or #)\n        #  - Non-resource schemes (data:, blob:, javascript:, mailto:) are excluded by _is_potential_image\n        #  - Special-case hosts (see IMAGE_HOSTS_ALLOW_NO_EXT) links without extensions are allowed by _is_potential_image\n        urls = [\n            m.group(\"url\")\n            for m in CANDIDATE_IMAGE_URL_PATTERN.finditer(content)\n            if self._is_potential_image(m.group(\"url\"))\n        ]\n\n        if not urls:\n            logger.info(f\"No image contexts found in {path or 'content'}\")\n            return content\n\n        # Deduplicate and validate/propose replacements concurrently\n        unique_urls: list[str] = list(dict.fromkeys(urls))\n        url_to_result: dict[str, tuple[str, bool]] = dict(\n            zip(\n                unique_urls,\n                await asyncio.gather(*[self._invalid_img_replace(u, content) for u in unique_urls]),\n            )\n        )\n\n        # Use regex substitution with a callback for safe, position-independent replacements\n        def _sub_replacer(m: re.Match) -> str:\n            old_url = m.group(\"url\")\n            new_url, was_invalid = url_to_result.get(old_url, (old_url, False))\n            return new_url if was_invalid and new_url != old_url else m.group(0)\n\n        new_content = CANDIDATE_IMAGE_URL_PATTERN.sub(_sub_replacer, content)\n\n        # Compute counts based on original URL occurrences\n        num_invalid_images = sum(1 for u in urls if url_to_result.get(u, (u, False))[1])\n        num_converted_images = sum(\n            1 for u in urls if url_to_result.get(u, (u, False))[1] and url_to_result.get(u, (u, False))[0] != u\n        )\n\n        logger.info(\n            f\"Processed {len(urls)} image refs. Detected {num_invalid_images} invalid refs. Fixed {num_converted_images} refs.\"\n        )\n        return new_content\n",
                        "tags": []
                    }
                },
                "force": true
            },
            "special_tool_commands": [
                "Plan.finish_current_task",
                "end",
                "Terminal.run",
                "RoleZero.ask_human"
            ],
            "editor": {
                "resource": {
                    "block": "Editor",
                    "uuid": "97aad5e8-9fa3-46bd-b6f2-ccd0778a4e14",
                    "enable_llm_stream": false,
                    "callback_url": ""
                },
                "current_file": null,
                "current_line": 1,
                "window": 50,
                "enable_auto_lint": true,
                "working_dir": "/workspace/ttml_app/main-main"
            },
            "browser": {
                "rp_meta": {
                    "uuid": "e22aeb25db1f4a90baacf1e720849c77",
                    "storage_dir": "/data/chats/28286616f10743a6baaf2bd913b64f39/workspace/.MGXTools",
                    "user_id": 644050,
                    "chat_id": "28286616f10743a6baaf2bd913b64f39",
                    "role_name": "Alex"
                }
            },
            "terminal": {
                "rp_meta": {
                    "uuid": "86ba52b244704943a6d1f8e2f7952aa4",
                    "storage_dir": "/data/chats/28286616f10743a6baaf2bd913b64f39/workspace/.MGXTools",
                    "user_id": 644050,
                    "chat_id": "28286616f10743a6baaf2bd913b64f39",
                    "role_name": "Alex"
                }
            },
            "working_dir": "/workspace/ttml_app/main-main",
            "command_rsp": "",
            "commands": [],
            "memory_k": 30,
            "use_fixed_sop": false,
            "use_summary": true,
            "browser_is_active": false,
            "supabase_manager": {
                "working_dir": "/workspace/ttml_app/main-main",
                "function_slugs": [],
                "reported_env_vars": {}
            },
            "run_eval": false,
            "output_diff": "",
            "code_tools": [],
            "code_tool_execution_list": [],
            "code_tool_recommender": null,
            "previewer": {
                "has_previewed": false,
                "preview_url": "",
                "project_path": "",
                "use_dev": true
            },
            "warning_prompt": "",
            "use_search_template": true,
            "is_first_dev_request": true,
            "template_tool": {
                "template_path": "/mgx_template/templates",
                "output_dir": "/workspace"
            },
            "template_info": "No template available",
            "__module_class_name": "metagpt.roles.di.frontend_engineer.FrontendEngineer"
        },
        "David": {
            "name": "David",
            "profile": "Data Analyst",
            "goal": "Take on any data-related tasks, such as machine learning, deep learning, web browsing, web scraping, web searching, terminal operation, etc.",
            "constraints": "",
            "desc": "",
            "is_human": false,
            "enable_memory": true,
            "role_id": "",
            "states": [
                "0. RunCommand"
            ],
            "actions": [
                {
                    "name": "RunCommand",
                    "i_context": "",
                    "prefix": "You are a Data Analyst, named David, your goal is Take on any data-related tasks, such as machine learning, deep learning, web browsing, web scraping, web searching, terminal operation, etc.. ",
                    "desc": "",
                    "__module_class_name": "metagpt.actions.di.run_command.RunCommand"
                }
            ],
            "rc": {
                "memory": {
                    "storage": [
                        {
                            "id": "35cca740059444188b1a9c2c91664ce9",
                            "timestamp": "20251126060539318",
                            "content": "# Project Summary\nThe \"Talk to My Lawyer\" project is a SaaS application designed to facilitate legal document generation and review. It supports three user roles: Subscribers who generate letters, Employees who assist with sales and earn commissions through coupon codes, and Admins who review and approve the generated letters. The platform leverages AI for drafting legal documents and integrates with payment processors for subscriptions.\n\n# Project Module Description\nThe application comprises several functional modules:\n- **User Authentication**: Manages user roles and sessions.\n- **Letter Generation**: Allows subscribers to create legal letters using AI.\n- **Subscription Management**: Handles user subscriptions and coupon codes.\n- **Admin Review**: Provides a dashboard for admins to review, edit, and approve letters.\n- **Email Notifications**: Sends emails to users regarding their letters and subscriptions.\n- **PDF Generation**: Converts approved letters into downloadable PDF format.\n\n# Directory Tree\n```\nttml_app/\n\u251c\u2500\u2500 main-main/\n\u2502   \u251c\u2500\u2500 app/\n\u2502   \u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 dashboard/\n\u2502   \u2502   \u251c\u2500\u2500 globals.css\n\u2502   \u2502   \u2514\u2500\u2500 layout.tsx\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u251c\u2500\u2500 supabase/\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 styles/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 package.json\n```\n\n# File Description Inventory\n- `app/api/`: Contains API routes for handling requests related to user authentication, letter generation, and subscriptions.\n- `app/dashboard/`: Contains components for the admin and user dashboards, including letter management and analytics.\n- `components/`: Reusable UI components used throughout the application.\n- `supabase/`: Contains database migrations and functions related to the Supabase backend.\n- `scripts/`: Includes scripts for database setup and administration tasks.\n- `styles/`: Contains global styles for the application.\n- `README.md`: Documentation for the project setup and usage.\n\n# Technology Stack\n- **Frontend**: Next.js 14, TypeScript, Tailwind CSS\n- **Backend**: Supabase, OpenAI GPT-4\n- **Payment Processing**: Stripe\n- **Email Service**: Resend\n\n# Usage\n1. **Install Dependencies**: Run the command to install required packages.\n2. **Build the Application**: Use the build command to compile the application.\n3. **Run the Application**: Start the application server for local development.\n\n\n# INSTRUCTION\n- You can search for the file path in the `/workspace/`\n- After modifying the project files, if this project can be previewed, then you need to reinstall dependencies, restart service and preview;\n\n\nChange the working directory to `/workspace/ttml_app/main-main`.",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "df14fbea5cb742d489435448bacf7ffe",
                            "timestamp": "20251126061240991",
                            "content": "[Message] from User to Mike: there are 0 tables ",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>",
                                "Mike"
                            ],
                            "metadata": {}
                        }
                    ],
                    "index": {
                        "metagpt.actions.add_requirement.UserRequirement": [
                            {
                                "id": "df14fbea5cb742d489435448bacf7ffe",
                                "timestamp": "20251126061240991",
                                "content": "[Message] from User to Mike: there are 0 tables ",
                                "instruct_content": null,
                                "role": "user",
                                "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                                "sent_from": "",
                                "send_to": [
                                    "<all>",
                                    "Mike"
                                ],
                                "metadata": {}
                            }
                        ]
                    },
                    "ignore_id": false,
                    "appendix": null,
                    "buffer": [
                        {
                            "id": "35cca740059444188b1a9c2c91664ce9",
                            "timestamp": "20251126060539318",
                            "content": "# Project Summary\nThe \"Talk to My Lawyer\" project is a SaaS application designed to facilitate legal document generation and review. It supports three user roles: Subscribers who generate letters, Employees who assist with sales and earn commissions through coupon codes, and Admins who review and approve the generated letters. The platform leverages AI for drafting legal documents and integrates with payment processors for subscriptions.\n\n# Project Module Description\nThe application comprises several functional modules:\n- **User Authentication**: Manages user roles and sessions.\n- **Letter Generation**: Allows subscribers to create legal letters using AI.\n- **Subscription Management**: Handles user subscriptions and coupon codes.\n- **Admin Review**: Provides a dashboard for admins to review, edit, and approve letters.\n- **Email Notifications**: Sends emails to users regarding their letters and subscriptions.\n- **PDF Generation**: Converts approved letters into downloadable PDF format.\n\n# Directory Tree\n```\nttml_app/\n\u251c\u2500\u2500 main-main/\n\u2502   \u251c\u2500\u2500 app/\n\u2502   \u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 dashboard/\n\u2502   \u2502   \u251c\u2500\u2500 globals.css\n\u2502   \u2502   \u2514\u2500\u2500 layout.tsx\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u251c\u2500\u2500 supabase/\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 styles/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 package.json\n```\n\n# File Description Inventory\n- `app/api/`: Contains API routes for handling requests related to user authentication, letter generation, and subscriptions.\n- `app/dashboard/`: Contains components for the admin and user dashboards, including letter management and analytics.\n- `components/`: Reusable UI components used throughout the application.\n- `supabase/`: Contains database migrations and functions related to the Supabase backend.\n- `scripts/`: Includes scripts for database setup and administration tasks.\n- `styles/`: Contains global styles for the application.\n- `README.md`: Documentation for the project setup and usage.\n\n# Technology Stack\n- **Frontend**: Next.js 14, TypeScript, Tailwind CSS\n- **Backend**: Supabase, OpenAI GPT-4\n- **Payment Processing**: Stripe\n- **Email Service**: Resend\n\n# Usage\n1. **Install Dependencies**: Run the command to install required packages.\n2. **Build the Application**: Use the build command to compile the application.\n3. **Run the Application**: Start the application server for local development.\n\n\n# INSTRUCTION\n- You can search for the file path in the `/workspace/`\n- After modifying the project files, if this project can be previewed, then you need to reinstall dependencies, restart service and preview;\n\n\nChange the working directory to `/workspace/ttml_app/main-main`.",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>"
                            ],
                            "metadata": {}
                        },
                        {
                            "id": "df14fbea5cb742d489435448bacf7ffe",
                            "timestamp": "20251126061240991",
                            "content": "[Message] from User to Mike: there are 0 tables ",
                            "instruct_content": null,
                            "role": "user",
                            "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                            "sent_from": "",
                            "send_to": [
                                "<all>",
                                "Mike"
                            ],
                            "metadata": {}
                        }
                    ],
                    "memory_k": 30,
                    "compress_k": 15,
                    "prev_summary": null,
                    "compress_model_name": "gpt-4o-mini",
                    "is_xml_format": null,
                    "compress_trigger_token_threshold": 0
                },
                "working_memory": {
                    "storage": [],
                    "index": {},
                    "ignore_id": false,
                    "appendix": null
                },
                "state": -1,
                "watch": [],
                "react_mode": "react",
                "max_react_loop": 50
            },
            "addresses": [
                "metagpt.roles.di.data_analyst.DataAnalyst",
                "David"
            ],
            "planner": {
                "plan": {
                    "goal": "",
                    "context": "",
                    "tasks": [],
                    "task_map": {},
                    "current_task_id": ""
                },
                "working_memory": {
                    "storage": [],
                    "index": {},
                    "ignore_id": false,
                    "appendix": null
                },
                "auto_run": true
            },
            "recovered": false,
            "latest_observed_msg": null,
            "observe_all_msg_from_buffer": true,
            "system_msg": null,
            "system_prompt": "\n<Basic Info>\n{role_info}\n</Basic Info>\n\n<Domain Info>\n{domain_info}\n</Domain Info>\n\n<Data Structure>\nclass Task(BaseModel):\n    task_id: str = \"\"\n    dependent_task_ids: list[str] = []\n    instruction: str = \"\"\n    task_type: str = \"\"\n    assignee: str = \"\"\n</Data Structure>\n\n<Available Task Types>\n{task_type_desc}\n</Available Task Types>\n\n<Available Commands>\n{available_commands}\nSpecial Command: Use <end></end> to indicate completion of all requirements and termination of the entire workflow.\n</Available Commands>\n\n<Example>\n{example}\n</Example>\n\n<Instruction>\n{instruction}\n\nYou may use any of the available commands. You may output multiple commands, they will be executed sequentially.\nIf the task requires writing multiple files, you should output multiple commands to write files instead of writing one by one.\nIf you finish current task, you will automatically take the next task in the existing plan, use Plan.finish_current_task, DON'T append a new task.\nReview the latest plan's outcome. If your completed task matches the current, consider it finished.\nIn your response, include at least one command. Only use <end></end> when all requirements are met, and please use RoleZero.reply_to_human immediately before <end></end> to report completion.\nDo NOT use <end></end> when waiting for user input or clarification.\n</Instruction>\n\n<Output>\nYou should output a list of commands, follow the format below, replace the <ClassName.method_name> and <args_name> with the actual name.\n\nSome thoughts...\n<ClassName.method_name>\n<args_name1>\nargs_value1\n</args_name1>\n<args_name2>\nargs_value2\n</args_name2>\n</ClassName.method_name>\n\nSome thoughts...\n<ClassName2.method_name2>\n<args_name1>\nargs_value1\n</args_name1>\n<args_name2>\nargs_value2\n</args_name2>\n</ClassName2.method_name2>\n</Output>\n",
            "cmd_prompt": "\n<Current State>\n{current_state}\n</Current State>\n\n<Current Plan>\n{plan_status}\n</Current Plan>\n\n<Current Task>\n{current_task}\n</Current Task>\n\nYour commands (output ONE and ONLY ONE command block, the block can contain one or more commands. Use <end></end> when all requirements are met):\n",
            "instruction": "\nBased on the context, write a plan or modify an existing plan to achieve the goal. A plan consists of one to 3 tasks.\nIf plan is created, you should track the progress and update the plan accordingly, such as Plan.finish_current_task, Plan.append_task, Plan.reset_task, Plan.replace_task, etc.\nWhen presented a current task, tackle the task using the available commands.\nPay close attention to new user message, review the conversation history, use RoleZero.reply_to_human to respond to new user requirement.\nNote:\n1. If you keeping encountering errors, unexpected situation, or you are not sure of proceeding, use RoleZero.ask_human to ask for help.\n2. Carefully review your progress at the current task, if your actions so far has not fulfilled the task instruction, you should continue with current task. Otherwise, finish current task by Plan.finish_current_task explicitly.\n3. Each time you finish a task, use RoleZero.reply_to_human to report your progress.\n4. Don't forget to append task first when all existing tasks are finished and new tasks are required.\n5. Avoid repeating tasks you have already completed. And end loop when all requirements are met.\n\n6. Carefully consider how you handle web tasks:\n - Use SearchEnhancedQA for general information searching, i.e. querying search engines, such as googling news, weather, wiki, etc. Usually, no link is provided.\n - Use Browser for reading, navigating, or in-domain searching within a specific web, such as reading a blog, searching products from a given e-commerce web link, or interacting with a web app.\n - Use DataAnalyst.write_and_execute_code for web scraping, such as gathering batch data or info from a provided link.\n - Write code to view the HTML content rather than using the Browser tool.\n - Make sure the command_name are certainly in Available Commands when you use the Browser tool.\n7. When you are making plan. It is highly recommend to plan and append all the tasks in first response once time, except for 7.1.\n7.1. When the requirement is inquiring about a pdf, docx, md, or txt document, read the document first through either Editor.read WITHOUT a plan. After reading the document, use RoleZero.reply_to_human if the requirement can be answered straightaway, otherwise, make a plan if further calculation is needed.\n8. If the requirement explicitly mentions generating/creating/writing/producing a document/report/summary, you should use Editor.write to output the document.\n9. Don't finish_current_task multiple times for the same task.\n10. Finish current task timely, such as when the code is written and executed successfully.\n11. When using the command 'end', add the command 'finish_current_task' before it.\n12. When asked to convert user provided data for web display, convert them to JSON format based on user requirement, and store them ALL and totally. DO NOT use ellipsis (...) or only part of the data.\n13. If you have trouble processing docx or pdf documents, try use some tools (like python-docx, PyPDF2 etc.). Never generate data by yourself. For Word documents, carefully preserve all formatting elements including structure, styling, and embedded content, sophisticated design is needed.\n",
            "task_type_desc": "- **eda**: For performing exploratory data analysis\n- **data preprocessing**: For preprocessing dataset in a data analysis or machine learning task ONLY,general data operation doesn't fall into this type\n- **feature engineering**: Only for creating new columns for input data.\n- **model train**: Only for training model.\n- **model evaluate**: Only for evaluating model.\n- **image2webpage**: For converting image into webpage code.\n- **other**: Any tasks not in the defined categories\n- **text2image**: Related to text2image, image2image using stable diffusion model.\n- **web scraping**: For scraping data from web pages.\n- **email login**: For logging to an email.\n- **develop software**: SOP related to develop software such as Writes a PRD, Writes a design, Writes a project plan and Writes code to implement designed features according to the project plan",
            "example": "",
            "react_mode": "react",
            "max_react_loop": 50,
            "consecutive_react_cnt": 0,
            "max_consecutive_react_limit": 20,
            "tools": [
                "Plan",
                "DataAnalyst",
                "RoleZero",
                "Browser",
                "Editor:write,read,similarity_search",
                "Terminal:run",
                "SearchEnhancedQA"
            ],
            "mcp_tools": [],
            "tool_recommender": {
                "tools": {
                    "Plan": {
                        "name": "Plan",
                        "path": "metagpt/schema.py",
                        "schemas": {
                            "type": "class",
                            "description": "Plan is a sequence of tasks towards a goal.",
                            "methods": {
                                "append_task": {
                                    "type": "function",
                                    "description": "Append a new task with task_id (number) to the end of existing task sequences. If dependent_task_ids is not empty, the task will depend on the tasks with the ids in the list. Note that the assignee should be the 'name' of the role.",
                                    "signature": "(self, task_id: 'str', dependent_task_ids: 'list[str]', instruction: 'str', assignee: 'str', task_type: 'str' = '')",
                                    "parameters": ""
                                },
                                "finish_current_task": {
                                    "type": "function",
                                    "description": "Finish current task, set Task.is_finished=True, set current task to next task",
                                    "signature": "(self)",
                                    "parameters": ""
                                },
                                "replace_task": {
                                    "type": "function",
                                    "description": "Replace an existing task (can be current task) based on task_id, and reset all tasks depending on it.",
                                    "signature": "(self, task_id: 'str', new_dependent_task_ids: 'list[str]', new_instruction: 'str', new_assignee: 'str')",
                                    "parameters": ""
                                },
                                "reset_task": {
                                    "type": "function",
                                    "description": "Reset a task based on task_id, i.e. set Task.is_finished=False and request redo. This also resets all tasks depending on it. ",
                                    "signature": "(self, task_id: 'str')",
                                    "parameters": "Args: task_id (str): The ID of the task to be reset."
                                }
                            },
                            "tool_path": "metagpt/schema.py"
                        },
                        "code": "@register_tool(\n    include_functions=[\n        \"append_task\",\n        \"reset_task\",\n        \"replace_task\",\n        \"finish_current_task\",\n    ]\n)\nclass Plan(BaseModel):\n    \"\"\"Plan is a sequence of tasks towards a goal.\"\"\"\n\n    goal: str\n    context: str = \"\"\n    tasks: list[Task] = []\n    task_map: dict[str, Task] = {}\n    current_task_id: str = \"\"\n\n    def _topological_sort(self, tasks: list[Task]):\n        task_map = {task.task_id: task for task in tasks}\n        dependencies = {task.task_id: set(task.dependent_task_ids) for task in tasks}\n        sorted_tasks = []\n        visited = set()\n\n        def visit(task_id):\n            if task_id in visited:\n                return\n            visited.add(task_id)\n            for dependent_id in dependencies.get(task_id, []):\n                visit(dependent_id)\n            sorted_tasks.append(task_map[task_id])\n\n        for task in tasks:\n            visit(task.task_id)\n\n        return sorted_tasks\n\n    def add_tasks(self, tasks: list[Task]):\n        \"\"\"\n        Integrates new tasks into the existing plan, ensuring dependency order is maintained.\n\n        This method performs two primary functions based on the current state of the task list:\n        1. If there are no existing tasks, it topologically sorts the provided tasks to ensure\n        correct execution order based on dependencies, and sets these as the current tasks.\n        2. If there are existing tasks, it merges the new tasks with the existing ones. It maintains\n        any common prefix of tasks (based on task_id and instruction) and appends the remainder\n        of the new tasks. The current task is updated to the first unfinished task in this merged list.\n\n        Args:\n            tasks (list[Task]): A list of tasks (may be unordered) to add to the plan.\n\n        Returns:\n            None: The method updates the internal state of the plan but does not return anything.\n        \"\"\"\n        if not tasks:\n            return\n\n        # Topologically sort the new tasks to ensure correct dependency order\n        new_tasks = self._topological_sort(tasks)\n\n        if not self.tasks:\n            # If there are no existing tasks, set the new tasks as the current tasks\n            self.tasks = new_tasks\n\n        else:\n            # Find the length of the common prefix between existing and new tasks\n            prefix_length = 0\n            for old_task, new_task in zip(self.tasks, new_tasks):\n                if old_task.task_id != new_task.task_id or old_task.instruction != new_task.instruction:\n                    break\n                prefix_length += 1\n\n            # Combine the common prefix with the remainder of the new tasks\n            final_tasks = self.tasks[:prefix_length] + new_tasks[prefix_length:]\n            self.tasks = final_tasks\n\n        # Update current_task_id to the first unfinished task in the merged list\n        self._update_current_task()\n\n        # Update the task map for quick access to tasks by ID\n        self.task_map = {task.task_id: task for task in self.tasks}\n\n    def reset_task(self, task_id: str):\n        \"\"\"\n        Reset a task based on task_id, i.e. set Task.is_finished=False and request redo. This also resets all tasks depending on it.\n\n        Args:\n            task_id (str): The ID of the task to be reset.\n        \"\"\"\n        if task_id in self.task_map:\n            task = self.task_map[task_id]\n            task.reset()\n            # reset all downstream tasks that are dependent on the reset task\n            for dep_task in self.tasks:\n                if task_id in dep_task.dependent_task_ids:\n                    # FIXME: if LLM generates cyclic tasks, this will result in infinite recursion\n                    self.reset_task(dep_task.task_id)\n\n        self._update_current_task()\n\n    def _replace_task(self, new_task: Task):\n        \"\"\"\n        Replace an existing task with the new input task based on task_id, and reset all tasks depending on it.\n\n        Args:\n            new_task (Task): The new task that will replace an existing one.\n\n        Returns:\n            None\n        \"\"\"\n        assert new_task.task_id in self.task_map\n        # Replace the task in the task map and the task list\n        self.task_map[new_task.task_id] = new_task\n        for i, task in enumerate(self.tasks):\n            if task.task_id == new_task.task_id:\n                self.tasks[i] = new_task\n                break\n\n        # Reset dependent tasks\n        for task in self.tasks:\n            if new_task.task_id in task.dependent_task_ids:\n                self.reset_task(task.task_id)\n\n        self._update_current_task()\n\n    def _append_task(self, new_task: Task):\n        \"\"\"\n        Append a new task to the end of existing task sequences\n\n        Args:\n            new_task (Task): The new task to be appended to the existing task sequence\n\n        Returns:\n            None\n        \"\"\"\n        # assert not self.has_task_id(new_task.task_id), \"Task already in current plan, use replace_task instead\"\n        if self.has_task_id(new_task.task_id):\n            logger.warning(\n                \"Task already in current plan, should use replace_task instead. Overwriting the existing task.\"\n            )\n\n        assert all(\n            [self.has_task_id(dep_id) for dep_id in new_task.dependent_task_ids]\n        ), \"New task has unknown dependencies\"\n\n        # Existing tasks do not depend on the new task, it's fine to put it to the end of the sorted task sequence\n        self.tasks.append(new_task)\n        self.task_map[new_task.task_id] = new_task\n        self._update_current_task()\n\n    def has_task_id(self, task_id: str) -> bool:\n        return task_id in self.task_map\n\n    def _update_current_task(self):\n        self.tasks = self._topological_sort(self.tasks)\n        # Update the task map for quick access to tasks by ID\n        self.task_map = {task.task_id: task for task in self.tasks}\n\n        current_task_id = \"\"\n        for task in self.tasks:\n            if not task.is_finished:\n                current_task_id = task.task_id\n                break\n        self.current_task_id = current_task_id\n        TaskReporter().report({\"tasks\": [i.model_dump() for i in self.tasks], \"current_task_id\": current_task_id})\n\n    @property\n    def current_task(self) -> Task:\n        \"\"\"Find current task to execute\n\n        Returns:\n            Task: the current task to be executed\n        \"\"\"\n        return self.task_map.get(self.current_task_id, None)\n\n    def finish_current_task(self):\n        \"\"\"Finish current task, set Task.is_finished=True, set current task to next task\"\"\"\n        if self.current_task_id:\n            self.current_task.is_finished = True\n            self._update_current_task()  # set to next task\n\n    def finish_all_tasks(self):\n        \"Finish all tasks.\"\n        while self.current_task:\n            self.finish_current_task()\n\n    def is_plan_finished(self) -> bool:\n        \"\"\"Check if all tasks are finished\"\"\"\n        return all(task.is_finished for task in self.tasks)\n\n    def get_finished_tasks(self) -> list[Task]:\n        \"\"\"return all finished tasks in correct linearized order\n\n        Returns:\n            list[Task]: list of finished tasks\n        \"\"\"\n        return [task for task in self.tasks if task.is_finished]\n\n    def append_task(\n        self, task_id: str, dependent_task_ids: list[str], instruction: str, assignee: str, task_type: str = \"\"\n    ):\n        \"\"\"\n        Append a new task with task_id (number) to the end of existing task sequences.\n        If dependent_task_ids is not empty, the task will depend on the tasks with the ids in the list.\n        Note that the assignee should be the 'name' of the role.\n        \"\"\"\n        new_task = Task(\n            task_id=task_id,\n            dependent_task_ids=dependent_task_ids,\n            instruction=instruction,\n            assignee=assignee,\n            task_type=task_type,\n        )\n        return self._append_task(new_task)\n\n    def replace_task(self, task_id: str, new_dependent_task_ids: list[str], new_instruction: str, new_assignee: str):\n        \"\"\"Replace an existing task (can be current task) based on task_id, and reset all tasks depending on it.\"\"\"\n        new_task = Task(\n            task_id=task_id,\n            dependent_task_ids=new_dependent_task_ids,\n            instruction=new_instruction,\n            assignee=new_assignee,\n        )\n        return self._replace_task(new_task)\n",
                        "tags": []
                    },
                    "DataAnalyst": {
                        "name": "DataAnalyst",
                        "path": "metagpt/roles/di/data_analyst.py",
                        "schemas": {
                            "type": "class",
                            "description": "A role who can think and act dynamically",
                            "methods": {
                                "write_and_exec_code": {
                                    "type": "async_function",
                                    "description": "Write a code block for current task and execute it in an interactive notebook environment. ",
                                    "signature": "(self, instruction: 'str' = '')",
                                    "parameters": "Args: instruction (optional, str): Further hints or notice other than the current task instruction, must be very concise and can be empty. Write text guidance, NOT code. Defaults to \"\"."
                                }
                            },
                            "tool_path": "metagpt/roles/di/data_analyst.py"
                        },
                        "code": "@register_tool(include_functions=[\"write_and_exec_code\"])\nclass DataAnalyst(RoleZero):\n    name: str = \"David\"\n    profile: str = \"DataAnalyst\"\n    goal: str = \"Take on any data-related tasks, such as machine learning, deep learning, web browsing, web scraping, web searching, terminal operation, etc.\"\n    instruction: str = ROLE_INSTRUCTION + EXTRA_INSTRUCTION\n    task_type_desc: str = TASK_TYPE_DESC\n\n    tools: list[str] = [\n        \"Plan\",\n        \"DataAnalyst\",\n        \"RoleZero\",\n        \"Browser\",\n        \"Editor:write,read,similarity_search\",\n        \"Terminal:run\",\n        \"SearchEnhancedQA\",\n    ]\n    custom_tools: list[str] = [\"web scraping\", \"Terminal\", \"Editor:write,read,similarity_search\"]\n    custom_tool_recommender: ToolRecommender = None\n    # a larger limit to allow for complex development\n    max_consecutive_react_limit: int = 20\n\n    use_reflection: bool = True\n    write_code: WriteAnalysisCode = Field(default_factory=WriteAnalysisCode, exclude=True)\n    execute_code: Annotated[\n        Union[ExecuteNbCode, RemoteProxy], BeforeValidator(ensure_execute_nb_code_instance)\n    ] = Field(default_factory=resolve_execute_nb_code)\n\n    @model_validator(mode=\"after\")\n    def set_custom_tool(self):\n        if self.custom_tools and not self.custom_tool_recommender:\n            self.custom_tool_recommender = BM25ToolRecommender(tools=self.custom_tools, force=True)\n\n    def _update_tool_execution(self):\n        self.tool_execution_map.update(\n            {\n                \"DataAnalyst.write_and_exec_code\": self.write_and_exec_code,\n            }\n        )\n\n    def _retrieve_example(self) -> str:\n        \"\"\"returns examples based on keywords in the context.\"\"\"\n        context = [str(msg) for msg in self.rc.memory.get(self.memory_k)]\n        context = \"\\n\\n\".join(context)\n        if \"https:\" in context.lower() or \"http:\" in context.lower() or \"search\" in context.lower():\n            if \"search\" in context.lower() or \"click\" in context.lower():\n                return WEB_SCRAPING_EXAMPLE\n            return WEB_SCRAPING_EXAMPLE_SIMPLE\n        return \"\"\n\n    async def write_and_exec_code(self, instruction: str = \"\"):\n        \"\"\"Write a code block for current task and execute it in an interactive notebook environment.\n\n        Args:\n            instruction (optional, str): Further hints or notice other than the current task instruction, must be very concise and can be empty. Write text guidance, NOT code. Defaults to \"\".\n        \"\"\"\n        if self.planner.plan:\n            logger.info(f\"Current task {self.planner.plan.current_task}\")\n\n        counter = 0\n        success = False\n        await self.execute_code.init_code()\n\n        # plan info\n        if self.planner.current_task:\n            # clear task result from plan to save token, since it has been in memory\n            plan_status = self.planner.get_plan_status(exclude=[\"task_result\"])\n            plan_status += f\"\\nFurther Task Instruction: {instruction}\"\n        else:\n            return \"No current_task found now. Please use command Plan.append_task to add a task first.\"\n\n        # tool info\n        if self.custom_tool_recommender:\n            plan = self.planner.plan\n            fixed = [\"Terminal\"] if \"Terminal\" in self.custom_tools else None\n            tool_info = await self.custom_tool_recommender.get_recommended_tool_info(fixed=fixed, plan=plan)\n        else:\n            tool_info = \"\"\n\n        # data info\n        await self._check_data()\n\n        while not success and counter < 3:\n            ### write code ###\n            logger.info(\"ready to WriteAnalysisCode\")\n            use_reflection = counter > 0 and self.use_reflection  # only use reflection after the first trial\n\n            code = await self.write_code.run(\n                user_requirement=self.planner.plan.goal,\n                plan_status=plan_status,\n                tool_info=tool_info,\n                working_memory=self.rc.working_memory.get(),\n                use_reflection=use_reflection,\n                memory=self.rc.memory.get(self.memory_k),\n            )\n            self.rc.working_memory.add(Message(content=code, role=\"assistant\", cause_by=WriteAnalysisCode))\n\n            ### execute code ###\n            result, success = await self.execute_code.run(code)\n            print(result)\n\n            self.rc.working_memory.add(Message(content=result, role=\"user\", cause_by=ExecuteNbCode))\n\n            ### process execution result ###\n            counter += 1\n            if success:\n                task_result = TaskResult(code=code, result=result, is_success=success)\n                self.planner.current_task.update_task_result(task_result)\n\n        status = \"Success\" if success else \"Failed\"\n        output = CODE_STATUS.format(code=code, status=status, result=result)\n        if success:\n            output += \"The code written has been executed successfully.\"\n        self.rc.working_memory.clear()\n        return output\n\n    async def _check_data(self):\n        if not self.planner.plan.get_finished_tasks() or self.planner.plan.current_task.task_type not in [\n            TaskType.DATA_PREPROCESS.type_name,\n            TaskType.FEATURE_ENGINEERING.type_name,\n            TaskType.MODEL_TRAIN.type_name,\n        ]:\n            return\n        logger.info(\"Check updated data\")\n        code = await CheckData().run(self.planner.plan)\n        if not code.strip():\n            return\n        result, success = await self.execute_code.run(code)\n        if success:\n            print(result)\n            data_info = DATA_INFO.format(info=result)\n            self.rc.working_memory.add(Message(content=data_info, role=\"user\", cause_by=CheckData))\n\n    async def _run_special_command(self, cmd) -> str:\n        \"\"\"command requiring special check or parsing.\"\"\"\n        # TODO: duplicate with Engineer2._run_special_command, consider dedup\n\n        # finish current task before end.\n        command_output = \"\"\n        if cmd[\"command_name\"] == \"end\" and not self.planner.plan.is_plan_finished():\n            self.planner.plan.finish_all_tasks()\n            command_output += \"All tasks are finished.\\n\"\n        command_output += await super()._run_special_command(cmd)\n        return command_output\n",
                        "tags": []
                    },
                    "RoleZero": {
                        "name": "RoleZero",
                        "path": "metagpt/roles/di/role_zero.py",
                        "schemas": {
                            "type": "class",
                            "description": "A role who can think and act dynamically",
                            "methods": {
                                "ask_human": {
                                    "type": "async_function",
                                    "description": "Use this when you fail the current task or if you are unsure of the situation encountered. Your response should contain a brief summary of your situation, ended with a clear and concise question.",
                                    "signature": "(self, question: 'str') -> 'str'",
                                    "parameters": ""
                                },
                                "reply_to_human": {
                                    "type": "async_function",
                                    "description": "Reply to human user with the content provided. Use this when you have a clear answer or solution to the user's question.",
                                    "signature": "(self, content: 'str') -> 'str'",
                                    "parameters": ""
                                }
                            },
                            "tool_path": "metagpt/roles/di/role_zero.py"
                        },
                        "code": "@register_tool(include_functions=[\"ask_human\", \"reply_to_human\"])\nclass RoleZero(Role):\n    \"\"\"A role who can think and act dynamically\"\"\"\n\n    # Basic Info\n    name: str = \"Zero\"\n    profile: str = \"RoleZero\"\n    goal: str = \"\"\n    system_msg: Optional[list[str]] = None  # Use None to conform to the default value at llm.aask\n    system_prompt: str = SYSTEM_PROMPT  # Use None to conform to the default value at llm.aask\n    cmd_prompt: str = CMD_PROMPT\n    instruction: str = ROLE_INSTRUCTION\n    task_type_desc: Optional[str] = None\n    example: str = \"\"  # global think & act demonstration for the role\n\n    # React Mode\n    react_mode: Literal[\"react\"] = \"react\"\n    max_react_loop: int = 50\n    # consecutive react count after receiving a message, used for triggering human intervention\n    consecutive_react_cnt: int = 0\n    max_consecutive_react_limit: int = 10\n\n    # Tools\n    tools: list[str] = []  # Use special symbol [\"<all>\"] to indicate use of all registered tools\n    mcp_tools: list[str] = []\n    tool_recommender: Optional[ToolRecommender] = None\n    tool_execution_map: Annotated[dict[str, Callable], Field(exclude=True)] = {}\n    special_tool_commands: list[str] = [\"Plan.finish_current_task\", \"end\", \"Terminal.run\", \"RoleZero.ask_human\"]\n\n    # Equipped with three basic tools by default for optional use\n    editor: Annotated[Union[Editor, RemoteProxy], BeforeValidator(ensure_editor_instance)] = Field(\n        default_factory=resolve_editor\n    )\n    browser: Annotated[Union[Browser, RemoteProxy], BeforeValidator(ensure_browser_instance)] = Field(\n        default_factory=resolve_browser\n    )\n    terminal: Annotated[Union[Terminal, RemoteProxy], BeforeValidator(ensure_terminal_instance)] = Field(\n        default_factory=resolve_terminal\n    )\n    working_dir: str = Field(default_factory=lambda: str(DEFAULT_WORKSPACE_ROOT.resolve()))\n\n    # Others\n    observe_all_msg_from_buffer: bool = True\n    command_rsp: str = \"\"  # the raw string containing the commands\n    commands: list[dict] = []  # commands to be executed\n    memory_k: int = 30  # number of memories (messages) to use as historical context\n    use_fixed_sop: bool = False\n    use_summary: bool = True  # whether to summarize at the end\n    browser_is_active: bool = False  # Track browser state to check if it is open\n    think_task: Optional[asyncio.Task] = Field(default=None, exclude=True)  # Track the background thinking task\n\n    # @model_validator(mode=\"after\")\n    # def validate_and_init_mcp_tools(self) -> \"RoleZero\":\n    #     \"\"\"Initialize mcp_tools if empty and MCP servers are enabled\"\"\"\n    #     if not self.mcp_tools:\n    #         if self.config.mcp.has_enabled_servers():\n    #             self.mcp_tools = mcp_manager.list_tool_names()\n    #     return self\n\n    @model_validator(mode=\"after\")\n    def set_plan_and_tool(self) -> \"RoleZero\":\n        # We force using this parameter for DataAnalyst\n        assert self.react_mode == \"react\"\n\n        # Roughly the same part as DataInterpreter.set_plan_and_tool\n        self._set_react_mode(react_mode=self.react_mode, max_react_loop=self.max_react_loop)\n\n        tools = self.tools  # + self.mcp_tools\n        if tools and not self.tool_recommender:\n            self.tool_recommender = BM25ToolRecommender(tools=tools, force=True)\n        self.set_actions([RunCommand])\n\n        # HACK: Init Planner, control it through dynamic thinking; Consider formalizing as a react mode\n        self.planner = Planner(goal=\"\", working_memory=self.rc.working_memory, auto_run=True)\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_tool_execution(self) -> \"RoleZero\":\n        # default map, includes all special tool commands\n        self.tool_execution_map = {\n            \"Plan.append_task\": self.planner.plan.append_task,\n            \"Plan.reset_task\": self.planner.plan.reset_task,\n            \"Plan.replace_task\": self.planner.plan.replace_task,\n            \"Terminal.run\": self.terminal.run,\n            \"Terminal\": self.terminal.run,  # alias\n            \"Terminal.run_command\": self.terminal.run,  # alias\n            \"Terminal.preview\": self.terminal.preview,\n            \"RoleZero.ask_human\": self.ask_human,\n            \"RoleZero.reply_to_human\": self.reply_to_human,\n            \"SearchEnhancedQA.run\": SearchEnhancedQA().run,\n            \"Plan.finish_current_task\": self.planner.plan.finish_current_task,\n            \"end\": self._end,\n        }\n        self.tool_execution_map.update(\n            {\n                f\"Browser.{i}\": getattr(self.browser, i)\n                for i in [\n                    \"click\",\n                    \"close_tab\",\n                    \"go_back\",\n                    \"go_forward\",\n                    \"goto\",\n                    \"hover\",\n                    \"press\",\n                    \"scroll\",\n                    \"tab_focus\",\n                    \"type\",\n                    \"take_screenshot\",\n                ]\n            }\n        )\n        self.tool_execution_map.update(\n            {\n                f\"Editor.{i}\": getattr(self.editor, i)\n                for i in [\n                    \"append_file\",\n                    \"create_file\",\n                    \"edit_file_by_replace\",\n                    \"find_file\",\n                    \"goto_line\",\n                    \"insert_content_at_line\",\n                    \"open_file\",\n                    \"read\",\n                    \"scroll_down\",\n                    \"scroll_up\",\n                    \"search_dir\",\n                    \"search_file\",\n                    \"similarity_search\",\n                    # \"set_workdir\",\n                    \"write\",\n                ]\n            }\n        )\n\n        for tool_name in self.mcp_tools:\n            self.tool_execution_map[tool_name] = mcp_manager.get_tool_executor(tool_name)\n\n        # can be updated by subclass\n        self._update_tool_execution()\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_tool_working_dir(self) -> \"RoleZero\":\n        asyncio.create_task(self._update_workdir_for_tools(self.working_dir))\n        return self\n\n    @model_validator(mode=\"after\")\n    def set_longterm_memory(self) -> \"RoleZero\":\n        \"\"\"Set up special memory for the role if enabled in the configuration.\"\"\"\n\n        # We set Role.rc.memory as SerializeAsAny to retain all fields in Memory subclasses, ensuring model_dump() includes subclass fields.\n        # However, for some of the fields, we want to use the latest values from the config.\n        # To avoid repeated args, we init the subclass first, then update the fields with the config\n        if self.config.role_zero.enable_compressable_memory:\n            kvs = self.rc.memory.model_dump()\n            kvs[\"compress_model_name\"] = self.config.role_zero.compress_model_name\n            self.rc.memory = CompressableMemory(**kvs)\n            self.rc.memory.memory_k = self.config.role_zero.memory_k\n            self.rc.memory.compress_k = self.config.role_zero.compress_k\n            self.rc.memory.compress_trigger_token_threshold = self.config.role_zero.compress_trigger_token_threshold\n            logger.info(f\"Compressable memory set for role '{self.name}'\")\n\n        elif self.config.role_zero.enable_longterm_memory:\n            # Use config.role_zero to initialize long-term memory\n            # The role name will be used as the collection name.\n            self.rc.memory = RoleZeroLongTermMemory(**self.rc.memory.model_dump())\n            memory = self.rc.memory\n            memory.persist_path = self.config.role_zero.longterm_memory_persist_path\n            memory.collection_name = self.name.replace(\" \", \"\")\n            memory.memory_k = self.config.role_zero.memory_k\n            memory.similarity_top_k = self.config.role_zero.similarity_top_k\n            memory.use_llm_ranker = self.config.role_zero.use_llm_ranker\n            logger.info(f\"Long-term memory set for role '{self.name}'\")\n\n        return self\n\n    def _update_tool_execution(self):\n        pass\n\n    def _retrieve_example(self) -> str:\n        \"\"\"Default implementation of example retrieval. Can be overwritten in subclasses.\"\"\"\n        return self.example\n\n    def _get_goal_from_memories(self):\n        \"\"\"Get goal from memories by finding the latest message from TL or UserRequirement\"\"\"\n        memories = self.get_memories()\n        for msg in reversed(memories):\n            if msg.sent_from == \"Mike\" or \"UserRequirement\" in msg.cause_by:\n                return msg.content\n        # Fallback to last message if no matching message found\n        return memories[-1].content if memories else \"\"\n\n    async def _think(self) -> bool:\n        \"\"\"Useful in 'react' mode. Use LLM to decide whether and what to do next.\"\"\"\n        # Compatibility\n        if self.use_fixed_sop:\n            return await super()._think()\n\n        ### 0. Preparation ###\n        if not self.rc.todo:\n            return False\n\n        if not self.planner.plan.goal:\n            self.planner.plan.goal = self._get_goal_from_memories()\n\n        ### 1. Example ###\n        example = self._retrieve_example()\n\n        ### 2. Plan Status ###\n        plan_status, current_task = get_plan_status(planner=self.planner)\n\n        ### 3. Tool/Command Info ###\n        tools = await self.tool_recommender.recommend_tools()\n        tool_info = json.dumps({tool.name: tool.schemas for tool in tools})\n\n        ### Role Instruction ###\n        instruction = self.instruction.strip()\n        system_prompt = self.system_prompt.format(\n            role_info=self._get_prefix(),\n            domain_info=self._get_domain_info(),\n            task_type_desc=self.task_type_desc,\n            available_commands=tool_info,\n            example=example,\n            instruction=instruction,\n        )\n\n        ### Make Decision Dynamically ###\n        prompt = self.cmd_prompt.format(\n            current_state=f\"current directory: {self.working_dir}\",\n            plan_status=plan_status,\n            current_task=current_task,\n        )\n\n        ### Recent Observation ###\n        if self.config.role_zero.enable_compressable_memory:\n            await self.rc.memory.compress(extra_context=super()._get_prefix())\n        memory = self.rc.memory.get(self.memory_k)\n        if self.browser_is_active:\n            memory = await parse_browser_actions(memory, browser=self.browser)\n        # memory = await parse_images(memory, llm=self.llm)\n\n        req = self.llm.format_msg(memory + [UserMessage(content=prompt)])\n\n        state_data = dict(\n            plan_status=plan_status,\n            current_task=current_task,\n            instruction=instruction,\n        )\n        logger.info(f\"{self.name}, ready to think\")\n        # background task to handle LLM thinking process\n        self.create_think_background_task(req, system_prompt, state_data)\n        return True\n\n    def create_think_background_task(self, req, system_prompt, state_data):\n        self.think_task = asyncio.create_task(self._think_background(req, system_prompt, state_data))\n\n    async def _think_background(self, req, system_prompt, state_data):\n        \"\"\"Background task to handle LLM thinking process.\"\"\"\n        async with ThoughtReporter(enable_llm_stream=True) as reporter:\n            await reporter.async_report({\"type\": \"react\"})\n            self.command_rsp = await self.llm_cached_aask(req=req, system_msgs=[system_prompt], state_data=state_data)\n        logger.info(f\"Command response:\\n{self.command_rsp}\")\n        rsp_hist = [mem.content for mem in self.rc.memory.get()]\n        self.command_rsp = await check_duplicates(\n            req=req, command_rsp=self.command_rsp, rsp_hist=rsp_hist, llm=self.llm\n        )\n\n    @exp_cache(context_builder=RoleZeroContextBuilder(), serializer=RoleZeroSerializer())\n    async def llm_cached_aask(self, *, req: list[dict], system_msgs: list[str], **kwargs) -> str:\n        \"\"\"Use `exp_cache` to automatically manage experiences.\n\n        The `RoleZeroContextBuilder` attempts to add experiences to `req`.\n        The `RoleZeroSerializer` extracts essential parts of `req` for the experience pool, trimming lengthy entries to retain only necessary parts.\n        \"\"\"\n        return await self.llm.aask(req, system_msgs=system_msgs)\n\n    def _get_domain_info(self) -> str:\n        return get_time_info() + MGX_INFO + get_enable_supabase_guidance()\n\n    async def _act(self) -> Message:\n        if self.use_fixed_sop:\n            return await super()._act()\n\n        command_outputs = []\n        async for cmd in parse_stream_commands2(role_instance=self, tool_execution_map=self.tool_execution_map):\n            is_success, output = await self._run_command(cmd)\n            command_outputs.append(output)\n            if not is_success:\n                break\n        if not command_outputs:\n            command_outputs = [\"No valid commands found for execution, pay attention to the output format.\"]\n        self.rc.memory.add(AIMessage(content=self.command_rsp))\n        outputs = \"\\n\\n\".join(command_outputs)\n        logger.info(f\"Commands outputs: \\n{outputs}\")\n        self.rc.memory.add(UserMessage(content=outputs, cause_by=RunCommand))\n\n        think_task = self.think_task\n        self.think_task = None\n        await think_task\n\n        return AIMessage(\n            content=f\"I have finished the task, please mark my task as finished. Outputs: {outputs}\",\n            sent_from=self.name,\n            cause_by=RunCommand,\n        )\n\n    async def _react(self) -> Message:\n        # NOTE: Diff 1: Each time landing here means news is observed, set todo to allow news processing in _think\n        self._set_state(0)\n\n        actions_taken = 0\n        self.consecutive_react_cnt = 0\n        rsp = AIMessage(content=\"No actions taken yet\", cause_by=Action)  # will be overwritten after Role _act\n        while actions_taken < self.rc.max_react_loop:\n            # NOTE: Diff 2: Keep observing within _react, news will go into memory, allowing adapting to new info\n            if await self._observe():\n                self.consecutive_react_cnt = 0\n                # Reset state to 0 to override potential `end` command (state = -1) and ensure the loop continues.\n                self._set_state(0)\n            # think\n            has_todo = await self._think()\n            if not has_todo:\n                break\n            # act\n            logger.debug(f\"{self._setting}: {self.rc.state=}, will do {self.rc.todo}\")\n            rsp = await self._act()\n            actions_taken += 1\n            self.consecutive_react_cnt += 1\n\n            # post-check\n            if self.rc.max_react_loop >= 10 and actions_taken >= self.rc.max_react_loop:\n                # If max_react_loop is a small value (e.g. < 10), it is intended to be reached and make the agent stop\n                logger.warning(f\"reached max_react_loop: {actions_taken}\")\n                human_rsp = await self.ask_human(\n                    \"I have reached my max action rounds, do you want me to continue? Yes or no\"\n                )\n                if \"yes\" in human_rsp.lower():\n                    actions_taken = 0\n            if self.consecutive_react_cnt >= self.max_consecutive_react_limit:\n                memory = self.get_memories(k=self.memory_k)\n                context = self.llm.format_msg(memory + [UserMessage(content=SUMMARIZE_STATUS_WHEN_CONSECUTIVE)])\n                question = await self.llm.aask(context)\n                human_rsp = await self.ask_human(question)\n                self.rc.memory.add(UserMessage(content=\"User's extra instruction: \" + human_rsp, cause_by=RunCommand))\n                self.consecutive_react_cnt = 0\n\n        return rsp  # return output from the last action\n\n    async def _run_command(self, cmd) -> tuple[bool, str]:\n        \"\"\"stream command execution\"\"\"\n        output = f\"Command {cmd['command_name']} executed\"\n        try:\n            # handle special command first\n            if self._is_special_command(cmd):\n                special_command_output = await self._run_special_command(cmd)\n                output += \":\" + special_command_output\n                cmd[\"status\"] = \"success\"  # mgx_backend need these fields\n                cmd[\"error_msg\"] = \"\"\n                return True, output\n\n            # run command as specified by tool_execute_map\n            if cmd[\"command_name\"] in self.tool_execution_map:\n                tool_obj = self.tool_execution_map[cmd[\"command_name\"]]\n                if inspect.iscoroutinefunction(tool_obj):\n                    tool_output = await tool_obj(**cmd[\"args\"])\n                else:\n                    tool_output = tool_obj(**cmd[\"args\"])\n                self._try_update_browser_state(cmd[\"command_name\"])\n                if tool_output:\n                    output += f\": {str(tool_output)}\"\n                cmd[\"status\"] = \"success\"\n                cmd[\"error_msg\"] = \"\"\n            else:\n                output = f\"Command {cmd['command_name']} not found.\"\n                cmd[\"status\"] = \"failed\"\n                cmd[\"error_msg\"] = f\"Command {cmd['command_name']} not found.\"\n                return False, output\n\n        except Exception:\n            tb = traceback.format_exc()\n            output += f\": {tb}\"\n            cmd[\"status\"] = \"failed\"\n            cmd[\"error_msg\"] = f\"Command {cmd['command_name']} failed: {tb}\"\n            return False, output\n\n        return True, output\n\n    def _is_special_command(self, cmd) -> bool:\n        return cmd[\"command_name\"] in self.special_tool_commands\n\n    async def _run_special_command(self, cmd) -> str:\n        \"\"\"command requiring special check or parsing\"\"\"\n        command_output = \"\"\n\n        if cmd[\"command_name\"] == \"Plan.finish_current_task\":\n            if not self.planner.plan.is_plan_finished():\n                self.planner.plan.finish_current_task()\n            command_output = (\n                \"Current task is finished. If you no longer need to take action, use the command 'end' to stop.\"\n            )\n\n        elif cmd[\"command_name\"] == \"end\":\n            command_output = await self._end()\n        elif cmd[\"command_name\"] == \"RoleZero.ask_human\":\n            args = cmd[\"args\"]\n            # fault tolerance for llm hallucination\n            question = args.get(\"question\") or args.get(\"content\") or args.get(\"message\")\n            assert question, \"Use kwarg 'question' to provide the question.\"\n            human_response = await self.ask_human(question=question)\n            self.consecutive_react_cnt = 0\n            if human_response.strip().lower().endswith((\"stop\", \"<stop>\")):\n                human_response += \"The user has asked me to stop because I have encountered a problem.\"\n                self.rc.memory.add(UserMessage(content=human_response, cause_by=RunCommand))\n                end_output = \"\\nCommand end executed:\"\n                end_output += await self._end()\n                return end_output\n            return human_response\n        # output from bash.run may be empty, add decorations to the output to ensure visibility.\n        elif \"Terminal\" in cmd[\"command_name\"]:  # Terminal.run and its alias\n            tool_obj = self.tool_execution_map[cmd[\"command_name\"]]\n            tool_output = await tool_obj(**cmd[\"args\"])\n            command_output = format_terminal_output(cmd=cmd, raw_output=tool_output)\n            self.working_dir = await self.terminal.get_cwd()\n            await self._update_workdir_for_tools(self.working_dir)\n            token_count = count_string_tokens(command_output, model_name=\"default\")\n            if token_count >= self.config.llm.max_token * 0.9:\n                command_output = \"OutputTooLongException: the content is too long to use. Please optimize the command parameters to reduce the amount of output.\"\n\n        return command_output\n\n    async def _update_workdir_for_tools(self, new_workdir: str) -> None:\n        \"\"\"Update working directory for all relevant tools.\n        This method can be overridden by subclasses to update additional tools.\n        \"\"\"\n        await self.editor.set_workdir(new_workdir)\n\n    async def ask_human(self, question: str) -> str:\n        \"\"\"Use this when you fail the current task or if you are unsure of the situation encountered. Your response should contain a brief summary of your situation, ended with a clear and concise question.\"\"\"\n        # NOTE: Can be overwritten in remote setting\n        from metagpt.environment.mgx.mgx_env import MGXEnv  # avoid circular import\n\n        if not isinstance(self.rc.env, MGXEnv):\n            return \"Not in MGXEnv, command will not be executed.\"\n        return await self.rc.env.ask_human(question, sent_from=self)\n\n    async def reply_to_human(self, content: str) -> str:\n        \"\"\"Reply to human user with the content provided. Use this when you have a clear answer or solution to the user's question.\"\"\"\n        # NOTE: Can be overwritten in remote setting\n        from metagpt.environment.mgx.mgx_env import MGXEnv  # avoid circular import\n\n        if not isinstance(self.rc.env, MGXEnv):\n            return \"Not in MGXEnv, command will not be executed.\"\n        return await self.rc.env.reply_to_human(content, sent_from=self)\n\n    async def _end(self, **kwarg):\n        self._set_state(-1)\n        memory = self.rc.memory.get(self.memory_k)\n        outputs = \"\"\n        # Summary of the Completed Task and Deliverables\n        if self.use_summary:\n            logger.info(\"end current run and summarize\")\n            async with ThoughtReporter() as reporter:\n                await reporter.async_report({\"type\": \"summary\"})\n                outputs = await self.llm.aask(self.llm.format_msg(memory + [UserMessage(SUMMARY_PROMPT)]))\n        return outputs\n\n    def _try_update_browser_state(self, command_name: str) -> None:\n        \"\"\"Update browser_is_active based on the command_name.\"\"\"\n\n        if command_name == \"Browser.goto\":\n            self.browser_is_active = True\n        elif command_name == \"Browser.close_tab\":\n            self.browser_is_active = False\n",
                        "tags": []
                    },
                    "Browser": {
                        "name": "Browser",
                        "path": "metagpt/tools/libs/browser.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool for browsing the web. Don't initialize a new instance of this class if one already exists. Note: If you plan to use the browser to assist you in completing tasks, then using the browser should be a standalone task, executing actions each time based on the content seen on the webpage before proceeding to the next step. ## Example Issue: The details of the latest issue in the geekan/MetaGPT repository. Plan: Use a browser to view the details of the latest issue in the geekan/MetaGPT repository. Solution: Let's first open the issue page of the MetaGPT repository with the `Browser.goto` command >>> await browser.goto(\"https://github.com/geekan/MetaGPT/issues\") From the output webpage, we've identified that the latest issue can be accessed by clicking on the element with ID \"1141\". >>> await browser.click(1141) Finally, we have found the webpage for the latest issue, we can close the tab and finish current task. >>> await browser.close_tab()",
                            "methods": {
                                "click": {
                                    "type": "async_function",
                                    "description": "clicks on an element with a specific id on the webpage.",
                                    "signature": "(self, element_id: 'int')",
                                    "parameters": ""
                                },
                                "close_tab": {
                                    "type": "async_function",
                                    "description": "Close the currently active tab.",
                                    "signature": "(self)",
                                    "parameters": ""
                                },
                                "go_back": {
                                    "type": "async_function",
                                    "description": "Navigate to the previously viewed page.",
                                    "signature": "(self)",
                                    "parameters": ""
                                },
                                "go_forward": {
                                    "type": "async_function",
                                    "description": "Navigate to the next page (if a previous 'go_back' action was performed).",
                                    "signature": "(self)",
                                    "parameters": ""
                                },
                                "goto": {
                                    "type": "async_function",
                                    "description": "Navigate to a specific URL.",
                                    "signature": "(self, url: 'str', timeout: 'float' = 90000)",
                                    "parameters": ""
                                },
                                "hover": {
                                    "type": "async_function",
                                    "description": "Hover over an element with id.",
                                    "signature": "(self, element_id: 'int')",
                                    "parameters": ""
                                },
                                "press": {
                                    "type": "async_function",
                                    "description": "Simulates the pressing of a key combination on the keyboard (e.g., Ctrl+v).",
                                    "signature": "(self, key_comb: 'str')",
                                    "parameters": ""
                                },
                                "scroll": {
                                    "type": "async_function",
                                    "description": "Scroll the page up or down.",
                                    "signature": "(self, direction: \"Literal['down', 'up']\")",
                                    "parameters": ""
                                },
                                "tab_focus": {
                                    "type": "async_function",
                                    "description": "Open a new, empty browser tab.",
                                    "signature": "(self, page_number: 'int')",
                                    "parameters": ""
                                },
                                "type": {
                                    "type": "async_function",
                                    "description": "Use this to type the content into the field with id.",
                                    "signature": "(self, element_id: 'int', content: 'str', press_enter_after: 'bool' = False)",
                                    "parameters": ""
                                }
                            },
                            "tool_path": "metagpt/tools/libs/browser.py"
                        },
                        "code": "@register_tool(\n    tags=[\"web\", \"browse\"],\n    include_functions=[\n        \"click\",\n        \"close_tab\",\n        \"go_back\",\n        \"go_forward\",\n        \"goto\",\n        \"hover\",\n        \"press\",\n        \"scroll\",\n        \"tab_focus\",\n        \"type\",\n    ],\n)\nclass Browser(BaseModel):\n    \"\"\"A tool for browsing the web. Don't initialize a new instance of this class if one already exists.\n\n    Note: If you plan to use the browser to assist you in completing tasks, then using the browser should be a standalone\n    task, executing actions each time based on the content seen on the webpage before proceeding to the next step.\n\n    ## Example\n    Issue: The details of the latest issue in the geekan/MetaGPT repository.\n    Plan: Use a browser to view the details of the latest issue in the geekan/MetaGPT repository.\n    Solution:\n    Let's first open the issue page of the MetaGPT repository with the `Browser.goto` command\n\n    >>> await browser.goto(\"https://github.com/geekan/MetaGPT/issues\")\n\n    From the output webpage, we've identified that the latest issue can be accessed by clicking on the element with ID \"1141\".\n\n    >>> await browser.click(1141)\n\n    Finally, we have found the webpage for the latest issue, we can close the tab and finish current task.\n\n    >>> await browser.close_tab()\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    playwright: Optional[Playwright] = Field(default=None, exclude=True)\n    browser_instance: Optional[Browser_] = Field(default=None, exclude=True)\n    browser_ctx: Optional[BrowserContext] = Field(default=None, exclude=True)\n    page: Optional[Page] = Field(default=None, exclude=True)\n    accessibility_tree: list = Field(default_factory=list)\n    headless: bool = Field(default=True)\n    proxy: Optional[dict] = Field(default_factory=get_proxy_from_env)\n    is_empty_page: bool = Field(default=True)\n    reporter: BrowserReporter = Field(default_factory=BrowserReporter)\n    url: Optional[str] = Field(default=None)\n\n    _recover_task: Optional[asyncio.Task] = PrivateAttr(None)\n\n    @model_validator(mode=\"after\")\n    def valid_page(self):\n        if not self.is_empty_page:\n            if self.url:\n                loop = asyncio.get_running_loop()\n                # Create an asynchronous task to perform the recovery in the background to avoid blocking the startup\n                task = self._recover_task = loop.create_task(self._recover())\n\n                def callback(task):\n                    self._recover_task = None\n\n                task.add_done_callback(callback)\n            else:\n                self.is_empty_page = True\n        return self\n\n    async def _recover(self):\n        try:\n            await self.start()\n            page = self.page\n            await page.goto(self.url)\n            await self._wait_until_page_idle(page)\n            self.accessibility_tree = await get_accessibility_tree(page)\n        except Exception:\n            logger.exception(f\"fail to recover the page {self.url}\")\n            self.is_empty_page = True\n\n    async def _check_recover(self):\n        if self._recover_task:\n            await self._recover_task\n\n    async def start(self) -> None:\n        \"\"\"Starts Playwright and launches a browser\"\"\"\n        if self.playwright is None:\n            self.playwright = playwright = await async_playwright().start()\n            browser = self.browser_instance = await playwright.chromium.launch(headless=self.headless, proxy=self.proxy)\n            browser_ctx = self.browser_ctx = await browser.new_context()\n            self.page = await browser_ctx.new_page()\n\n    async def stop(self):\n        if self.playwright:\n            playwright = self.playwright\n            self.playwright = None\n            self.browser_instance = None\n            self.browser_ctx = None\n            await playwright.stop()\n\n    async def click(self, element_id: int):\n        \"\"\"clicks on an element with a specific id on the webpage.\"\"\"\n        await self._check_recover()\n        await click_element(self.page, get_backend_node_id(element_id, self.accessibility_tree))\n        return await self._wait_page()\n\n    async def type(self, element_id: int, content: str, press_enter_after: bool = False):\n        \"\"\"Use this to type the content into the field with id.\"\"\"\n        await self._check_recover()\n        if press_enter_after:\n            content += \"\\n\"\n        await click_element(self.page, get_backend_node_id(element_id, self.accessibility_tree))\n        await type_text(self.page, content)\n        return await self._wait_page()\n\n    async def hover(self, element_id: int):\n        \"\"\"Hover over an element with id.\"\"\"\n        await self._check_recover()\n        await hover_element(self.page, get_backend_node_id(element_id, self.accessibility_tree))\n        return await self._wait_page()\n\n    async def press(self, key_comb: str):\n        \"\"\"Simulates the pressing of a key combination on the keyboard (e.g., Ctrl+v).\"\"\"\n        await self._check_recover()\n        await key_press(self.page, key_comb)\n        return await self._wait_page()\n\n    async def scroll(self, direction: Literal[\"down\", \"up\"]):\n        \"\"\"Scroll the page up or down.\"\"\"\n        await self._check_recover()\n        await scroll_page(self.page, direction)\n        return await self._wait_page()\n\n    async def goto(self, url: str, timeout: float = 90000):\n        \"\"\"Navigate to a specific URL.\"\"\"\n        await self._check_recover()\n        if self.page is None:\n            await self.start()\n        async with self.reporter as reporter:\n            await reporter.async_report(url, \"url\")\n            await self.page.goto(url, timeout=timeout)\n            self.is_empty_page = False\n            self.url = self.page.url\n            return await self._wait_page()\n\n    async def go_back(self):\n        \"\"\"Navigate to the previously viewed page.\"\"\"\n        await self._check_recover()\n        await self.page.go_back()\n        return await self._wait_page()\n\n    async def go_forward(self):\n        \"\"\"Navigate to the next page (if a previous 'go_back' action was performed).\"\"\"\n        await self._check_recover()\n        await self.page.go_forward()\n        return await self._wait_page()\n\n    async def tab_focus(self, page_number: int):\n        \"\"\"Open a new, empty browser tab.\"\"\"\n        await self._check_recover()\n        page = self.browser_ctx.pages[page_number]\n        await page.bring_to_front()\n        return await self._wait_page()\n\n    async def close_tab(self):\n        \"\"\"Close the currently active tab.\"\"\"\n        await self._check_recover()\n        await self.page.close()\n        if len(self.browser_ctx.pages) > 0:\n            self.page = self.browser_ctx.pages[-1]\n        else:\n            self.page = await self.browser_ctx.new_page()\n            self.is_empty_page = True\n            self.url = \"\"\n        return await self._wait_page()\n\n    async def view(self):\n        await self._check_recover()\n        observation = parse_accessibility_tree(self.accessibility_tree)\n        return f\"Current Browser Viewer\\n URL: {self.page.url}\\nOBSERVATION:\\n{observation[0]}\\n\"\n\n    async def check_if_page_is_empty(self):\n        return self.is_empty_page\n\n    async def take_screenshot(self, url, timeout=90000, save_path=\"\") -> str:\n        logger.info(f\"Take screenshot for {url}\")\n        await self.goto(url, timeout=timeout)\n        await self.page.wait_for_load_state(\"networkidle\")\n        save_path = save_path or f\"{TMP}/test.png\"\n        await self.page.screenshot(path=save_path, full_page=True, type=\"png\")\n        return encode_image(save_path)\n\n    async def _wait_page(self):\n        page = self.page\n        await self._wait_until_page_idle(page)\n        self.accessibility_tree = await get_accessibility_tree(page)\n        await self.reporter.async_report(page, \"page\")\n        return f\"SUCCESS, URL: {page.url} have been loaded.\"\n\n    def _register_page_event(self, page: Page):\n        page.last_busy_time = time.time()\n        page.requests = set()\n        page.on(\"domcontentloaded\", self._update_page_last_busy_time)\n        page.on(\"load\", self._update_page_last_busy_time)\n        page.on(\"request\", self._on_page_request)\n        page.on(\"requestfailed\", self._on_page_requestfinished)\n        page.on(\"requestfinished\", self._on_page_requestfinished)\n        page.on(\"frameattached\", self._on_frame_change)\n        page.on(\"framenavigated\", self._on_frame_change)\n\n    async def _wait_until_page_idle(self, page) -> None:\n        if not hasattr(page, \"last_busy_time\"):\n            self._register_page_event(page)\n        else:\n            page.last_busy_time = time.time()\n        while time.time() - page.last_busy_time < 0.5:\n            await page.wait_for_timeout(100)\n\n    async def _update_page_last_busy_time(self, page: Page):\n        page.last_busy_time = time.time()\n\n    async def _on_page_request(self, request: Request):\n        page = request.frame.page\n        page.requests.add(request)\n        await self._update_page_last_busy_time(page)\n\n    async def _on_page_requestfinished(self, request: Request):\n        request.frame.page.requests.discard(request)\n\n    async def _on_frame_change(self, frame: Frame):\n        await self._update_page_last_busy_time(frame.page)\n\n    async def __aenter__(self):\n        await self.start()\n        return self\n\n    async def __aexit__(self, *args, **kwargs):\n        await self.stop()\n",
                        "tags": [
                            "web",
                            "browse"
                        ]
                    },
                    "Editor": {
                        "name": "Editor",
                        "path": "metagpt/tools/libs/editor.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool for reading, understanding, writing, and editing files. Support local file including text-based files (txt, md, json, py, html, js, css, etc.), pdf, docx, excluding images, csv, excel, or online links For agent: Do NOT initiate multiple Editor.insert_content_at_line calls at the same time, since the line number will change starting with the first execution, making line number of the subsequent calls incorrect. Split the calls into separate responses. For the same reason, Editor.insert_content_at_line should NOT go behind Editor.edit_file_by_replace in the same response. Perform insert operation in a separate response.",
                            "methods": {
                                "write": {
                                    "type": "async_function",
                                    "description": "Write the whole content to a file. When used, make sure content arg contains the full content of the file.",
                                    "signature": "(self, path: str, content: str)",
                                    "parameters": ""
                                },
                                "read": {
                                    "type": "async_function",
                                    "description": "Read the whole content of a file. Using an absolute path as the argument for specifying the file location.",
                                    "signature": "(self, path: str) -> metagpt.tools.libs.editor.FileBlock",
                                    "parameters": ""
                                },
                                "similarity_search": {
                                    "type": "async_function",
                                    "description": "Given a filename or a pathname, performs a similarity search for a given query across the specified file or path. This method searches the index repository for the provided query, classifying the specified files or paths. It performs a search on each cluster of files and handles non-indexed files separately, merging results from structured indices with any direct results from non-indexed files. This function call does not depend on other functions. ",
                                    "signature": "(query: str, path: Union[str, pathlib.Path]) -> List[str]",
                                    "parameters": "Args: query (str): The search query string to look for in the indexed files. path (Union[str, Path]): A pathname or filename to search within. Returns: List[str]: A list of results as strings, containing the text from the merged results and any direct results from non-indexed files. Example: >>> query = \"The problem to be analyzed from the document\" >>> file_or_path = \"The pathname or filename you want to search within\" >>> texts: List[str] = await Editor.similarity_search(query=query, path=file_or_path) >>> print(texts)"
                                }
                            },
                            "tool_path": "metagpt/tools/libs/editor.py"
                        },
                        "code": "@register_tool(\n    include_functions=[\n        \"write\",\n        \"read\",\n        \"open_file\",\n        \"goto_line\",\n        \"scroll_down\",\n        \"scroll_up\",\n        \"create_file\",\n        \"edit_file_by_replace\",\n        \"insert_content_at_line\",\n        \"append_file\",\n        \"search_dir\",\n        \"search_file\",\n        \"find_file\",\n        \"similarity_search\",\n    ]\n)\nclass Editor(BaseModel):\n    \"\"\"\n    A tool for reading, understanding, writing, and editing files.\n    Support local file including text-based files (txt, md, json, py, html, js, css, etc.), pdf, docx, excluding images, csv, excel, or online links\n    For agent: Do NOT initiate multiple Editor.insert_content_at_line calls at the same time, since the line number will change starting with the first execution, making line number of the subsequent calls incorrect. Split the calls into separate responses.\n               For the same reason, Editor.insert_content_at_line should NOT go behind Editor.edit_file_by_replace in the same response. Perform insert operation in a separate response.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    resource: EditorReporter = EditorReporter()\n    current_file: Optional[Path] = None\n    current_line: int = 1\n    window: int = 50\n    enable_auto_lint: bool = True\n    working_dir: Path = DEFAULT_WORKSPACE_ROOT\n\n    async def write(self, path: str, content: str):\n        \"\"\"Write the whole content to a file. When used, make sure content arg contains the full content of the file.\"\"\"\n\n        path = self._try_fix_path(path)\n        content = ensure_string(content)  # Undo possible MCP conversion\n\n        if \"\\n\" not in content and \"\\\\n\" in content:\n            # A very raw rule to correct the content: If 'content' lacks actual newlines ('\\n') but includes '\\\\n', consider\n            # replacing them with '\\n' to potentially correct mistaken representations of newline characters.\n            content = content.replace(\"\\\\n\", \"\\n\")\n        directory = os.path.dirname(path)\n        if directory and not os.path.exists(directory):\n            os.makedirs(directory)\n        async with aiofiles.open(path, \"w\", encoding=\"utf-8\") as f:\n            await f.write(content)\n        # self.resource.report(path, \"path\")\n        return f\"File successfully written and saved to {path}.\"\n\n    def _read_structural_data(self, path: str) -> FileBlock:\n        \"\"\"Read the whole content of a csv file. Using an absolute path as the argument for specifying the file location.\"\"\"\n        if path.endswith(\".csv\"):\n            df = pd.read_csv(path)\n        elif path.endswith(\".xlsx\"):\n            df = pd.read_excel(path)\n        else:\n            raise ValueError(\"The file must be a csv or excel file.\")\n        return FileBlock(path=path, content=f\"Showing head of the data:\\n{df.head().to_string()}\")\n\n    async def read(self, path: str) -> FileBlock:\n        \"\"\"Read the whole content of a file. Using an absolute path as the argument for specifying the file location.\"\"\"\n\n        path = self._try_fix_path(path)\n\n        if path.suffix in STRUCTURAL_DATA_SUFFIX:\n            result = self._read_structural_data(str(path))\n            self.resource.report(str(path), \"path\")\n            return result\n\n        error = FileBlock(\n            path=str(path),\n            content=\"The file is too large to read. Use `Editor.similarity_search` to read the file instead.\",\n        )\n        path = Path(path)\n\n        # Limit the size only for text documents\n        # TODO: Formulate a complete mechanism to handle files of all types and of any size\n        if path.stat().st_size > 5 * DEFAULT_MIN_TOKEN_COUNT and path.suffix in TEXT_DOC_SUFFIX:\n            return error\n\n        content = await File.read_text_file(path)\n        if not content:\n            return FileBlock(path=str(path), content=\"\")\n        if self.is_large_file(content=content):\n            return error\n        self.resource.report(str(path), \"path\")\n\n        result = FileBlock(\n            path=str(path),\n            content=content,\n        )\n        return result\n\n    @staticmethod\n    def _is_valid_filename(file_name: str) -> bool:\n        if not file_name or not file_name.strip():\n            return False\n        invalid_chars = '<>:\"/\\\\|?*'\n        if os.name == \"nt\":  # Windows\n            invalid_chars = '<>:\"/\\\\|?*'\n        elif os.name == \"posix\":  # Unix-like systems\n            invalid_chars = \"\\0\"\n\n        for char in invalid_chars:\n            if char in file_name:\n                return False\n        return True\n\n    @staticmethod\n    def _is_valid_path(path: Path) -> bool:\n        try:\n            return path.exists()\n        except PermissionError:\n            return False\n\n    @staticmethod\n    def _create_paths(file_path: Path) -> bool:\n        try:\n            if file_path.parent:\n                file_path.parent.mkdir(parents=True, exist_ok=True)\n            return True\n        except PermissionError:\n            return False\n\n    def _check_current_file(self, file_path: Optional[Path] = None) -> bool:\n        if file_path is None:\n            file_path = self.current_file\n        if not file_path or not file_path.is_file():\n            raise ValueError(\"No file open. Use the open_file function first.\")\n        return True\n\n    @staticmethod\n    def _clamp(value, min_value, max_value):\n        return max(min_value, min(value, max_value))\n\n    def _lint_file(self, file_path: Path) -> tuple[Optional[str], Optional[int]]:\n        \"\"\"Lint the file at the given path and return a tuple with a boolean indicating if there are errors,\n        and the line number of the first error, if any.\n\n        Returns:\n            tuple[str | None, int | None]: (lint_error, first_error_line_number)\n        \"\"\"\n\n        linter = Linter(root=self.working_dir)\n        lint_error = linter.lint(str(file_path))\n        if not lint_error:\n            # Linting successful. No issues found.\n            return None, None\n        return \"ERRORS:\\n\" + lint_error.text, lint_error.lines[0]\n\n    async def _print_window(self, file_path: Path, targeted_line: int, window: int):\n        self._check_current_file(file_path)\n        async with aiofiles.open(file_path, \"r\") as file:\n            content = await file.read()\n\n            # Ensure the content ends with a newline character\n            if not content.endswith(\"\\n\"):\n                content += \"\\n\"\n\n            lines = content.splitlines(True)  # Keep all line ending characters\n            total_lines = len(lines)\n\n            # cover edge cases\n            self.current_line = self._clamp(targeted_line, 1, total_lines)\n            half_window = max(1, window // 2)\n\n            # Ensure at least one line above and below the targeted line\n            start = max(1, self.current_line - half_window)\n            end = min(total_lines, self.current_line + half_window)\n\n            # Adjust start and end to ensure at least one line above and below\n            if start == 1:\n                end = min(total_lines, start + window - 1)\n            if end == total_lines:\n                start = max(1, end - window + 1)\n\n            output = \"\"\n\n            # only display this when there's at least one line above\n            if start > 1:\n                output += f\"({start - 1} more lines above)\\n\"\n            else:\n                output += \"(this is the beginning of the file)\\n\"\n            for i in range(start, end + 1):\n                _new_line = f\"{i:03d}|{lines[i - 1]}\"\n                if not _new_line.endswith(\"\\n\"):\n                    _new_line += \"\\n\"\n                output += _new_line\n            if end < total_lines:\n                output += f\"({total_lines - end} more lines below)\\n\"\n            else:\n                output += \"(this is the end of the file)\\n\"\n            output = output.rstrip()\n\n            return output\n\n    @staticmethod\n    def _cur_file_header(current_file: Path, total_lines: int) -> str:\n        if not current_file:\n            return \"\"\n        return f\"[File: {current_file.resolve()} ({total_lines} lines total)]\\n\"\n\n    async def set_workdir(self, path: str) -> None:\n        \"\"\"\n        Sets the working directory to the given path. eg: repo directory.\n        You MUST to set it up before open the file.\n\n        Args:\n            path: str: The path to set as the working directory.\n        \"\"\"\n        self.working_dir = Path(path)\n\n    async def open_file(\n        self, path: Union[Path, str], line_number: Optional[int] = 1, context_lines: Optional[int] = None\n    ) -> str:\n        \"\"\"Opens the file at the given path in the editor. If line_number is provided, the window will be moved to include that line.\n        It only shows the first 100 lines by default! Max `context_lines` supported is 2000, use `scroll up/down`\n        to view the file if you want to see more.\n\n        Args:\n            path: str: The path to the file to open, preferred absolute path.\n            line_number: int | None = 1: The line number to move to. Defaults to 1.\n            context_lines: int | None = 100: Only shows this number of lines in the context window (usually from line 1), with line_number as the center (if possible). Defaults to 100.\n        \"\"\"\n        if context_lines is None:\n            context_lines = self.window\n\n        path = self._try_fix_path(path)\n\n        if not path.is_file():\n            raise FileNotFoundError(f\"File {path} not found\")\n\n        self.current_file = path\n\n        total_lines = await self._count_file_lines(file_path=path)\n\n        if not isinstance(line_number, int) or line_number < 1 or line_number > total_lines:\n            raise ValueError(f\"Line number must be between 1 and {total_lines}\")\n        self.current_line = line_number\n\n        # Override WINDOW with context_lines\n        if context_lines is None or context_lines < 1:\n            context_lines = self.window\n\n        output = self._cur_file_header(path, total_lines)\n        output += await self._print_window(path, self.current_line, self._clamp(context_lines, 1, 2000))\n        self.resource.report(path, \"path\")\n        return output\n\n    async def goto_line(self, line_number: int) -> str:\n        \"\"\"Moves the window to show the specified line number.\n\n        Args:\n            line_number: int: The line number to move to.\n        \"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        if not isinstance(line_number, int) or line_number < 1 or line_number > total_lines:\n            raise ValueError(f\"Line number must be between 1 and {total_lines}\")\n\n        self.current_line = self._clamp(line_number, 1, total_lines)\n\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def scroll_down(self) -> str:\n        \"\"\"Moves the window down by 100 lines.\"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        self.current_line = self._clamp(self.current_line + self.window, 1, total_lines)\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def scroll_up(self) -> str:\n        \"\"\"Moves the window up by 100 lines.\"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        self.current_line = self._clamp(self.current_line - self.window, 1, total_lines)\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def create_file(self, filename: str) -> str:\n        \"\"\"Creates and opens a new file with the given name.\n\n        Args:\n            filename: str: The name of the file to create. If the parent directory does not exist, it will be created.\n        \"\"\"\n        filename = self._try_fix_path(filename)\n\n        if filename.exists():\n            raise FileExistsError(f\"File '{filename}' already exists.\")\n        await awrite(filename, \"\\n\")\n\n        self.open_file(filename)\n        return f\"[File {filename} created.]\"\n\n    @staticmethod\n    def _append_impl(lines, content):\n        \"\"\"Internal method to handle appending to a file.\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            content: str: The content to append to the file.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n        \"\"\"\n        content_lines = content.splitlines(keepends=True)\n        n_added_lines = len(content_lines)\n        if lines and not (len(lines) == 1 and lines[0].strip() == \"\"):\n            # file is not empty\n            if not lines[-1].endswith(\"\\n\"):\n                lines[-1] += \"\\n\"\n            new_lines = lines + content_lines\n            content = \"\".join(new_lines)\n        else:\n            # file is empty\n            content = \"\".join(content_lines)\n\n        return content, n_added_lines\n\n    @staticmethod\n    def _insert_impl(lines, start, content):\n        \"\"\"Internal method to handle inserting to a file.\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            start: int: The start line number for inserting.\n            content: str: The content to insert to the file.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n\n        Raises:\n            LineNumberError: If the start line number is invalid.\n        \"\"\"\n        inserted_lines = [content + \"\\n\" if not content.endswith(\"\\n\") else content]\n        if len(lines) == 0:\n            new_lines = inserted_lines\n        elif start is not None:\n            if len(lines) == 1 and lines[0].strip() == \"\":\n                # if the file with only 1 line and that line is empty\n                lines = []\n\n            if len(lines) == 0:\n                new_lines = inserted_lines\n            else:\n                new_lines = lines[: start - 1] + inserted_lines + lines[start - 1 :]\n        else:\n            raise LineNumberError(\n                f\"Invalid line number: {start}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n\n        content = \"\".join(new_lines)\n        n_added_lines = len(inserted_lines)\n        return content, n_added_lines\n\n    @staticmethod\n    def _edit_impl(lines, start, end, content):\n        \"\"\"Internal method to handle editing a file.\n\n        REQUIRES (should be checked by caller):\n            start <= end\n            start and end are between 1 and len(lines) (inclusive)\n            content ends with a newline\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            start: int: The start line number for editing.\n            end: int: The end line number for editing.\n            content: str: The content to replace the lines with.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n        \"\"\"\n        # Handle cases where start or end are None\n        if start is None:\n            start = 1  # Default to the beginning\n        if end is None:\n            end = len(lines)  # Default to the end\n        # Check arguments\n        if not (1 <= start <= len(lines)):\n            raise LineNumberError(\n                f\"Invalid start line number: {start}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n        if not (1 <= end <= len(lines)):\n            raise LineNumberError(\n                f\"Invalid end line number: {end}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n        if start > end:\n            raise LineNumberError(f\"Invalid line range: {start}-{end}. Start must be less than or equal to end.\")\n\n        # Split content into lines and ensure it ends with a newline\n        if not content.endswith(\"\\n\"):\n            content += \"\\n\"\n        content_lines = content.splitlines(True)\n\n        # Calculate the number of lines to be added\n        n_added_lines = len(content_lines)\n\n        # Remove the specified range of lines and insert the new content\n        new_lines = lines[: start - 1] + content_lines + lines[end:]\n\n        # Handle the case where the original lines are empty\n        if len(lines) == 0:\n            new_lines = content_lines\n\n        # Join the lines to create the new content\n        content = \"\".join(new_lines)\n        return content, n_added_lines\n\n    def _get_indentation_info(self, content, first_line):\n        \"\"\"\n        The indentation of the first insert line and the previous line, along with guidance for the next attempt.\n        \"\"\"\n        content_lines = content.split(\"\\n\")\n        pre_line = content_lines[first_line - 2] if first_line - 2 >= 0 else \"\"\n        pre_line_indent = len(pre_line) - len(pre_line.lstrip())\n        insert_line = content_lines[first_line - 1]\n        insert_line_indent = len(insert_line) - len(insert_line.lstrip())\n        ret_str = INDENTATION_INFO.format(\n            pre_line=pre_line,\n            pre_line_indent=pre_line_indent,\n            insert_line=insert_line,\n            insert_line_indent=insert_line_indent,\n            sub_4_space=max(insert_line_indent - 4, 0),\n            add_4_space=insert_line_indent + 4,\n        )\n        return ret_str\n\n    async def _edit_file_impl(\n        self,\n        file_name: Path,\n        start: Optional[int] = None,\n        end: Optional[int] = None,\n        content: str = \"\",\n        is_insert: bool = False,\n        is_append: bool = False,\n    ) -> str:\n        \"\"\"Internal method to handle common logic for edit_/append_file methods.\n\n        Args:\n            file_name: Path: The name of the file to edit or append to.\n            start: int | None = None: The start line number for editing. Ignored if is_append is True.\n            end: int | None = None: The end line number for editing. Ignored if is_append is True.\n            content: str: The content to replace the lines with or to append.\n            is_insert: bool = False: Whether to insert content at the given line number instead of editing.\n            is_append: bool = False: Whether to append content to the file instead of editing.\n        \"\"\"\n\n        ERROR_MSG = f\"[Error editing file {file_name}. Please confirm the file is correct.]\"\n        ERROR_MSG_SUFFIX = (\n            \"Your changes have NOT been applied. Please fix your edit command and try again.\\n\"\n            \"You either need to 1) Open the correct file and try again or 2) Specify the correct line number arguments.\\n\"\n            \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n        )\n\n        if not self._is_valid_filename(file_name.name):\n            raise FileNotFoundError(\"Invalid file name.\")\n\n        if not self._is_valid_path(file_name):\n            raise FileNotFoundError(\"Invalid path or file name.\")\n\n        if not self._create_paths(file_name):\n            raise PermissionError(\"Could not access or create directories.\")\n\n        if not file_name.is_file():\n            raise FileNotFoundError(f\"File {file_name} not found.\")\n\n        if is_insert and is_append:\n            raise ValueError(\"Cannot insert and append at the same time.\")\n\n        # Use a temporary file to write changes\n        content = str(content or \"\")\n        temp_file_path = \"\"\n        src_abs_path = file_name.resolve()\n        first_error_line = None\n        # The file to store previous content and will be removed automatically.\n        temp_backup_file = tempfile.NamedTemporaryFile(\"w\", delete=True)\n\n        try:\n            # lint the original file\n            if self.enable_auto_lint:\n                original_lint_error, _ = self._lint_file(file_name)\n\n            # Create a temporary file\n            with tempfile.NamedTemporaryFile(\"w\", delete=False) as temp_file:\n                temp_file_path = temp_file.name\n\n                # Read the original file and check if empty and for a trailing newline\n                async with aiofiles.open(file_name, \"r\") as original_file:\n                    lines = await original_file.readlines()\n\n                if is_append:\n                    content, n_added_lines = self._append_impl(lines, content)\n                elif is_insert:\n                    try:\n                        content, n_added_lines = self._insert_impl(lines, start, content)\n                    except LineNumberError as e:\n                        return (f\"{ERROR_MSG}\\n\" f\"{e}\\n\" f\"{ERROR_MSG_SUFFIX}\") + \"\\n\"\n                else:\n                    try:\n                        content, n_added_lines = self._edit_impl(lines, start, end, content)\n                    except LineNumberError as e:\n                        return (f\"{ERROR_MSG}\\n\" f\"{e}\\n\" f\"{ERROR_MSG_SUFFIX}\") + \"\\n\"\n\n                if not content.endswith(\"\\n\"):\n                    content += \"\\n\"\n\n                # Write the new content to the temporary file\n                temp_file.write(content)\n\n            # Replace the original file with the temporary file atomically\n            shutil.move(temp_file_path, src_abs_path)\n\n            # Handle linting\n            # NOTE: we need to get env var inside this function\n            # because the env var will be set AFTER the agentskills is imported\n            if self.enable_auto_lint:\n                # BACKUP the original file\n                temp_backup_file.writelines(lines)\n                temp_backup_file.flush()\n                lint_error, first_error_line = self._lint_file(file_name)\n\n                # Select the errors caused by the modification\n                def extract_last_part(line):\n                    parts = line.split(\":\")\n                    if len(parts) > 1:\n                        return parts[-1].strip()\n                    return line.strip()\n\n                def subtract_strings(str1, str2) -> str:\n                    lines1 = str1.splitlines()\n                    lines2 = str2.splitlines()\n\n                    last_parts1 = [extract_last_part(line) for line in lines1]\n\n                    remaining_lines = [line for line in lines2 if extract_last_part(line) not in last_parts1]\n\n                    result = \"\\n\".join(remaining_lines)\n                    return result\n\n                if original_lint_error and lint_error:\n                    lint_error = subtract_strings(original_lint_error, lint_error)\n                    if lint_error == \"\":\n                        lint_error = None\n                        first_error_line = None\n\n                if lint_error is not None:\n                    # show the first insert line.\n                    if is_append:\n                        # original end-of-file\n                        show_line = len(lines)\n                    # insert OR edit WILL provide meaningful line numbers\n                    elif start is not None and end is not None:\n                        show_line = int((start + end) / 2)\n                    else:\n                        raise ValueError(\"Invalid state. This should never happen.\")\n\n                    guidance_message = self._get_indentation_info(content, start or len(lines))\n                    guidance_message += (\n                        \"You either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\\n\"\n                        \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n                    )\n                    lint_error_info = ERROR_GUIDANCE.format(\n                        linter_error_msg=LINTER_ERROR_MSG + lint_error,\n                        window_after_applied=await self._print_window(file_name, show_line, n_added_lines + 20),\n                        window_before_applied=await self._print_window(\n                            Path(temp_backup_file.name), show_line, n_added_lines + 20\n                        ),\n                        guidance_message=guidance_message,\n                    ).strip()\n\n                    # recover the original file\n                    shutil.move(temp_backup_file.name, src_abs_path)\n                    return lint_error_info\n\n        except FileNotFoundError as e:\n            return f\"File not found: {e}\\n\"\n        except IOError as e:\n            return f\"An error occurred while handling the file: {e}\\n\"\n        except ValueError as e:\n            return f\"Invalid input: {e}\\n\"\n        except Exception as e:\n            guidance_message = self._get_indentation_info(content, start or len(lines))\n            guidance_message += (\n                \"You either need to 1) Specify the correct start/end line arguments or 2) Enlarge the range of original code.\\n\"\n                \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n            )\n            error_info = ERROR_GUIDANCE.format(\n                linter_error_msg=LINTER_ERROR_MSG + str(e),\n                window_after_applied=await self._print_window(file_name, start or len(lines), 100),\n                window_before_applied=await self._print_window(Path(temp_backup_file.name), start or len(lines), 100),\n                guidance_message=guidance_message,\n            ).strip()\n            # Clean up the temporary file if an error occurs\n            shutil.move(temp_backup_file.name, src_abs_path)\n            if temp_file_path and Path(temp_file_path).exists():\n                Path(temp_file_path).unlink()\n\n            raise Exception(f\"{error_info}\") from e\n\n        # Update the file information and print the updated content\n        n_total_lines = await self._count_file_lines(file_path=file_name, encoding=\"utf-8\")\n\n        if first_error_line is not None and int(first_error_line) > 0:\n            self.current_line = first_error_line\n        else:\n            if is_append:\n                self.current_line = max(1, len(lines))  # end of original file\n            else:\n                self.current_line = start or n_total_lines or 1\n\n        success_edit_info = SUCCESS_EDIT_INFO.format(\n            file_name=file_name.resolve(),\n            n_total_lines=n_total_lines,\n            window_after_applied=await self._print_window(file_name, self.current_line, self.window),\n            line_number=self.current_line,\n        ).strip()\n\n        return success_edit_info\n\n    async def edit_file_by_replace(self, file_name: str, to_replace: str, new_content: str) -> str:\n        \"\"\"Edit a file. This will search for `to_replace` in the given file and replace it with `new_content`.\n\n        Every *to_replace* must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n        * The `to_replace` parameter should match EXACTLY one or more consecutive lines from the original file. Be mindful of whitespaces!\n        * If the `to_replace` parameter is not unique in the file, the replacement will not be performed. Make sure to include enough context in `to_replace` to make it unique\n        * The `new_content` parameter should contain the edited lines that should replace the `to_replace`\n\n        For example, given a file \"path/to/example.txt\" with the following content:\n        ```\n        line 1\n        line 2\n        line 2\n        line 3\n        ```\n\n        EDITING: If you want to replace the second occurrence of \"line 2\", you can make `to_replace` unique:\n\n        edit_file_by_replace(\n            'path/to/example.txt',\n            to_replace='line 2\\nline 3',\n            new_content='new line\\nline 3',\n        )\n\n        This will replace only the second \"line 2\" with \"new line\". The first \"line 2\" will remain unchanged.\n\n        The resulting file will be:\n        ```\n        line 1\n        line 2\n        new line\n        line 3\n        ```\n\n        REMOVAL: If you want to remove \"line 2\" and \"line 3\", you can set `new_content` to an empty string:\n\n        edit_file_by_replace(\n            'path/to/example.txt',\n            to_replace='line 2\\nline 3',\n            new_content='',\n        )\n        \"\"\"\n        # search for `to_replace` in the file\n        # if found, replace it with `new_content`\n        # if not found, perform a fuzzy search to find the closest match and replace it with `new_content`\n        file_name = self._try_fix_path(file_name)\n\n        async with aiofiles.open(file_name, \"r\") as file:\n            file_content = await file.read()\n\n        if to_replace.strip() == \"\":\n            if file_content.strip() == \"\":\n                raise ValueError(f\"The file '{file_name}' is empty. Please use the append method to add content.\")\n            raise ValueError(\"`to_replace` must not be empty.\")\n\n        to_replace = self._rm_line_no(to_replace)\n        new_content = self._rm_line_no(new_content)\n\n        if file_content.count(to_replace) > 1:\n            raise ValueError(\n                f\"`to_replace` ```{to_replace}``` appears more than once, please include enough lines to make code in `to_replace` unique.\"\n            )\n        elif file_content.count(to_replace) == 0:\n            # If no exact match, try to find close matches using LCS\n            if len(to_replace) > 20:\n                # white spaces are usually missing in multi-line content, thus hard coding a threshold of 20\n                _, similarity, file_content_match = self.lcs(to_replace, file_content)\n                if similarity > 0.9:  # High similarity suggests missing whitespace\n                    raise ValueError(\n                        f\"`to_replace` ```{to_replace}``` not found in {file_name}. Pay attention to spaces and line breaks! Did you actually mean ```{file_content_match}```\"\n                    )\n            raise ValueError(\n                f\"`to_replace` ```{to_replace}``` not found in {file_name}. Read the file carefully with Editor.read and make sure you give the right content to replace. If you want to insert new content, use Editor.insert_content_at_line instead.\"\n            )\n\n        ### Take a easy way to replace the content with direct string operation. Also disable linting for now ###\n        ### TODO: unittest to cover this part ###\n        new_content = file_content.replace(to_replace, new_content)\n        async with aiofiles.open(file_name, \"w\") as file:\n            await file.write(new_content)\n\n        start = file_content.find(to_replace) + 1  # index of starting char\n        start_line_number = file_content[:start].count(\"\\n\") + 1\n        window_after_applied = await self._print_window(file_name, start_line_number, 30)\n        success_edit_info = SUCCESS_EDIT_INFO.format(\n            file_name=file_name.resolve(),\n            n_total_lines=\"\",\n            window_after_applied=window_after_applied,\n            line_number=start_line_number,\n        ).strip()\n        self.resource.report(file_name, \"path\")\n        return success_edit_info\n\n    async def insert_content_at_line(self, file_name: str, line_number: int, insert_content: str) -> str:\n        \"\"\"Insert a complete block of code before the given line number in a file. That is, the new content will start at the beginning of the specified line, and the existing content of that line will be moved down.\n        This operation will NOT modify the content of the lines before or after the given line number.\n        This function can not insert content the end of the file. Please use append_file instead,\n        For example, if the file has the following content:\n        ```\n        001|contain g\n        002|contain h\n        003|contain i\n        004|contain j\n        ```\n        and you call\n        insert_content_at_line(\n            file_name='file.txt',\n            line_number=2,\n            insert_content='new line'\n        )\n        the file will be updated to:\n        ```\n        001|contain g\n        002|new line\n        003|contain h\n        004|contain i\n        005|contain j\n        ```\n\n        Args:\n            file_name: (str): The name of the file to edit.\n            line_number (int): The line number (starting from 1) to insert the content after. The insert content will be add between the line of line_number-1 and line_number\n            insert_content (str): The content to insert betweed the previous_line_content and current_line_content.The insert_content must be a complete block of code at.\n        \"\"\"\n        file_name = self._try_fix_path(file_name)\n        ret_str = await self._edit_file_impl(\n            file_name,\n            start=line_number,\n            end=line_number,\n            content=insert_content,\n            is_insert=True,\n            is_append=False,\n        )\n        self.resource.report(file_name, \"path\")\n        return ret_str\n\n    async def append_file(self, file_name: str, content: str) -> str:\n        \"\"\"Append content to the given file.\n        It appends text `content` to the end of the specified file.\n\n        Args:\n            file_name: str: The name of the file to edit.\n            content: str: The content to insert.\n        \"\"\"\n        file_name = self._try_fix_path(file_name)\n        ret_str = await self._edit_file_impl(\n            file_name,\n            start=None,\n            end=None,\n            content=content,\n            is_insert=False,\n            is_append=True,\n        )\n        self.resource.report(file_name, \"path\")\n        return ret_str\n\n    async def search_dir(self, search_term: str, dir_path: str = \"./\") -> str:\n        \"\"\"Searches for search_term in all files in dir. If dir is not provided, searches in the current directory.\n\n        Args:\n            search_term: str: The term to search for.\n            dir_path: str: The path to the directory to search.\n        \"\"\"\n        dir_path = self._try_fix_path(dir_path)\n        if not dir_path.is_dir():\n            raise FileNotFoundError(f\"Directory {dir_path} not found\")\n\n        matches = []\n        for root, _, files in os.walk(dir_path):\n            for file in files:\n                if file.startswith(\".\"):\n                    continue\n                file_path = Path(root) / file\n                try:\n                    async with aiofiles.open(file_path, \"r\", errors=\"ignore\") as f:\n                        line_num = 0\n                        async for line in f:\n                            line_num += 1\n                            if search_term in line:\n                                matches.append((file_path, line_num, line.strip()))\n                except Exception:\n                    # Skip files that can't be read\n                    continue\n\n        if not matches:\n            return f'No matches found for \"{search_term}\" in {dir_path}'\n\n        num_matches = len(matches)\n        num_files = len(set(match[0] for match in matches))\n\n        if num_files > 100:\n            return f'More than {num_files} files matched for \"{search_term}\" in {dir_path}. Please narrow your search.'\n\n        res_list = [f'[Found {num_matches} matches for \"{search_term}\" in {dir_path}]']\n        for file_path, line_num, line in matches:\n            res_list.append(f\"{file_path} (Line {line_num}): {line}\")\n        res_list.append(f'[End of matches for \"{search_term}\" in {dir_path}]')\n        return \"\\n\".join(res_list)\n\n    async def search_file(self, search_term: str, file_path: Optional[str] = None) -> str:\n        \"\"\"Searches for search_term in file. If file is not provided, searches in the current open file.\n\n        Args:\n            search_term: str: The term to search for.\n            file_path: str | None: The path to the file to search.\n        \"\"\"\n        if file_path is None:\n            file_path = self.current_file\n        else:\n            file_path = self._try_fix_path(file_path)\n        if file_path is None:\n            raise FileNotFoundError(\"No file specified or open. Use the open_file function first.\")\n        if not file_path.is_file():\n            raise FileNotFoundError(f\"File {file_path} not found\")\n\n        matches = []\n        async with aiofiles.open(file_path, \"r\") as file:\n            i = 0\n            async for line in file:\n                i += 1\n                if search_term in line:\n                    matches.append((i, line.strip()))\n\n        res_list = []\n        if matches:\n            res_list.append(f'[Found {len(matches)} matches for \"{search_term}\" in {file_path}]')\n            for match in matches:\n                res_list.append(f\"Line {match[0]}: {match[1]}\")\n            res_list.append(f'[End of matches for \"{search_term}\" in {file_path}]')\n        else:\n            res_list.append(f'[No matches found for \"{search_term}\" in {file_path}]')\n\n        extra = {\"type\": \"search\", \"symbol\": search_term, \"lines\": [i[0] - 1 for i in matches]} if matches else None\n        self.resource.report(file_path, \"path\", extra=extra)\n        return \"\\n\".join(res_list)\n\n    async def find_file(self, file_name: str, dir_path: str = \"./\") -> str:\n        \"\"\"Finds all files with the given name or path in the specified directory.\n\n        Args:\n            file_name: str: The name or path of the file to find.\n                           If it's an absolute path, will match the exact path.\n                           If it's a relative path, will match the path suffix.\n                           If it's just a name, will match file names.\n            dir_path: str: The path to the directory to search.\n        \"\"\"\n        dir_path = self._try_fix_path(dir_path)\n        if not dir_path.is_dir():\n            raise FileNotFoundError(f\"Directory {dir_path} not found\")\n\n        file_path = Path(file_name)\n\n        # TODO: use asyncio.gather to concurrently check multiple paths\n        matches = []\n        for root, _, files in os.walk(dir_path):\n            for file in files:\n                current_path = Path(root) / file\n\n                if file_path.is_absolute():\n                    if current_path == file_path:\n                        matches.append(current_path)\n                elif \"/\" in str(file_path) or \"\\\\\" in str(file_path):\n                    if str(current_path).endswith(str(file_path)):\n                        matches.append(current_path)\n                else:\n                    if current_path.name == file_name:\n                        matches.append(current_path)\n\n        res_list = []\n        if matches:\n            res_list.append(f'[Found {len(matches)} matches for \"{file_name}\" in {dir_path}]')\n            for match in matches:\n                res_list.append(f\"{match}\")\n            res_list.append(f'[End of matches for \"{file_name}\" in {dir_path}]')\n        else:\n            res_list.append(f'[No matches found for \"{file_name}\" in {dir_path}]')\n        return \"\\n\".join(res_list)\n\n    def _try_fix_path(self, path: Union[Path, str]) -> Path:\n        \"\"\"Tries to fix the path if it is not absolute.\"\"\"\n        if not isinstance(path, Path):\n            path = Path(path)\n        if not path.is_absolute():\n            path = self.working_dir / path\n        return path\n\n    @staticmethod\n    def _rm_line_no(content: str) -> str:\n        line_no_pattern = \"\\n[0-9]{3}\\|\"\n        if re.search(line_no_pattern, content):\n            logger.warning(\"Line numbers detected in content and will be removed\")\n            content = re.sub(line_no_pattern, \"\\n\", content)\n        return content\n\n    @staticmethod\n    async def similarity_search(query: str, path: Union[str, Path]) -> List[str]:\n        \"\"\"Given a filename or a pathname, performs a similarity search for a given query across the specified file or path.\n\n        This method searches the index repository for the provided query, classifying the specified\n        files or paths. It performs a search on each cluster of files and handles non-indexed files\n        separately, merging results from structured indices with any direct results from non-indexed files.\n        This function call does not depend on other functions.\n\n        Args:\n            query (str): The search query string to look for in the indexed files.\n            path (Union[str, Path]): A pathname or filename to search within.\n\n        Returns:\n            List[str]: A list of results as strings, containing the text from the merged results\n                        and any direct results from non-indexed files.\n\n        Example:\n            >>> query = \"The problem to be analyzed from the document\"\n            >>> file_or_path = \"The pathname or filename you want to search within\"\n            >>> texts: List[str] = await Editor.similarity_search(query=query, path=file_or_path)\n            >>> print(texts)\n        \"\"\"\n        return await IndexRepo.cross_repo_search(query=query, file_or_path=path)\n\n    @staticmethod\n    def is_large_file(content: str, mix_token_count: int = 0) -> bool:\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n        token_count = len(encoding.encode(content))\n        mix_token_count = mix_token_count or DEFAULT_MIN_TOKEN_COUNT\n        return token_count >= mix_token_count\n\n    @staticmethod\n    def lcs(s1: str, s2: str) -> tuple[str, float, str]:\n        \"\"\"Find the longest common subsequence between two strings and the matching section from s2. Assuming s1 is always a subsequence of s2.\n\n        Args:\n            s1: First string (typically the to_replace content)\n            s2: Second string (typically the file content)\n\n        Returns:\n            tuple[str, float, str]: The LCS string, its similarity score (0-1), and the matching section from s2\n        \"\"\"\n        # Create DP table\n        m, n = len(s1), len(s2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n\n        # Fill DP table\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if s1[i - 1] == s2[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n\n        # Reconstruct LCS and track positions in s2\n        lcs = []\n        i, j = m, n\n        positions = []  # Store positions of matches in s2\n        while i > 0 and j > 0:\n            if s1[i - 1] == s2[j - 1]:\n                lcs.append(s1[i - 1])\n                positions.append(j - 1)\n                i -= 1\n                j -= 1\n            elif dp[i - 1][j] > dp[i][j - 1]:\n                i -= 1\n            else:\n                j -= 1\n\n        lcs = \"\".join(reversed(lcs))\n        positions = list(reversed(positions))\n\n        # Calculate similarity score\n        score = len(lcs) / len(s1)\n\n        # Extract the matching section from s2\n        if positions:\n            start = min(positions)\n            end = max(positions) + 1\n            matching_section = s2[start:end]\n        else:\n            matching_section = \"\"\n        matching_section = matching_section[\n            : int(1.2 * len(s1))\n        ]  # hard truncate to avoid greedily match too long string\n\n        return lcs, score, matching_section\n\n    async def _count_file_lines(self, file_path: Union[Path, str], encoding: Optional[str] = None) -> int:\n        \"\"\"Asynchronously count the total number of lines in a file.\n\n        Args:\n            file_path: Path to the file.\n            encoding: The encoding used to open the file. Defaults to None (system default encoding).\n\n        Returns:\n            int: Total number of lines in the file, minimum 1.\n        \"\"\"\n        total_lines = 0\n        async with aiofiles.open(file_path, \"r\", encoding=encoding) as file:\n            async for _ in file:\n                total_lines += 1\n        return max(1, total_lines)\n\n    async def remove_file(self, file_path: Union[Path, str], missing_ok: bool = True) -> str:\n        \"\"\"Remove a file from the file system.\n\n        Args:\n            file_path: Path to the file to remove.\n            missing_ok: If True, don't raise error if file doesn't exist.\n        \"\"\"\n        path = self._try_fix_path(file_path)\n        path.unlink(missing_ok=missing_ok)\n        await self.resource.async_report(file_path, \"delete\")\n        return f\"File {path} removed successfully\"\n",
                        "tags": []
                    },
                    "Terminal": {
                        "name": "Terminal",
                        "path": "metagpt/tools/libs/terminal.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool for running terminal commands. Don't initialize a new instance of this class if one already exists.",
                            "methods": {
                                "run": {
                                    "type": "async_function",
                                    "description": "Executes a specified command in the terminal and streams the output back in real time. ",
                                    "signature": "(self, cmd: str) -> str",
                                    "parameters": "Args: cmd (str): The command to execute in the terminal. Returns: str: The command's output."
                                }
                            },
                            "tool_path": "metagpt/tools/libs/terminal.py"
                        },
                        "code": "@register_tool(include_functions=[\"run\"])\nclass Terminal(BaseModel):\n    \"\"\"A tool for running terminal commands. Don't initialize a new instance of this class if one already exists.\"\"\"\n\n    tabs: dict[str, Tab] = {}\n    current_tab_id: str = \"\"\n    current_tab: Optional[Tab] = Field(None, exclude=True)\n    #  The cmd in forbidden_terminal_commands will be replace by pass ana return the advise. example:{\"cmd\":\"forbidden_reason/advice\"}\n    forbidden_commands: dict[str, str] = {\n        # \"run dev\": \"Use Deployer.deploy_to_public instead.\",\n        \"run preview\": \"Use Deployer.deploy_to_public instead.\",\n        # serve cmd have a space behind it,\n        \"serve \": \"Use Deployer.deploy_to_public instead.\",\n        \"python -m http.server\": \"Use python -u -m http.server port_number instead.\",  # force buffer flush for http.server\n    }\n    timeout: float = 300.0  # timeout for reading output\n    working_dir: str = Field(default_factory=lambda: str(DEFAULT_WORKSPACE_ROOT.absolute()))\n\n    @model_validator(mode=\"after\")\n    def valid_current_tab(self):\n        if self.current_tab_id:\n            self.current_tab = self.tabs[self.current_tab_id]\n        return self\n\n    async def switch_tab(self, tab_id: str = \"\") -> str:\n        \"\"\"Switch tab based on tab_id. Useful for checking out new output from detached tabs or typing on desired tabs.\"\"\"\n        # NOTE: Hide from agent for now (not registered), need to solve the echo END_MARKER_VALUE problem\n        if tab_id in self.tabs:\n            self.current_tab = self.tabs[tab_id]\n            self.current_tab_id = tab_id\n            tab_new_output = self.current_tab.read_background_output()\n            return f\"Switched to tab {tab_id}, pwd is {self.current_tab.cwd}, the tab has new output: {tab_new_output}\"\n        return f\"Tab {tab_id} not found, created tabs are {list(self.tabs.keys())}\"\n\n    async def _create_new_tab(self) -> str:\n        \"\"\"create a new tab and switch to it\"\"\"\n        new_tab_id = f\"{len(self.tabs):02}\"\n        new_tab = Tab(tab_id=new_tab_id, cwd=self.cwd)\n        await new_tab.start()\n        self.tabs.update({new_tab_id: new_tab})\n        switch_tab_info = await self.switch_tab(new_tab_id)\n        return f\"Tab {new_tab_id} created. \" + switch_tab_info\n\n    @property\n    def cwd(self):\n        return self.current_tab.cwd if self.current_tab else self.working_dir\n\n    async def get_cwd(self):\n        return self.cwd\n\n    async def run(self, cmd: str) -> str:\n        \"\"\"\n        Executes a specified command in the terminal and streams the output back in real time.\n\n        Args:\n            cmd (str): The command to execute in the terminal.\n\n        Returns:\n            str: The command's output.\n        \"\"\"\n        if not self.current_tab:\n            await self._create_new_tab()\n\n        output = \"\"\n        # Precheck for forbidden commands\n        commands = re.split(r\"\\s*&&\\s*\", cmd)\n        for cmd_name, reason in self.forbidden_commands.items():\n            for command in commands:\n                if cmd_name in command:\n                    output += f\"{command} is forbidden. {reason}\\n\"\n        if output:\n            output = f\"Command not executed due to forbidden syntax:\\n{output}\\nPlease rewrite the whole command for execution.\"\n            return output\n\n        cmd = \" && \".join(commands)\n\n        # Send the command\n        current_tab = self.current_tab\n        if current_tab.is_running:\n            return (\n                \"Cannot execute the command because a previous one is still running in the current terminal tab\"\n                f\" (ID: {self.current_tab_id}).\"\n            )\n\n        # clear the output queue before execute command\n        output_queue = current_tab.output_queue\n        while not output_queue.empty():\n            output_queue.get_nowait()\n\n        await current_tab.execute(cmd)\n\n        tmp = []\n        is_service_flag = False\n        while True:\n            try:\n                # shorter timeout for service process by detecting output patterns such as localhost:port, ip:port\n                timeout = self.timeout if not is_service_flag else 3\n                line = await asyncio.wait_for(output_queue.get(), timeout=timeout)\n                if line is None:\n                    # ensure the read_and_process_output task is done\n                    await current_tab.task\n                    break\n                is_service_flag = is_service_flag or is_service_process(line)  # if True already, skip checking\n                tmp.append(line)\n            except asyncio.TimeoutError:\n                output_so_far = \"\".join(tmp)\n                if (returncode := current_tab.process.returncode) is not None:\n                    msg = f\"The terminal has exited with code {returncode}\"\n                    logger.warning(msg)\n                    return f\"{msg}, currently with output: {output_so_far}\"\n\n                current_tab.update_cwd()  # update cwd for the command still running\n                logger.info(f\"No more output after {timeout}s, detached from current tab and switched to a new tab\")\n\n                detached_tab_id = self.current_tab_id\n                new_tab_info = await self._create_new_tab()\n                instruction = DETACH_PROMPT.format(\n                    detached_tab_id=detached_tab_id,\n                    output_so_far=output_so_far,\n                    new_tab_info=new_tab_info,\n                )\n                # print(instruction)\n                return instruction\n        return \"\".join(tmp)\n\n    async def preview(self, tab_id: str, port: int, proj_name: str) -> str:\n        \"\"\"Preview a web project by forwarding a local port to public. Specify the id of the tab that runs the service, which is usually not the current tab but some detached tab.\"\"\"\n        if tab_id not in self.tabs:\n            return f\"Tab {tab_id} not found, created tabs are {list(self.tabs.keys())}, specify the correct tab_id that runs the service.\"\n        return await self.tabs[tab_id].preview(port, proj_name)\n\n    async def execute_in_conda_env(self, cmd: str, env) -> str:\n        \"\"\"\n        Executes a given command within a specified Conda environment automatically without\n        the need for manual activation. Users just need to provide the name of the Conda\n        environment and the command to execute.\n\n        Args:\n            cmd (str): The command to execute within the Conda environment.\n            env (str, optional): The name of the Conda environment to activate before executing the command.\n                                 If not specified, the command will run in the current active environment.\n\n        Returns:\n            str: The command's output, or an empty string if `daemon` is True, with output processed\n                 asynchronously in that case.\n\n        Note:\n            This function wraps `run`, prepending the necessary Conda activation commands\n            to ensure the specified environment is active for the command's execution.\n        \"\"\"\n        cmd = f\"conda run -n {env} {cmd}\"\n        return await self.run(cmd)\n\n    @handle_exception\n    async def close(self):\n        for tab in self.tabs.values():\n            await tab.close()\n        self.tabs = {}\n\n    @handle_exception\n    def __del__(self):\n        \"\"\"Close tabls when Terminal instance is destroyed\"\"\"\n        if self.tabs:\n            loop = asyncio.get_running_loop()\n            loop.create_task(self.close())\n",
                        "tags": []
                    },
                    "SearchEnhancedQA": {
                        "name": "SearchEnhancedQA",
                        "path": "metagpt/actions/search_enhanced_qa.py",
                        "schemas": {
                            "type": "class",
                            "description": "Question answering and info searching through search engine.",
                            "methods": {
                                "run": {
                                    "type": "async_function",
                                    "description": "Answer a query by leveraging web search results. ",
                                    "signature": "(self, query: 'str', rewrite_query: 'bool' = True) -> 'str'",
                                    "parameters": "Args: query (str): The original user query. rewrite_query (bool): Whether to rewrite the query for better web search results. Defaults to True. Returns: str: A detailed answer based on web search results. Raises: ValueError: If the query is invalid."
                                }
                            },
                            "tool_path": "metagpt/actions/search_enhanced_qa.py"
                        },
                        "code": "@register_tool(include_functions=[\"run\"])\nclass SearchEnhancedQA(Action):\n    \"\"\"Question answering and info searching through search engine.\"\"\"\n\n    name: str = \"SearchEnhancedQA\"\n    desc: str = \"Integrating search engine results to anwser the question.\"\n\n    collect_links_action: CollectLinks = Field(\n        default_factory=CollectLinks, description=\"Action to collect relevant links from a search engine.\"\n    )\n    web_browse_and_summarize_action: WebBrowseAndSummarize = Field(\n        default=None,\n        description=\"Action to explore the web and provide summaries of articles and webpages.\",\n    )\n    per_page_timeout: float = Field(\n        default=15, description=\"The maximum time for fetching a single page is in seconds. Defaults to 15s.\"\n    )\n    java_script_enabled: bool = Field(\n        default=False, description=\"Whether or not to enable JavaScript in the web browser context. Defaults to False.\"\n    )\n    user_agent: str = Field(\n        default=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.81\",\n        description=\"Specific user agent to use in browser.\",\n    )\n    extra_http_headers: dict = Field(\n        default={\"sec-ch-ua\": 'Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24'},\n        description=\"An object containing additional HTTP headers to be sent with every request.\",\n    )\n    max_chars_per_webpage_summary: int = Field(\n        default=4000, description=\"Maximum summary length for each web page content.\"\n    )\n    max_search_results: int = Field(\n        default=15,\n        description=\"Maximum number of search results (links) to collect using the collect_links_action. This controls the number of potential sources for answering the question.\",\n    )\n\n    _reporter: ThoughtReporter = PrivateAttr(ThoughtReporter())\n\n    @model_validator(mode=\"after\")\n    def initialize(self):\n        if self.web_browse_and_summarize_action is None:\n            web_browser_engine = WebBrowserEngine.from_browser_config(\n                self.config.browser,\n                proxy=self.config.proxy,\n                java_script_enabled=self.java_script_enabled,\n                extra_http_headers=self.extra_http_headers,\n                user_agent=self.user_agent,\n            )\n\n            self.web_browse_and_summarize_action = WebBrowseAndSummarize(web_browser_engine=web_browser_engine)\n\n        return self\n\n    async def run(self, query: str, rewrite_query: bool = True) -> str:\n        \"\"\"Answer a query by leveraging web search results.\n\n        Args:\n            query (str): The original user query.\n            rewrite_query (bool): Whether to rewrite the query for better web search results. Defaults to True.\n\n        Returns:\n            str: A detailed answer based on web search results.\n\n        Raises:\n            ValueError: If the query is invalid.\n        \"\"\"\n        async with self._reporter:\n            await self._reporter.async_report({\"type\": \"search\", \"stage\": \"init\"})\n            self._validate_query(query)\n\n            processed_query = await self._process_query(query, rewrite_query)\n            context = await self._build_context(processed_query)\n\n            return await self._generate_answer(processed_query, context)\n\n    def _validate_query(self, query: str) -> None:\n        \"\"\"Validate the input query.\n\n        Args:\n            query (str): The query to validate.\n\n        Raises:\n            ValueError: If the query is invalid.\n        \"\"\"\n\n        if not query.strip():\n            raise ValueError(\"Query cannot be empty or contain only whitespace.\")\n\n    async def _process_query(self, query: str, should_rewrite: bool) -> str:\n        \"\"\"Process the query, optionally rewriting it.\"\"\"\n\n        if should_rewrite:\n            return await self._rewrite_query(query)\n\n        return query\n\n    async def _rewrite_query(self, query: str) -> str:\n        \"\"\"Write a better search query for web search engine.\n\n        If the rewrite process fails, the original query is returned.\n\n        Args:\n            query (str): The original search query.\n\n        Returns:\n            str: The rewritten query if successful, otherwise the original query.\n        \"\"\"\n\n        prompt = REWRITE_QUERY_PROMPT.format(q=query)\n\n        try:\n            resp = await self._aask(prompt)\n            rewritten_query = self._extract_rewritten_query(resp)\n\n            logger.info(f\"Query rewritten: '{query}' -> '{rewritten_query}'\")\n            return rewritten_query\n        except Exception as e:\n            logger.warning(f\"Query rewrite failed. Returning original query. Error: {e}\")\n            return query\n\n    def _extract_rewritten_query(self, response: str) -> str:\n        \"\"\"Extract the rewritten query from the LLM's JSON response.\"\"\"\n\n        resp_json = json.loads(CodeParser.parse_code(response, lang=\"json\"))\n        return resp_json[\"query\"]\n\n    async def _build_context(self, query: str) -> str:\n        \"\"\"Construct a context string from web search citations.\n\n        Args:\n            query (str): The search query.\n\n        Returns:\n            str: Formatted context with numbered citations.\n        \"\"\"\n\n        citations = await self._search_citations(query)\n        context = \"\\n\\n\".join([f\"[[citation:{i+1}]] {c}\" for i, c in enumerate(citations)])\n\n        return context\n\n    async def _search_citations(self, query: str) -> list[str]:\n        \"\"\"Perform web search and summarize relevant content.\n\n        Args:\n            query (str): The search query.\n\n        Returns:\n            list[str]: Summaries of relevant web content.\n        \"\"\"\n\n        relevant_urls = await self._collect_relevant_links(query)\n        await self._reporter.async_report({\"type\": \"search\", \"stage\": \"searching\", \"urls\": relevant_urls})\n        if not relevant_urls:\n            logger.warning(f\"No relevant URLs found for query: {query}\")\n            return []\n\n        logger.info(f\"The Relevant links are: {relevant_urls}\")\n\n        web_summaries = await self._summarize_web_content(relevant_urls)\n        if not web_summaries:\n            logger.warning(f\"No summaries generated for query: {query}\")\n            return []\n\n        citations = list(web_summaries.values())\n\n        return citations\n\n    async def _collect_relevant_links(self, query: str) -> list[str]:\n        \"\"\"Search and rank URLs relevant to the query.\n\n        Args:\n            query (str): The search query.\n\n        Returns:\n            list[str]: Ranked list of relevant URLs.\n        \"\"\"\n\n        return await self.collect_links_action._search_and_rank_urls(\n            topic=query, query=query, max_num_results=self.max_search_results\n        )\n\n    async def _summarize_web_content(self, urls: list[str]) -> dict[str, str]:\n        \"\"\"Fetch and summarize content from given URLs.\n\n        Args:\n            urls (list[str]): List of URLs to summarize.\n\n        Returns:\n            dict[str, str]: Mapping of URLs to their summaries.\n        \"\"\"\n\n        contents = await self._fetch_web_contents(urls)\n        logger.info(\"Fetching web contents done\")\n\n        summaries = {}\n        await self._reporter.async_report(\n            {\"type\": \"search\", \"stage\": \"browsing\", \"pages\": [i.model_dump() for i in contents]}\n        )\n        for content in contents:\n            url = content.url\n            inner_text = content.inner_text.replace(\"\\n\", \"\")\n            if self.web_browse_and_summarize_action._is_content_invalid(inner_text):\n                logger.warning(f\"Invalid content detected for URL {url}: {inner_text[:10]}...\")\n                continue\n\n            summary = inner_text[: self.max_chars_per_webpage_summary]\n            summaries[url] = summary\n\n        return summaries\n\n    async def _fetch_web_contents(self, urls: list[str]) -> list[WebPage]:\n        return await self.web_browse_and_summarize_action._fetch_web_contents(\n            *urls, per_page_timeout=self.per_page_timeout\n        )\n\n    async def _generate_answer(self, query: str, context: str) -> str:\n        \"\"\"Generate an answer using the query and context.\n\n        Args:\n            query (str): The user's question.\n            context (str): Relevant information from web search.\n\n        Returns:\n            str: Generated answer based on the context.\n        \"\"\"\n\n        system_prompt = SEARCH_ENHANCED_QA_SYSTEM_PROMPT.format(context=context)\n\n        async with ThoughtReporter(uuid=self._reporter.uuid, enable_llm_stream=True) as reporter:\n            await reporter.async_report({\"type\": \"search\", \"stage\": \"answer\"})\n            rsp = await self._aask(query, [system_prompt])\n        return rsp\n",
                        "tags": []
                    }
                },
                "force": true
            },
            "special_tool_commands": [
                "Plan.finish_current_task",
                "end",
                "Terminal.run",
                "RoleZero.ask_human"
            ],
            "editor": {
                "resource": {
                    "block": "Editor",
                    "uuid": "97aad5e8-9fa3-46bd-b6f2-ccd0778a4e14",
                    "enable_llm_stream": false,
                    "callback_url": ""
                },
                "current_file": null,
                "current_line": 1,
                "window": 50,
                "enable_auto_lint": true,
                "working_dir": "/workspace"
            },
            "browser": {
                "rp_meta": {
                    "uuid": "7e325b9b160443baaa362003318dcfd6",
                    "storage_dir": "/data/chats/28286616f10743a6baaf2bd913b64f39/workspace/.MGXTools",
                    "user_id": 644050,
                    "chat_id": "28286616f10743a6baaf2bd913b64f39",
                    "role_name": "David"
                }
            },
            "terminal": {
                "rp_meta": {
                    "uuid": "0edf8b24813e45a0b84397959abbb7c3",
                    "storage_dir": "/data/chats/28286616f10743a6baaf2bd913b64f39/workspace/.MGXTools",
                    "user_id": 644050,
                    "chat_id": "28286616f10743a6baaf2bd913b64f39",
                    "role_name": "David"
                }
            },
            "working_dir": "/workspace/",
            "command_rsp": "",
            "commands": [],
            "memory_k": 30,
            "use_fixed_sop": false,
            "use_summary": true,
            "browser_is_active": false,
            "custom_tools": [
                "web scraping",
                "Terminal",
                "Editor:write,read,similarity_search"
            ],
            "custom_tool_recommender": {
                "tools": {
                    "view_page_element_to_scrape": {
                        "name": "view_page_element_to_scrape",
                        "path": "metagpt/tools/libs/web_scraping.py",
                        "schemas": {
                            "type": "async_function",
                            "description": "view the HTML content of current page to understand the structure. ",
                            "signature": "(url: str, requirement: str, keep_links: bool = False) -> str",
                            "parameters": "Args: url (str): The URL of the web page to scrape. requirement (str): Providing a clear and detailed requirement helps in focusing the inspection on the desired elements. keep_links (bool): Whether to keep the hyperlinks in the HTML content. Set to True if links are required Returns: str: The HTML content of the page.",
                            "tool_path": "metagpt/tools/libs/web_scraping.py"
                        },
                        "code": "@register_tool(tags=[\"web scraping\"])\nasync def view_page_element_to_scrape(url: str, requirement: str, keep_links: bool = False) -> str:\n    \"\"\"view the HTML content of current page to understand the structure.\n\n    Args:\n        url (str): The URL of the web page to scrape.\n        requirement (str): Providing a clear and detailed requirement helps in focusing the inspection on the desired elements.\n        keep_links (bool): Whether to keep the hyperlinks in the HTML content. Set to True if links are required\n    Returns:\n        str: The HTML content of the page.\n    \"\"\"\n    async with Browser() as browser:\n        await browser.goto(url)\n        page = browser.page\n        html = await page.content()\n        html = simplify_html(html, url=page.url, keep_links=keep_links)\n    mem_fs = MemoryFileSystem()\n    filename = f\"{uuid4().hex}.html\"\n    with mem_fs.open(filename, \"w\") as f:\n        f.write(html)\n\n    # Since RAG is an optional optimization, if it fails, the simplified HTML can be used as a fallback.\n    with contextlib.suppress(Exception):\n        from metagpt.rag.engines import SimpleEngine  # avoid circular import\n\n        # TODO make `from_docs` asynchronous\n        engine = SimpleEngine.from_docs(input_files=[filename], fs=mem_fs)\n        nodes = await engine.aretrieve(requirement)\n        html = \"\\n\".join(i.text for i in nodes)\n\n    mem_fs.rm_file(filename)\n    return html\n",
                        "tags": [
                            "web scraping"
                        ]
                    },
                    "Terminal": {
                        "name": "Terminal",
                        "path": "metagpt/tools/libs/terminal.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool for running terminal commands. Don't initialize a new instance of this class if one already exists.",
                            "methods": {
                                "run": {
                                    "type": "async_function",
                                    "description": "Executes a specified command in the terminal and streams the output back in real time. ",
                                    "signature": "(self, cmd: str) -> str",
                                    "parameters": "Args: cmd (str): The command to execute in the terminal. Returns: str: The command's output."
                                }
                            },
                            "tool_path": "metagpt/tools/libs/terminal.py"
                        },
                        "code": "@register_tool(include_functions=[\"run\"])\nclass Terminal(BaseModel):\n    \"\"\"A tool for running terminal commands. Don't initialize a new instance of this class if one already exists.\"\"\"\n\n    tabs: dict[str, Tab] = {}\n    current_tab_id: str = \"\"\n    current_tab: Optional[Tab] = Field(None, exclude=True)\n    #  The cmd in forbidden_terminal_commands will be replace by pass ana return the advise. example:{\"cmd\":\"forbidden_reason/advice\"}\n    forbidden_commands: dict[str, str] = {\n        # \"run dev\": \"Use Deployer.deploy_to_public instead.\",\n        \"run preview\": \"Use Deployer.deploy_to_public instead.\",\n        # serve cmd have a space behind it,\n        \"serve \": \"Use Deployer.deploy_to_public instead.\",\n        \"python -m http.server\": \"Use python -u -m http.server port_number instead.\",  # force buffer flush for http.server\n    }\n    timeout: float = 300.0  # timeout for reading output\n    working_dir: str = Field(default_factory=lambda: str(DEFAULT_WORKSPACE_ROOT.absolute()))\n\n    @model_validator(mode=\"after\")\n    def valid_current_tab(self):\n        if self.current_tab_id:\n            self.current_tab = self.tabs[self.current_tab_id]\n        return self\n\n    async def switch_tab(self, tab_id: str = \"\") -> str:\n        \"\"\"Switch tab based on tab_id. Useful for checking out new output from detached tabs or typing on desired tabs.\"\"\"\n        # NOTE: Hide from agent for now (not registered), need to solve the echo END_MARKER_VALUE problem\n        if tab_id in self.tabs:\n            self.current_tab = self.tabs[tab_id]\n            self.current_tab_id = tab_id\n            tab_new_output = self.current_tab.read_background_output()\n            return f\"Switched to tab {tab_id}, pwd is {self.current_tab.cwd}, the tab has new output: {tab_new_output}\"\n        return f\"Tab {tab_id} not found, created tabs are {list(self.tabs.keys())}\"\n\n    async def _create_new_tab(self) -> str:\n        \"\"\"create a new tab and switch to it\"\"\"\n        new_tab_id = f\"{len(self.tabs):02}\"\n        new_tab = Tab(tab_id=new_tab_id, cwd=self.cwd)\n        await new_tab.start()\n        self.tabs.update({new_tab_id: new_tab})\n        switch_tab_info = await self.switch_tab(new_tab_id)\n        return f\"Tab {new_tab_id} created. \" + switch_tab_info\n\n    @property\n    def cwd(self):\n        return self.current_tab.cwd if self.current_tab else self.working_dir\n\n    async def get_cwd(self):\n        return self.cwd\n\n    async def run(self, cmd: str) -> str:\n        \"\"\"\n        Executes a specified command in the terminal and streams the output back in real time.\n\n        Args:\n            cmd (str): The command to execute in the terminal.\n\n        Returns:\n            str: The command's output.\n        \"\"\"\n        if not self.current_tab:\n            await self._create_new_tab()\n\n        output = \"\"\n        # Precheck for forbidden commands\n        commands = re.split(r\"\\s*&&\\s*\", cmd)\n        for cmd_name, reason in self.forbidden_commands.items():\n            for command in commands:\n                if cmd_name in command:\n                    output += f\"{command} is forbidden. {reason}\\n\"\n        if output:\n            output = f\"Command not executed due to forbidden syntax:\\n{output}\\nPlease rewrite the whole command for execution.\"\n            return output\n\n        cmd = \" && \".join(commands)\n\n        # Send the command\n        current_tab = self.current_tab\n        if current_tab.is_running:\n            return (\n                \"Cannot execute the command because a previous one is still running in the current terminal tab\"\n                f\" (ID: {self.current_tab_id}).\"\n            )\n\n        # clear the output queue before execute command\n        output_queue = current_tab.output_queue\n        while not output_queue.empty():\n            output_queue.get_nowait()\n\n        await current_tab.execute(cmd)\n\n        tmp = []\n        is_service_flag = False\n        while True:\n            try:\n                # shorter timeout for service process by detecting output patterns such as localhost:port, ip:port\n                timeout = self.timeout if not is_service_flag else 3\n                line = await asyncio.wait_for(output_queue.get(), timeout=timeout)\n                if line is None:\n                    # ensure the read_and_process_output task is done\n                    await current_tab.task\n                    break\n                is_service_flag = is_service_flag or is_service_process(line)  # if True already, skip checking\n                tmp.append(line)\n            except asyncio.TimeoutError:\n                output_so_far = \"\".join(tmp)\n                if (returncode := current_tab.process.returncode) is not None:\n                    msg = f\"The terminal has exited with code {returncode}\"\n                    logger.warning(msg)\n                    return f\"{msg}, currently with output: {output_so_far}\"\n\n                current_tab.update_cwd()  # update cwd for the command still running\n                logger.info(f\"No more output after {timeout}s, detached from current tab and switched to a new tab\")\n\n                detached_tab_id = self.current_tab_id\n                new_tab_info = await self._create_new_tab()\n                instruction = DETACH_PROMPT.format(\n                    detached_tab_id=detached_tab_id,\n                    output_so_far=output_so_far,\n                    new_tab_info=new_tab_info,\n                )\n                # print(instruction)\n                return instruction\n        return \"\".join(tmp)\n\n    async def preview(self, tab_id: str, port: int, proj_name: str) -> str:\n        \"\"\"Preview a web project by forwarding a local port to public. Specify the id of the tab that runs the service, which is usually not the current tab but some detached tab.\"\"\"\n        if tab_id not in self.tabs:\n            return f\"Tab {tab_id} not found, created tabs are {list(self.tabs.keys())}, specify the correct tab_id that runs the service.\"\n        return await self.tabs[tab_id].preview(port, proj_name)\n\n    async def execute_in_conda_env(self, cmd: str, env) -> str:\n        \"\"\"\n        Executes a given command within a specified Conda environment automatically without\n        the need for manual activation. Users just need to provide the name of the Conda\n        environment and the command to execute.\n\n        Args:\n            cmd (str): The command to execute within the Conda environment.\n            env (str, optional): The name of the Conda environment to activate before executing the command.\n                                 If not specified, the command will run in the current active environment.\n\n        Returns:\n            str: The command's output, or an empty string if `daemon` is True, with output processed\n                 asynchronously in that case.\n\n        Note:\n            This function wraps `run`, prepending the necessary Conda activation commands\n            to ensure the specified environment is active for the command's execution.\n        \"\"\"\n        cmd = f\"conda run -n {env} {cmd}\"\n        return await self.run(cmd)\n\n    @handle_exception\n    async def close(self):\n        for tab in self.tabs.values():\n            await tab.close()\n        self.tabs = {}\n\n    @handle_exception\n    def __del__(self):\n        \"\"\"Close tabls when Terminal instance is destroyed\"\"\"\n        if self.tabs:\n            loop = asyncio.get_running_loop()\n            loop.create_task(self.close())\n",
                        "tags": []
                    },
                    "Editor": {
                        "name": "Editor",
                        "path": "metagpt/tools/libs/editor.py",
                        "schemas": {
                            "type": "class",
                            "description": "A tool for reading, understanding, writing, and editing files. Support local file including text-based files (txt, md, json, py, html, js, css, etc.), pdf, docx, excluding images, csv, excel, or online links For agent: Do NOT initiate multiple Editor.insert_content_at_line calls at the same time, since the line number will change starting with the first execution, making line number of the subsequent calls incorrect. Split the calls into separate responses. For the same reason, Editor.insert_content_at_line should NOT go behind Editor.edit_file_by_replace in the same response. Perform insert operation in a separate response.",
                            "methods": {
                                "write": {
                                    "type": "async_function",
                                    "description": "Write the whole content to a file. When used, make sure content arg contains the full content of the file.",
                                    "signature": "(self, path: str, content: str)",
                                    "parameters": ""
                                },
                                "read": {
                                    "type": "async_function",
                                    "description": "Read the whole content of a file. Using an absolute path as the argument for specifying the file location.",
                                    "signature": "(self, path: str) -> metagpt.tools.libs.editor.FileBlock",
                                    "parameters": ""
                                },
                                "similarity_search": {
                                    "type": "async_function",
                                    "description": "Given a filename or a pathname, performs a similarity search for a given query across the specified file or path. This method searches the index repository for the provided query, classifying the specified files or paths. It performs a search on each cluster of files and handles non-indexed files separately, merging results from structured indices with any direct results from non-indexed files. This function call does not depend on other functions. ",
                                    "signature": "(query: str, path: Union[str, pathlib.Path]) -> List[str]",
                                    "parameters": "Args: query (str): The search query string to look for in the indexed files. path (Union[str, Path]): A pathname or filename to search within. Returns: List[str]: A list of results as strings, containing the text from the merged results and any direct results from non-indexed files. Example: >>> query = \"The problem to be analyzed from the document\" >>> file_or_path = \"The pathname or filename you want to search within\" >>> texts: List[str] = await Editor.similarity_search(query=query, path=file_or_path) >>> print(texts)"
                                }
                            },
                            "tool_path": "metagpt/tools/libs/editor.py"
                        },
                        "code": "@register_tool(\n    include_functions=[\n        \"write\",\n        \"read\",\n        \"open_file\",\n        \"goto_line\",\n        \"scroll_down\",\n        \"scroll_up\",\n        \"create_file\",\n        \"edit_file_by_replace\",\n        \"insert_content_at_line\",\n        \"append_file\",\n        \"search_dir\",\n        \"search_file\",\n        \"find_file\",\n        \"similarity_search\",\n    ]\n)\nclass Editor(BaseModel):\n    \"\"\"\n    A tool for reading, understanding, writing, and editing files.\n    Support local file including text-based files (txt, md, json, py, html, js, css, etc.), pdf, docx, excluding images, csv, excel, or online links\n    For agent: Do NOT initiate multiple Editor.insert_content_at_line calls at the same time, since the line number will change starting with the first execution, making line number of the subsequent calls incorrect. Split the calls into separate responses.\n               For the same reason, Editor.insert_content_at_line should NOT go behind Editor.edit_file_by_replace in the same response. Perform insert operation in a separate response.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    resource: EditorReporter = EditorReporter()\n    current_file: Optional[Path] = None\n    current_line: int = 1\n    window: int = 50\n    enable_auto_lint: bool = True\n    working_dir: Path = DEFAULT_WORKSPACE_ROOT\n\n    async def write(self, path: str, content: str):\n        \"\"\"Write the whole content to a file. When used, make sure content arg contains the full content of the file.\"\"\"\n\n        path = self._try_fix_path(path)\n        content = ensure_string(content)  # Undo possible MCP conversion\n\n        if \"\\n\" not in content and \"\\\\n\" in content:\n            # A very raw rule to correct the content: If 'content' lacks actual newlines ('\\n') but includes '\\\\n', consider\n            # replacing them with '\\n' to potentially correct mistaken representations of newline characters.\n            content = content.replace(\"\\\\n\", \"\\n\")\n        directory = os.path.dirname(path)\n        if directory and not os.path.exists(directory):\n            os.makedirs(directory)\n        async with aiofiles.open(path, \"w\", encoding=\"utf-8\") as f:\n            await f.write(content)\n        # self.resource.report(path, \"path\")\n        return f\"File successfully written and saved to {path}.\"\n\n    def _read_structural_data(self, path: str) -> FileBlock:\n        \"\"\"Read the whole content of a csv file. Using an absolute path as the argument for specifying the file location.\"\"\"\n        if path.endswith(\".csv\"):\n            df = pd.read_csv(path)\n        elif path.endswith(\".xlsx\"):\n            df = pd.read_excel(path)\n        else:\n            raise ValueError(\"The file must be a csv or excel file.\")\n        return FileBlock(path=path, content=f\"Showing head of the data:\\n{df.head().to_string()}\")\n\n    async def read(self, path: str) -> FileBlock:\n        \"\"\"Read the whole content of a file. Using an absolute path as the argument for specifying the file location.\"\"\"\n\n        path = self._try_fix_path(path)\n\n        if path.suffix in STRUCTURAL_DATA_SUFFIX:\n            result = self._read_structural_data(str(path))\n            self.resource.report(str(path), \"path\")\n            return result\n\n        error = FileBlock(\n            path=str(path),\n            content=\"The file is too large to read. Use `Editor.similarity_search` to read the file instead.\",\n        )\n        path = Path(path)\n\n        # Limit the size only for text documents\n        # TODO: Formulate a complete mechanism to handle files of all types and of any size\n        if path.stat().st_size > 5 * DEFAULT_MIN_TOKEN_COUNT and path.suffix in TEXT_DOC_SUFFIX:\n            return error\n\n        content = await File.read_text_file(path)\n        if not content:\n            return FileBlock(path=str(path), content=\"\")\n        if self.is_large_file(content=content):\n            return error\n        self.resource.report(str(path), \"path\")\n\n        result = FileBlock(\n            path=str(path),\n            content=content,\n        )\n        return result\n\n    @staticmethod\n    def _is_valid_filename(file_name: str) -> bool:\n        if not file_name or not file_name.strip():\n            return False\n        invalid_chars = '<>:\"/\\\\|?*'\n        if os.name == \"nt\":  # Windows\n            invalid_chars = '<>:\"/\\\\|?*'\n        elif os.name == \"posix\":  # Unix-like systems\n            invalid_chars = \"\\0\"\n\n        for char in invalid_chars:\n            if char in file_name:\n                return False\n        return True\n\n    @staticmethod\n    def _is_valid_path(path: Path) -> bool:\n        try:\n            return path.exists()\n        except PermissionError:\n            return False\n\n    @staticmethod\n    def _create_paths(file_path: Path) -> bool:\n        try:\n            if file_path.parent:\n                file_path.parent.mkdir(parents=True, exist_ok=True)\n            return True\n        except PermissionError:\n            return False\n\n    def _check_current_file(self, file_path: Optional[Path] = None) -> bool:\n        if file_path is None:\n            file_path = self.current_file\n        if not file_path or not file_path.is_file():\n            raise ValueError(\"No file open. Use the open_file function first.\")\n        return True\n\n    @staticmethod\n    def _clamp(value, min_value, max_value):\n        return max(min_value, min(value, max_value))\n\n    def _lint_file(self, file_path: Path) -> tuple[Optional[str], Optional[int]]:\n        \"\"\"Lint the file at the given path and return a tuple with a boolean indicating if there are errors,\n        and the line number of the first error, if any.\n\n        Returns:\n            tuple[str | None, int | None]: (lint_error, first_error_line_number)\n        \"\"\"\n\n        linter = Linter(root=self.working_dir)\n        lint_error = linter.lint(str(file_path))\n        if not lint_error:\n            # Linting successful. No issues found.\n            return None, None\n        return \"ERRORS:\\n\" + lint_error.text, lint_error.lines[0]\n\n    async def _print_window(self, file_path: Path, targeted_line: int, window: int):\n        self._check_current_file(file_path)\n        async with aiofiles.open(file_path, \"r\") as file:\n            content = await file.read()\n\n            # Ensure the content ends with a newline character\n            if not content.endswith(\"\\n\"):\n                content += \"\\n\"\n\n            lines = content.splitlines(True)  # Keep all line ending characters\n            total_lines = len(lines)\n\n            # cover edge cases\n            self.current_line = self._clamp(targeted_line, 1, total_lines)\n            half_window = max(1, window // 2)\n\n            # Ensure at least one line above and below the targeted line\n            start = max(1, self.current_line - half_window)\n            end = min(total_lines, self.current_line + half_window)\n\n            # Adjust start and end to ensure at least one line above and below\n            if start == 1:\n                end = min(total_lines, start + window - 1)\n            if end == total_lines:\n                start = max(1, end - window + 1)\n\n            output = \"\"\n\n            # only display this when there's at least one line above\n            if start > 1:\n                output += f\"({start - 1} more lines above)\\n\"\n            else:\n                output += \"(this is the beginning of the file)\\n\"\n            for i in range(start, end + 1):\n                _new_line = f\"{i:03d}|{lines[i - 1]}\"\n                if not _new_line.endswith(\"\\n\"):\n                    _new_line += \"\\n\"\n                output += _new_line\n            if end < total_lines:\n                output += f\"({total_lines - end} more lines below)\\n\"\n            else:\n                output += \"(this is the end of the file)\\n\"\n            output = output.rstrip()\n\n            return output\n\n    @staticmethod\n    def _cur_file_header(current_file: Path, total_lines: int) -> str:\n        if not current_file:\n            return \"\"\n        return f\"[File: {current_file.resolve()} ({total_lines} lines total)]\\n\"\n\n    async def set_workdir(self, path: str) -> None:\n        \"\"\"\n        Sets the working directory to the given path. eg: repo directory.\n        You MUST to set it up before open the file.\n\n        Args:\n            path: str: The path to set as the working directory.\n        \"\"\"\n        self.working_dir = Path(path)\n\n    async def open_file(\n        self, path: Union[Path, str], line_number: Optional[int] = 1, context_lines: Optional[int] = None\n    ) -> str:\n        \"\"\"Opens the file at the given path in the editor. If line_number is provided, the window will be moved to include that line.\n        It only shows the first 100 lines by default! Max `context_lines` supported is 2000, use `scroll up/down`\n        to view the file if you want to see more.\n\n        Args:\n            path: str: The path to the file to open, preferred absolute path.\n            line_number: int | None = 1: The line number to move to. Defaults to 1.\n            context_lines: int | None = 100: Only shows this number of lines in the context window (usually from line 1), with line_number as the center (if possible). Defaults to 100.\n        \"\"\"\n        if context_lines is None:\n            context_lines = self.window\n\n        path = self._try_fix_path(path)\n\n        if not path.is_file():\n            raise FileNotFoundError(f\"File {path} not found\")\n\n        self.current_file = path\n\n        total_lines = await self._count_file_lines(file_path=path)\n\n        if not isinstance(line_number, int) or line_number < 1 or line_number > total_lines:\n            raise ValueError(f\"Line number must be between 1 and {total_lines}\")\n        self.current_line = line_number\n\n        # Override WINDOW with context_lines\n        if context_lines is None or context_lines < 1:\n            context_lines = self.window\n\n        output = self._cur_file_header(path, total_lines)\n        output += await self._print_window(path, self.current_line, self._clamp(context_lines, 1, 2000))\n        self.resource.report(path, \"path\")\n        return output\n\n    async def goto_line(self, line_number: int) -> str:\n        \"\"\"Moves the window to show the specified line number.\n\n        Args:\n            line_number: int: The line number to move to.\n        \"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        if not isinstance(line_number, int) or line_number < 1 or line_number > total_lines:\n            raise ValueError(f\"Line number must be between 1 and {total_lines}\")\n\n        self.current_line = self._clamp(line_number, 1, total_lines)\n\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def scroll_down(self) -> str:\n        \"\"\"Moves the window down by 100 lines.\"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        self.current_line = self._clamp(self.current_line + self.window, 1, total_lines)\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def scroll_up(self) -> str:\n        \"\"\"Moves the window up by 100 lines.\"\"\"\n        self._check_current_file()\n\n        total_lines = await self._count_file_lines(file_path=self.current_file)\n\n        self.current_line = self._clamp(self.current_line - self.window, 1, total_lines)\n        output = self._cur_file_header(self.current_file, total_lines)\n        output += await self._print_window(self.current_file, self.current_line, self.window)\n        return output\n\n    async def create_file(self, filename: str) -> str:\n        \"\"\"Creates and opens a new file with the given name.\n\n        Args:\n            filename: str: The name of the file to create. If the parent directory does not exist, it will be created.\n        \"\"\"\n        filename = self._try_fix_path(filename)\n\n        if filename.exists():\n            raise FileExistsError(f\"File '{filename}' already exists.\")\n        await awrite(filename, \"\\n\")\n\n        self.open_file(filename)\n        return f\"[File {filename} created.]\"\n\n    @staticmethod\n    def _append_impl(lines, content):\n        \"\"\"Internal method to handle appending to a file.\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            content: str: The content to append to the file.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n        \"\"\"\n        content_lines = content.splitlines(keepends=True)\n        n_added_lines = len(content_lines)\n        if lines and not (len(lines) == 1 and lines[0].strip() == \"\"):\n            # file is not empty\n            if not lines[-1].endswith(\"\\n\"):\n                lines[-1] += \"\\n\"\n            new_lines = lines + content_lines\n            content = \"\".join(new_lines)\n        else:\n            # file is empty\n            content = \"\".join(content_lines)\n\n        return content, n_added_lines\n\n    @staticmethod\n    def _insert_impl(lines, start, content):\n        \"\"\"Internal method to handle inserting to a file.\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            start: int: The start line number for inserting.\n            content: str: The content to insert to the file.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n\n        Raises:\n            LineNumberError: If the start line number is invalid.\n        \"\"\"\n        inserted_lines = [content + \"\\n\" if not content.endswith(\"\\n\") else content]\n        if len(lines) == 0:\n            new_lines = inserted_lines\n        elif start is not None:\n            if len(lines) == 1 and lines[0].strip() == \"\":\n                # if the file with only 1 line and that line is empty\n                lines = []\n\n            if len(lines) == 0:\n                new_lines = inserted_lines\n            else:\n                new_lines = lines[: start - 1] + inserted_lines + lines[start - 1 :]\n        else:\n            raise LineNumberError(\n                f\"Invalid line number: {start}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n\n        content = \"\".join(new_lines)\n        n_added_lines = len(inserted_lines)\n        return content, n_added_lines\n\n    @staticmethod\n    def _edit_impl(lines, start, end, content):\n        \"\"\"Internal method to handle editing a file.\n\n        REQUIRES (should be checked by caller):\n            start <= end\n            start and end are between 1 and len(lines) (inclusive)\n            content ends with a newline\n\n        Args:\n            lines: list[str]: The lines in the original file.\n            start: int: The start line number for editing.\n            end: int: The end line number for editing.\n            content: str: The content to replace the lines with.\n\n        Returns:\n            content: str: The new content of the file.\n            n_added_lines: int: The number of lines added to the file.\n        \"\"\"\n        # Handle cases where start or end are None\n        if start is None:\n            start = 1  # Default to the beginning\n        if end is None:\n            end = len(lines)  # Default to the end\n        # Check arguments\n        if not (1 <= start <= len(lines)):\n            raise LineNumberError(\n                f\"Invalid start line number: {start}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n        if not (1 <= end <= len(lines)):\n            raise LineNumberError(\n                f\"Invalid end line number: {end}. Line numbers must be between 1 and {len(lines)} (inclusive).\"\n            )\n        if start > end:\n            raise LineNumberError(f\"Invalid line range: {start}-{end}. Start must be less than or equal to end.\")\n\n        # Split content into lines and ensure it ends with a newline\n        if not content.endswith(\"\\n\"):\n            content += \"\\n\"\n        content_lines = content.splitlines(True)\n\n        # Calculate the number of lines to be added\n        n_added_lines = len(content_lines)\n\n        # Remove the specified range of lines and insert the new content\n        new_lines = lines[: start - 1] + content_lines + lines[end:]\n\n        # Handle the case where the original lines are empty\n        if len(lines) == 0:\n            new_lines = content_lines\n\n        # Join the lines to create the new content\n        content = \"\".join(new_lines)\n        return content, n_added_lines\n\n    def _get_indentation_info(self, content, first_line):\n        \"\"\"\n        The indentation of the first insert line and the previous line, along with guidance for the next attempt.\n        \"\"\"\n        content_lines = content.split(\"\\n\")\n        pre_line = content_lines[first_line - 2] if first_line - 2 >= 0 else \"\"\n        pre_line_indent = len(pre_line) - len(pre_line.lstrip())\n        insert_line = content_lines[first_line - 1]\n        insert_line_indent = len(insert_line) - len(insert_line.lstrip())\n        ret_str = INDENTATION_INFO.format(\n            pre_line=pre_line,\n            pre_line_indent=pre_line_indent,\n            insert_line=insert_line,\n            insert_line_indent=insert_line_indent,\n            sub_4_space=max(insert_line_indent - 4, 0),\n            add_4_space=insert_line_indent + 4,\n        )\n        return ret_str\n\n    async def _edit_file_impl(\n        self,\n        file_name: Path,\n        start: Optional[int] = None,\n        end: Optional[int] = None,\n        content: str = \"\",\n        is_insert: bool = False,\n        is_append: bool = False,\n    ) -> str:\n        \"\"\"Internal method to handle common logic for edit_/append_file methods.\n\n        Args:\n            file_name: Path: The name of the file to edit or append to.\n            start: int | None = None: The start line number for editing. Ignored if is_append is True.\n            end: int | None = None: The end line number for editing. Ignored if is_append is True.\n            content: str: The content to replace the lines with or to append.\n            is_insert: bool = False: Whether to insert content at the given line number instead of editing.\n            is_append: bool = False: Whether to append content to the file instead of editing.\n        \"\"\"\n\n        ERROR_MSG = f\"[Error editing file {file_name}. Please confirm the file is correct.]\"\n        ERROR_MSG_SUFFIX = (\n            \"Your changes have NOT been applied. Please fix your edit command and try again.\\n\"\n            \"You either need to 1) Open the correct file and try again or 2) Specify the correct line number arguments.\\n\"\n            \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n        )\n\n        if not self._is_valid_filename(file_name.name):\n            raise FileNotFoundError(\"Invalid file name.\")\n\n        if not self._is_valid_path(file_name):\n            raise FileNotFoundError(\"Invalid path or file name.\")\n\n        if not self._create_paths(file_name):\n            raise PermissionError(\"Could not access or create directories.\")\n\n        if not file_name.is_file():\n            raise FileNotFoundError(f\"File {file_name} not found.\")\n\n        if is_insert and is_append:\n            raise ValueError(\"Cannot insert and append at the same time.\")\n\n        # Use a temporary file to write changes\n        content = str(content or \"\")\n        temp_file_path = \"\"\n        src_abs_path = file_name.resolve()\n        first_error_line = None\n        # The file to store previous content and will be removed automatically.\n        temp_backup_file = tempfile.NamedTemporaryFile(\"w\", delete=True)\n\n        try:\n            # lint the original file\n            if self.enable_auto_lint:\n                original_lint_error, _ = self._lint_file(file_name)\n\n            # Create a temporary file\n            with tempfile.NamedTemporaryFile(\"w\", delete=False) as temp_file:\n                temp_file_path = temp_file.name\n\n                # Read the original file and check if empty and for a trailing newline\n                async with aiofiles.open(file_name, \"r\") as original_file:\n                    lines = await original_file.readlines()\n\n                if is_append:\n                    content, n_added_lines = self._append_impl(lines, content)\n                elif is_insert:\n                    try:\n                        content, n_added_lines = self._insert_impl(lines, start, content)\n                    except LineNumberError as e:\n                        return (f\"{ERROR_MSG}\\n\" f\"{e}\\n\" f\"{ERROR_MSG_SUFFIX}\") + \"\\n\"\n                else:\n                    try:\n                        content, n_added_lines = self._edit_impl(lines, start, end, content)\n                    except LineNumberError as e:\n                        return (f\"{ERROR_MSG}\\n\" f\"{e}\\n\" f\"{ERROR_MSG_SUFFIX}\") + \"\\n\"\n\n                if not content.endswith(\"\\n\"):\n                    content += \"\\n\"\n\n                # Write the new content to the temporary file\n                temp_file.write(content)\n\n            # Replace the original file with the temporary file atomically\n            shutil.move(temp_file_path, src_abs_path)\n\n            # Handle linting\n            # NOTE: we need to get env var inside this function\n            # because the env var will be set AFTER the agentskills is imported\n            if self.enable_auto_lint:\n                # BACKUP the original file\n                temp_backup_file.writelines(lines)\n                temp_backup_file.flush()\n                lint_error, first_error_line = self._lint_file(file_name)\n\n                # Select the errors caused by the modification\n                def extract_last_part(line):\n                    parts = line.split(\":\")\n                    if len(parts) > 1:\n                        return parts[-1].strip()\n                    return line.strip()\n\n                def subtract_strings(str1, str2) -> str:\n                    lines1 = str1.splitlines()\n                    lines2 = str2.splitlines()\n\n                    last_parts1 = [extract_last_part(line) for line in lines1]\n\n                    remaining_lines = [line for line in lines2 if extract_last_part(line) not in last_parts1]\n\n                    result = \"\\n\".join(remaining_lines)\n                    return result\n\n                if original_lint_error and lint_error:\n                    lint_error = subtract_strings(original_lint_error, lint_error)\n                    if lint_error == \"\":\n                        lint_error = None\n                        first_error_line = None\n\n                if lint_error is not None:\n                    # show the first insert line.\n                    if is_append:\n                        # original end-of-file\n                        show_line = len(lines)\n                    # insert OR edit WILL provide meaningful line numbers\n                    elif start is not None and end is not None:\n                        show_line = int((start + end) / 2)\n                    else:\n                        raise ValueError(\"Invalid state. This should never happen.\")\n\n                    guidance_message = self._get_indentation_info(content, start or len(lines))\n                    guidance_message += (\n                        \"You either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\\n\"\n                        \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n                    )\n                    lint_error_info = ERROR_GUIDANCE.format(\n                        linter_error_msg=LINTER_ERROR_MSG + lint_error,\n                        window_after_applied=await self._print_window(file_name, show_line, n_added_lines + 20),\n                        window_before_applied=await self._print_window(\n                            Path(temp_backup_file.name), show_line, n_added_lines + 20\n                        ),\n                        guidance_message=guidance_message,\n                    ).strip()\n\n                    # recover the original file\n                    shutil.move(temp_backup_file.name, src_abs_path)\n                    return lint_error_info\n\n        except FileNotFoundError as e:\n            return f\"File not found: {e}\\n\"\n        except IOError as e:\n            return f\"An error occurred while handling the file: {e}\\n\"\n        except ValueError as e:\n            return f\"Invalid input: {e}\\n\"\n        except Exception as e:\n            guidance_message = self._get_indentation_info(content, start or len(lines))\n            guidance_message += (\n                \"You either need to 1) Specify the correct start/end line arguments or 2) Enlarge the range of original code.\\n\"\n                \"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n            )\n            error_info = ERROR_GUIDANCE.format(\n                linter_error_msg=LINTER_ERROR_MSG + str(e),\n                window_after_applied=await self._print_window(file_name, start or len(lines), 100),\n                window_before_applied=await self._print_window(Path(temp_backup_file.name), start or len(lines), 100),\n                guidance_message=guidance_message,\n            ).strip()\n            # Clean up the temporary file if an error occurs\n            shutil.move(temp_backup_file.name, src_abs_path)\n            if temp_file_path and Path(temp_file_path).exists():\n                Path(temp_file_path).unlink()\n\n            raise Exception(f\"{error_info}\") from e\n\n        # Update the file information and print the updated content\n        n_total_lines = await self._count_file_lines(file_path=file_name, encoding=\"utf-8\")\n\n        if first_error_line is not None and int(first_error_line) > 0:\n            self.current_line = first_error_line\n        else:\n            if is_append:\n                self.current_line = max(1, len(lines))  # end of original file\n            else:\n                self.current_line = start or n_total_lines or 1\n\n        success_edit_info = SUCCESS_EDIT_INFO.format(\n            file_name=file_name.resolve(),\n            n_total_lines=n_total_lines,\n            window_after_applied=await self._print_window(file_name, self.current_line, self.window),\n            line_number=self.current_line,\n        ).strip()\n\n        return success_edit_info\n\n    async def edit_file_by_replace(self, file_name: str, to_replace: str, new_content: str) -> str:\n        \"\"\"Edit a file. This will search for `to_replace` in the given file and replace it with `new_content`.\n\n        Every *to_replace* must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n        * The `to_replace` parameter should match EXACTLY one or more consecutive lines from the original file. Be mindful of whitespaces!\n        * If the `to_replace` parameter is not unique in the file, the replacement will not be performed. Make sure to include enough context in `to_replace` to make it unique\n        * The `new_content` parameter should contain the edited lines that should replace the `to_replace`\n\n        For example, given a file \"path/to/example.txt\" with the following content:\n        ```\n        line 1\n        line 2\n        line 2\n        line 3\n        ```\n\n        EDITING: If you want to replace the second occurrence of \"line 2\", you can make `to_replace` unique:\n\n        edit_file_by_replace(\n            'path/to/example.txt',\n            to_replace='line 2\\nline 3',\n            new_content='new line\\nline 3',\n        )\n\n        This will replace only the second \"line 2\" with \"new line\". The first \"line 2\" will remain unchanged.\n\n        The resulting file will be:\n        ```\n        line 1\n        line 2\n        new line\n        line 3\n        ```\n\n        REMOVAL: If you want to remove \"line 2\" and \"line 3\", you can set `new_content` to an empty string:\n\n        edit_file_by_replace(\n            'path/to/example.txt',\n            to_replace='line 2\\nline 3',\n            new_content='',\n        )\n        \"\"\"\n        # search for `to_replace` in the file\n        # if found, replace it with `new_content`\n        # if not found, perform a fuzzy search to find the closest match and replace it with `new_content`\n        file_name = self._try_fix_path(file_name)\n\n        async with aiofiles.open(file_name, \"r\") as file:\n            file_content = await file.read()\n\n        if to_replace.strip() == \"\":\n            if file_content.strip() == \"\":\n                raise ValueError(f\"The file '{file_name}' is empty. Please use the append method to add content.\")\n            raise ValueError(\"`to_replace` must not be empty.\")\n\n        to_replace = self._rm_line_no(to_replace)\n        new_content = self._rm_line_no(new_content)\n\n        if file_content.count(to_replace) > 1:\n            raise ValueError(\n                f\"`to_replace` ```{to_replace}``` appears more than once, please include enough lines to make code in `to_replace` unique.\"\n            )\n        elif file_content.count(to_replace) == 0:\n            # If no exact match, try to find close matches using LCS\n            if len(to_replace) > 20:\n                # white spaces are usually missing in multi-line content, thus hard coding a threshold of 20\n                _, similarity, file_content_match = self.lcs(to_replace, file_content)\n                if similarity > 0.9:  # High similarity suggests missing whitespace\n                    raise ValueError(\n                        f\"`to_replace` ```{to_replace}``` not found in {file_name}. Pay attention to spaces and line breaks! Did you actually mean ```{file_content_match}```\"\n                    )\n            raise ValueError(\n                f\"`to_replace` ```{to_replace}``` not found in {file_name}. Read the file carefully with Editor.read and make sure you give the right content to replace. If you want to insert new content, use Editor.insert_content_at_line instead.\"\n            )\n\n        ### Take a easy way to replace the content with direct string operation. Also disable linting for now ###\n        ### TODO: unittest to cover this part ###\n        new_content = file_content.replace(to_replace, new_content)\n        async with aiofiles.open(file_name, \"w\") as file:\n            await file.write(new_content)\n\n        start = file_content.find(to_replace) + 1  # index of starting char\n        start_line_number = file_content[:start].count(\"\\n\") + 1\n        window_after_applied = await self._print_window(file_name, start_line_number, 30)\n        success_edit_info = SUCCESS_EDIT_INFO.format(\n            file_name=file_name.resolve(),\n            n_total_lines=\"\",\n            window_after_applied=window_after_applied,\n            line_number=start_line_number,\n        ).strip()\n        self.resource.report(file_name, \"path\")\n        return success_edit_info\n\n    async def insert_content_at_line(self, file_name: str, line_number: int, insert_content: str) -> str:\n        \"\"\"Insert a complete block of code before the given line number in a file. That is, the new content will start at the beginning of the specified line, and the existing content of that line will be moved down.\n        This operation will NOT modify the content of the lines before or after the given line number.\n        This function can not insert content the end of the file. Please use append_file instead,\n        For example, if the file has the following content:\n        ```\n        001|contain g\n        002|contain h\n        003|contain i\n        004|contain j\n        ```\n        and you call\n        insert_content_at_line(\n            file_name='file.txt',\n            line_number=2,\n            insert_content='new line'\n        )\n        the file will be updated to:\n        ```\n        001|contain g\n        002|new line\n        003|contain h\n        004|contain i\n        005|contain j\n        ```\n\n        Args:\n            file_name: (str): The name of the file to edit.\n            line_number (int): The line number (starting from 1) to insert the content after. The insert content will be add between the line of line_number-1 and line_number\n            insert_content (str): The content to insert betweed the previous_line_content and current_line_content.The insert_content must be a complete block of code at.\n        \"\"\"\n        file_name = self._try_fix_path(file_name)\n        ret_str = await self._edit_file_impl(\n            file_name,\n            start=line_number,\n            end=line_number,\n            content=insert_content,\n            is_insert=True,\n            is_append=False,\n        )\n        self.resource.report(file_name, \"path\")\n        return ret_str\n\n    async def append_file(self, file_name: str, content: str) -> str:\n        \"\"\"Append content to the given file.\n        It appends text `content` to the end of the specified file.\n\n        Args:\n            file_name: str: The name of the file to edit.\n            content: str: The content to insert.\n        \"\"\"\n        file_name = self._try_fix_path(file_name)\n        ret_str = await self._edit_file_impl(\n            file_name,\n            start=None,\n            end=None,\n            content=content,\n            is_insert=False,\n            is_append=True,\n        )\n        self.resource.report(file_name, \"path\")\n        return ret_str\n\n    async def search_dir(self, search_term: str, dir_path: str = \"./\") -> str:\n        \"\"\"Searches for search_term in all files in dir. If dir is not provided, searches in the current directory.\n\n        Args:\n            search_term: str: The term to search for.\n            dir_path: str: The path to the directory to search.\n        \"\"\"\n        dir_path = self._try_fix_path(dir_path)\n        if not dir_path.is_dir():\n            raise FileNotFoundError(f\"Directory {dir_path} not found\")\n\n        matches = []\n        for root, _, files in os.walk(dir_path):\n            for file in files:\n                if file.startswith(\".\"):\n                    continue\n                file_path = Path(root) / file\n                try:\n                    async with aiofiles.open(file_path, \"r\", errors=\"ignore\") as f:\n                        line_num = 0\n                        async for line in f:\n                            line_num += 1\n                            if search_term in line:\n                                matches.append((file_path, line_num, line.strip()))\n                except Exception:\n                    # Skip files that can't be read\n                    continue\n\n        if not matches:\n            return f'No matches found for \"{search_term}\" in {dir_path}'\n\n        num_matches = len(matches)\n        num_files = len(set(match[0] for match in matches))\n\n        if num_files > 100:\n            return f'More than {num_files} files matched for \"{search_term}\" in {dir_path}. Please narrow your search.'\n\n        res_list = [f'[Found {num_matches} matches for \"{search_term}\" in {dir_path}]']\n        for file_path, line_num, line in matches:\n            res_list.append(f\"{file_path} (Line {line_num}): {line}\")\n        res_list.append(f'[End of matches for \"{search_term}\" in {dir_path}]')\n        return \"\\n\".join(res_list)\n\n    async def search_file(self, search_term: str, file_path: Optional[str] = None) -> str:\n        \"\"\"Searches for search_term in file. If file is not provided, searches in the current open file.\n\n        Args:\n            search_term: str: The term to search for.\n            file_path: str | None: The path to the file to search.\n        \"\"\"\n        if file_path is None:\n            file_path = self.current_file\n        else:\n            file_path = self._try_fix_path(file_path)\n        if file_path is None:\n            raise FileNotFoundError(\"No file specified or open. Use the open_file function first.\")\n        if not file_path.is_file():\n            raise FileNotFoundError(f\"File {file_path} not found\")\n\n        matches = []\n        async with aiofiles.open(file_path, \"r\") as file:\n            i = 0\n            async for line in file:\n                i += 1\n                if search_term in line:\n                    matches.append((i, line.strip()))\n\n        res_list = []\n        if matches:\n            res_list.append(f'[Found {len(matches)} matches for \"{search_term}\" in {file_path}]')\n            for match in matches:\n                res_list.append(f\"Line {match[0]}: {match[1]}\")\n            res_list.append(f'[End of matches for \"{search_term}\" in {file_path}]')\n        else:\n            res_list.append(f'[No matches found for \"{search_term}\" in {file_path}]')\n\n        extra = {\"type\": \"search\", \"symbol\": search_term, \"lines\": [i[0] - 1 for i in matches]} if matches else None\n        self.resource.report(file_path, \"path\", extra=extra)\n        return \"\\n\".join(res_list)\n\n    async def find_file(self, file_name: str, dir_path: str = \"./\") -> str:\n        \"\"\"Finds all files with the given name or path in the specified directory.\n\n        Args:\n            file_name: str: The name or path of the file to find.\n                           If it's an absolute path, will match the exact path.\n                           If it's a relative path, will match the path suffix.\n                           If it's just a name, will match file names.\n            dir_path: str: The path to the directory to search.\n        \"\"\"\n        dir_path = self._try_fix_path(dir_path)\n        if not dir_path.is_dir():\n            raise FileNotFoundError(f\"Directory {dir_path} not found\")\n\n        file_path = Path(file_name)\n\n        # TODO: use asyncio.gather to concurrently check multiple paths\n        matches = []\n        for root, _, files in os.walk(dir_path):\n            for file in files:\n                current_path = Path(root) / file\n\n                if file_path.is_absolute():\n                    if current_path == file_path:\n                        matches.append(current_path)\n                elif \"/\" in str(file_path) or \"\\\\\" in str(file_path):\n                    if str(current_path).endswith(str(file_path)):\n                        matches.append(current_path)\n                else:\n                    if current_path.name == file_name:\n                        matches.append(current_path)\n\n        res_list = []\n        if matches:\n            res_list.append(f'[Found {len(matches)} matches for \"{file_name}\" in {dir_path}]')\n            for match in matches:\n                res_list.append(f\"{match}\")\n            res_list.append(f'[End of matches for \"{file_name}\" in {dir_path}]')\n        else:\n            res_list.append(f'[No matches found for \"{file_name}\" in {dir_path}]')\n        return \"\\n\".join(res_list)\n\n    def _try_fix_path(self, path: Union[Path, str]) -> Path:\n        \"\"\"Tries to fix the path if it is not absolute.\"\"\"\n        if not isinstance(path, Path):\n            path = Path(path)\n        if not path.is_absolute():\n            path = self.working_dir / path\n        return path\n\n    @staticmethod\n    def _rm_line_no(content: str) -> str:\n        line_no_pattern = \"\\n[0-9]{3}\\|\"\n        if re.search(line_no_pattern, content):\n            logger.warning(\"Line numbers detected in content and will be removed\")\n            content = re.sub(line_no_pattern, \"\\n\", content)\n        return content\n\n    @staticmethod\n    async def similarity_search(query: str, path: Union[str, Path]) -> List[str]:\n        \"\"\"Given a filename or a pathname, performs a similarity search for a given query across the specified file or path.\n\n        This method searches the index repository for the provided query, classifying the specified\n        files or paths. It performs a search on each cluster of files and handles non-indexed files\n        separately, merging results from structured indices with any direct results from non-indexed files.\n        This function call does not depend on other functions.\n\n        Args:\n            query (str): The search query string to look for in the indexed files.\n            path (Union[str, Path]): A pathname or filename to search within.\n\n        Returns:\n            List[str]: A list of results as strings, containing the text from the merged results\n                        and any direct results from non-indexed files.\n\n        Example:\n            >>> query = \"The problem to be analyzed from the document\"\n            >>> file_or_path = \"The pathname or filename you want to search within\"\n            >>> texts: List[str] = await Editor.similarity_search(query=query, path=file_or_path)\n            >>> print(texts)\n        \"\"\"\n        return await IndexRepo.cross_repo_search(query=query, file_or_path=path)\n\n    @staticmethod\n    def is_large_file(content: str, mix_token_count: int = 0) -> bool:\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n        token_count = len(encoding.encode(content))\n        mix_token_count = mix_token_count or DEFAULT_MIN_TOKEN_COUNT\n        return token_count >= mix_token_count\n\n    @staticmethod\n    def lcs(s1: str, s2: str) -> tuple[str, float, str]:\n        \"\"\"Find the longest common subsequence between two strings and the matching section from s2. Assuming s1 is always a subsequence of s2.\n\n        Args:\n            s1: First string (typically the to_replace content)\n            s2: Second string (typically the file content)\n\n        Returns:\n            tuple[str, float, str]: The LCS string, its similarity score (0-1), and the matching section from s2\n        \"\"\"\n        # Create DP table\n        m, n = len(s1), len(s2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n\n        # Fill DP table\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if s1[i - 1] == s2[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n\n        # Reconstruct LCS and track positions in s2\n        lcs = []\n        i, j = m, n\n        positions = []  # Store positions of matches in s2\n        while i > 0 and j > 0:\n            if s1[i - 1] == s2[j - 1]:\n                lcs.append(s1[i - 1])\n                positions.append(j - 1)\n                i -= 1\n                j -= 1\n            elif dp[i - 1][j] > dp[i][j - 1]:\n                i -= 1\n            else:\n                j -= 1\n\n        lcs = \"\".join(reversed(lcs))\n        positions = list(reversed(positions))\n\n        # Calculate similarity score\n        score = len(lcs) / len(s1)\n\n        # Extract the matching section from s2\n        if positions:\n            start = min(positions)\n            end = max(positions) + 1\n            matching_section = s2[start:end]\n        else:\n            matching_section = \"\"\n        matching_section = matching_section[\n            : int(1.2 * len(s1))\n        ]  # hard truncate to avoid greedily match too long string\n\n        return lcs, score, matching_section\n\n    async def _count_file_lines(self, file_path: Union[Path, str], encoding: Optional[str] = None) -> int:\n        \"\"\"Asynchronously count the total number of lines in a file.\n\n        Args:\n            file_path: Path to the file.\n            encoding: The encoding used to open the file. Defaults to None (system default encoding).\n\n        Returns:\n            int: Total number of lines in the file, minimum 1.\n        \"\"\"\n        total_lines = 0\n        async with aiofiles.open(file_path, \"r\", encoding=encoding) as file:\n            async for _ in file:\n                total_lines += 1\n        return max(1, total_lines)\n\n    async def remove_file(self, file_path: Union[Path, str], missing_ok: bool = True) -> str:\n        \"\"\"Remove a file from the file system.\n\n        Args:\n            file_path: Path to the file to remove.\n            missing_ok: If True, don't raise error if file doesn't exist.\n        \"\"\"\n        path = self._try_fix_path(file_path)\n        path.unlink(missing_ok=missing_ok)\n        await self.resource.async_report(file_path, \"delete\")\n        return f\"File {path} removed successfully\"\n",
                        "tags": []
                    }
                },
                "force": true
            },
            "use_reflection": true,
            "execute_code": {
                "rp_meta": {
                    "uuid": "3b0a8f9a2e3a4d1f90a3ca3be5f638fc",
                    "storage_dir": "/data/chats/28286616f10743a6baaf2bd913b64f39/workspace/.MGXTools",
                    "user_id": 644050,
                    "chat_id": "28286616f10743a6baaf2bd913b64f39",
                    "role_name": "David"
                }
            },
            "__module_class_name": "metagpt.roles.di.data_analyst.DataAnalyst"
        }
    },
    "history": {
        "storage": [
            {
                "id": "df14fbea5cb742d489435448bacf7ffe",
                "timestamp": "20251126061240991",
                "content": "[Message] from User to Mike: there are 0 tables ",
                "instruct_content": null,
                "role": "user",
                "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                "sent_from": "",
                "send_to": [
                    "<all>",
                    "Mike"
                ],
                "metadata": {}
            },
            {
                "id": "df14fbea5cb742d489435448bacf7ffe",
                "timestamp": "20251126061240991",
                "content": "there are 0 tables ",
                "instruct_content": null,
                "role": "user",
                "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                "sent_from": "",
                "send_to": [
                    "<all>",
                    "Mike"
                ],
                "metadata": {}
            }
        ],
        "index": {
            "metagpt.actions.add_requirement.UserRequirement": [
                {
                    "id": "df14fbea5cb742d489435448bacf7ffe",
                    "timestamp": "20251126061240991",
                    "content": "[Message] from User to Mike: there are 0 tables ",
                    "instruct_content": null,
                    "role": "user",
                    "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                    "sent_from": "",
                    "send_to": [
                        "<all>",
                        "Mike"
                    ],
                    "metadata": {}
                },
                {
                    "id": "df14fbea5cb742d489435448bacf7ffe",
                    "timestamp": "20251126061240991",
                    "content": "there are 0 tables ",
                    "instruct_content": null,
                    "role": "user",
                    "cause_by": "metagpt.actions.add_requirement.UserRequirement",
                    "sent_from": "",
                    "send_to": [
                        "<all>",
                        "Mike"
                    ],
                    "metadata": {}
                }
            ]
        },
        "ignore_id": false,
        "appendix": null
    },
    "direct_chat_roles": [],
    "is_public_chat": true,
    "attach_image_k": 3,
    "__module_class_name": "mgx_thinking.chat.MGXEnv"
}